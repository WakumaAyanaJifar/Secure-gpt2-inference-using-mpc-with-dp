{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4e5789-a120-46be-99ed-7b517c18a864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval [-4, -2]:\n",
      "  Polynomial Coefficients: [-0.01101871 -0.11797748 -0.42209696 -0.50535995]\n",
      "  Maximum Absolute Error: 0.0006560470649745742\n",
      "Interval [-2, 0]:\n",
      "  Polynomial Coefficients: [ 0.10369006  0.44591346  0.50133061 -0.00097519]\n",
      "  Maximum Absolute Error: 0.004100745803154372\n",
      "Interval [0, 2]:\n",
      "  Polynomial Coefficients: [-0.10369006  0.44591346  0.49866939 -0.00097519]\n",
      "  Maximum Absolute Error: 0.004100745803153005\n",
      "Interval [2, 4]:\n",
      "  Polynomial Coefficients: [ 0.01101871 -0.11797748  1.42209696 -0.50535995]\n",
      "  Maximum Absolute Error: 0.0006560470649699113\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAIjCAYAAADFthA8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACODUlEQVR4nOzdeZyNdf/H8dd1Zs7sG2MZDDN2Q7bImiyRSMiNpGKoRNaU0Er7IhERJcpNtzbVLd1CUclOlizZQ7KMMft2zrl+f8j8GjOYYY5rlvezxzzGuc51fa/3+Z5rpvOZ63t9L8M0TRMREREREZFiwmZ1ABERERERketJRZCIiIiIiBQrKoJERERERKRYUREkIiIiIiLFioogEREREREpVlQEiYiIiIhIsaIiSEREREREihUVQSIiIiIiUqyoCBIRERERkWJFRZCIFHlt2rShTZs2+dLW4cOHMQyDefPm5Ut719uqVaswDINVq1ZZHeWq5ef7abV58+ZhGAaHDx/O87bR0dFERkbme6bC6lr60p0iIyOJjo62OoaIXERFkEghdujQIYYNG0aNGjXw8/PDz8+P2rVrM3ToULZv355l3QkTJmAYxiW//vrrL+D/P+RPmjTpkvuNjIykS5cuOT63adOmQlskREZGZumTMmXK0KpVKxYvXmx1tELtwofTC18+Pj7UqFGDYcOGcfLkSavjFStNmjTBMAxmzpxpdZQi5ZdffmHChAmcO3fO6igikkueVgcQkauzZMkS7r77bjw9Pbn33nupX78+NpuNPXv28MUXXzBz5kwOHTpERERElu1mzpxJQEBAtvZCQkKuU/KCrUGDBjz22GMA/Pnnn8yaNYsePXowc+ZMBg8eTEREBCkpKdjtdouTXp1bbrmFlJQUvLy8rvu+n3/+eSpXrkxqaio///wzM2fOZOnSpezcuRM/P7/rnqcguP/+++nTpw/e3t5u39e+ffvYuHEjkZGRLFiwgCFDhrh9n9fT9ezLi/3yyy9MnDiR6OjobL9L9+7di82mvzmLFDQqgkQKoQMHDtCnTx8iIiJYuXIl5cqVy/L8a6+9xowZM3L8H2/Pnj0pVarU9Ypa6FSoUIH77rsv83G/fv2oVq0ab731FoMHD848k1FY2Ww2y/J36tSJxo0bA/Dggw8SGhrK5MmT+eqrr7jnnnssyWQ1Dw8PPDw8rsu+/v3vf1OmTBnefPNNevbsyeHDhy0bTpeUlIS/v3++tnk9+zIvrCjKROTK9KcJkULo9ddfJykpiblz52YrgAA8PT0ZMWIEFStWtCBd7pw9e5bHH3+cunXrEhAQQFBQEJ06dWLbtm1Z1rtwDcsnn3zCSy+9RHh4OD4+Ptx6663s378/W7uzZ8+matWq+Pr60qRJE3766adryhkWFkZUVBSHDh0CLn1N0J49e+jZsyclS5bEx8eHxo0b8/XXX2dr79y5czz66KNERkbi7e1NeHg4/fr148yZM5nrpKWl8dxzz1GtWjW8vb2pWLEiTzzxBGlpaZnr9OjRgxtvvDFL23feeSeGYWTZ7/r16zEMg2+//RbI+Zqgffv28a9//YuwsDB8fHwIDw+nT58+xMXFZWn/3//+N40aNcLX15eSJUvSp08fjh49mrcO/Yd27doBZPatw+HghRdeoGrVqnh7exMZGcmTTz6Z5XVfLDExEX9/f0aOHJntuWPHjuHh4cErr7wC/P+wvDVr1jB69GhKly6Nv78/d911F6dPn862/YwZM6hTpw7e3t6UL1+eoUOHZhvu1KZNG2644Qa2b99O69at8fPzo1q1anz22WcArF69mqZNm+Lr60vNmjVZsWJFlu1zuo7lq6++4o477qB8+fJ4e3tTtWpVXnjhBZxO55U79TIWLlxIz5496dKlC8HBwSxcuDDbOheGze7Zs4fevXsTFBREaGgoI0eOJDU1Ncu6hmEwbNgwFixYQM2aNfHx8aFRo0b8+OOPOba5a9cu+vbtS4kSJbj55puBK7/npmnStm1bSpcuzalTpzLbTE9Pp27dulStWpWkpKRL9uWFoburVq2icePG+Pr6Urdu3czj/4svvqBu3bqZ2bdu3Zol+/bt24mOjqZKlSr4+PgQFhbGwIEDiYmJyfL6xowZA0DlypUzh31eyJHTNUEHDx6kV69elCxZEj8/P5o1a8Y333yTZZ28/O7L7c+wiPw/nQkSKYSWLFlCtWrVaNq0aZ63PXv2bLZlnp6e13043MGDB/nyyy/p1asXlStX5uTJk8yaNYvWrVuza9cuypcvn2X9V199FZvNxuOPP05cXByvv/469957L+vXr89cZ86cOTz88MO0aNGCUaNGcfDgQbp27UrJkiWvuiDMyMjg6NGjhIaGXnKd3377jZYtW1KhQgXGjRuHv78/n3zyCd27d+fzzz/nrrvuAs5/YG/VqhW7d+9m4MCB3HjjjZw5c4avv/6aY8eOUapUKVwuF127duXnn39m0KBBREVFsWPHDt566y1+//13vvzySwBatWrFV199RXx8PEFBQZimyZo1a7DZbPz000907doVgJ9++gmbzUbLli1zzJ6enk7Hjh1JS0tj+PDhhIWFcfz4cZYsWcK5c+cIDg4G4KWXXuKZZ56hd+/ePPjgg5w+fZpp06Zxyy23sHXr1qs6fg4cOACQ2bcPPvggH374IT179uSxxx5j/fr1vPLKK+zevfuS12UFBARw1113sWjRIiZPnpzlTMDHH3+MaZrce++9WbYZPnw4JUqU4LnnnuPw4cNMmTKFYcOGsWjRosx1JkyYwMSJE2nfvj1Dhgxh7969zJw5k40bN7JmzZoswyFjY2Pp0qULffr0oVevXsycOZM+ffqwYMECRo0axeDBg+nbty9vvPEGPXv25OjRowQGBl6yX+bNm0dAQACjR48mICCA77//nmeffZb4+HjeeOONPPcznC+G9+/fz9y5c/Hy8qJHjx4sWLCAJ598Msf1e/fuTWRkJK+88grr1q3j7bffJjY2lo8++ijLeqtXr2bRokWMGDECb29vZsyYwe23386GDRu44YYbsqzbq1cvqlevzssvv4xpmsCV33PDMPjggw+oV68egwcP5osvvgDgueee47fffmPVqlVXPKO0f/9++vbty8MPP8x9993HpEmTuPPOO3n33Xd58skneeSRRwB45ZVX6N27d5bha8uXL+fgwYMMGDCAsLAwfvvtN2bPns1vv/3GunXrMAyDHj168Pvvv/Pxxx/z1ltvZZ5pL126dI55Tp48SYsWLUhOTmbEiBGEhoby4Ycf0rVrVz777LPM3xcXXOl3X25/hkXkIqaIFCpxcXEmYHbv3j3bc7Gxsebp06czv5KTkzOfe+6550wgx6+aNWtmrnfo0CETMN94441LZoiIiDDvuOOOHJ/buHGjCZhz58697OtITU01nU5nlmWHDh0yvb29zeeffz5z2Q8//GACZlRUlJmWlpa5fOrUqSZg7tixwzRN00xPTzfLlCljNmjQIMt6s2fPNgGzdevWl81z4XXddtttmf23bds2s0+fPiZgDh8+PDPjxa/v1ltvNevWrWumpqZmLnO5XGaLFi3M6tWrZy579tlnTcD84osvsu3b5XKZpmma8+fPN202m/nTTz9lef7dd981AXPNmjWmaf5/Py9dutQ0TdPcvn27CZi9evUymzZtmrld165dzYYNG2brzx9++ME0TdPcunWrCZiffvrpJfvl8OHDpoeHh/nSSy9lWb5jxw7T09Mz2/KLzZ071wTMFStWmKdPnzaPHj1q/uc//zFDQ0NNX19f89ixY+avv/5qAuaDDz6YZdvHH3/cBMzvv/8+c1nr1q2zvJ/Lli0zAfPbb7/Nsm29evWyrHchR/v27TP72zRN89FHHzU9PDzMc+fOmaZpmqdOnTK9vLzM2267LcsxOn36dBMwP/jggyxZAHPhwoWZy/bs2WMCps1mM9etW5ct5z+PnQuZDh06lLnsnz+3Fzz88MOmn59flmOsf//+ZkRERLZ1czJs2DCzYsWKma/7u+++MwFz69atWda78Huia9euWZY/8sgjJmBu27Ytc9mF3x+bNm3KXHbkyBHTx8fHvOuuu7K1ec8992RpMy/v+axZs0zA/Pe//22uW7fO9PDwMEeNGpVlu5z6MiIiwgTMX375JXPZhffB19fXPHLkSLZ9XPjZMM2c34uPP/7YBMwff/wxc9kbb7yRbd//zNC/f//Mx6NGjTKBLD/jCQkJZuXKlc3IyMjMYy63v/ty8zMsItlpOJxIIRMfHw+Q4+QGbdq0oXTp0plf77zzTrZ1Pv/8c5YvX57la+7cuW7PfTFvb+/Mv7Y6nU5iYmIICAigZs2abNmyJdv6AwYMyHIxf6tWrYDzZ5Tg/Kx0p06dYvDgwVnWi46OztNfQr/77rvM/qtfvz6ffvop999/P6+99lqO6589e5bvv/+e3r17k5CQwJkzZzhz5gwxMTF07NiRffv2cfz4ceB839evXz/bX3rh/NAigE8//ZSoqChq1aqV2daZM2cyh4798MMPADRs2JCAgIDMoUc//fRT5tC6LVu2kJycjGma/Pzzz5l9lZMLfbNs2TKSk5NzXOeLL77A5XLRu3fvLJnCwsKoXr16ZqYrad++PaVLl6ZixYr06dOHgIAAFi9eTIUKFVi6dCkAo0ePzrLNhUkqLh4qdHG75cuXZ8GCBZnLdu7cyfbt27Nc33XBoEGDMvsbzh9LTqeTI0eOALBixQrS09MZNWpUluvqHnroIYKCgrJlCQgIoE+fPpmPa9asSUhICFFRUVnO1l7494Vj9lJ8fX0z/33hmGrVqhXJycns2bPnstvmxOFwsGjRIu6+++7M192uXTvKlCmTpc/+aejQoVkeDx8+HCDzfbqgefPmNGrUKPNxpUqV6NatG8uWLcs2fG/w4MFZHuflPR80aBAdO3Zk+PDh3H///VStWpWXX3758i/8b7Vr16Z58+aZjy+8D+3ataNSpUrZlv/z/fnne5GamsqZM2do1qwZQI6/p3Jj6dKlNGnSJHNIIJw/hgYNGsThw4fZtWtXlvWv9LsvNz/DIpKdhsOJFDIXhtEkJiZme27WrFkkJCRw8uTJHD/8wfnZwa7HxAj//JCZE5fLxdSpU5kxYwaHDh3K8oEpp6Fn//ywAlCiRAng/FAkIPMDbPXq1bOsZ7fbqVKlSq5zN23alBdffBHDMPDz8yMqKuqyQ73279+PaZo888wzPPPMMzmuc+rUKSpUqMCBAwf417/+ddn979u3j927d19yKM2F6yI8PDxo3rx55jVPP/30E61ateLmm2/G6XSybt06ypYty9mzZy9bBFWuXJnRo0czefJkFixYQKtWrejatSv33Xdf5oerffv2YZpmtr69ILcz5b3zzjvUqFEDT09PypYtS82aNTOLjCNHjmCz2ahWrVqWbcLCwggJCcl8f3Nis9m49957mTlzJsnJyfj5+bFgwQJ8fHzo1atXtvVzeyzVrFkzy3peXl5UqVIlW5bw8PBsx3twcHC2IZgX+vPCfi7lt99+4+mnn+b777/P/KPHBVdzjcd3333H6dOnadKkSZZrSdq2bcvHH3/Ma6+9lm0SlYvf66pVq2Kz2bLdgyenY6JGjRokJydz+vRpwsLCMpdXrlw5y3p5fc/nzJlD1apV2bdvH7/88kuWAuVyLn6/L7wPuXl/zp49y8SJE/nPf/6T5ZokuLr3As6/7pyGMkdFRWU+/8+hhFc6XnPzMywi2akIEilkgoODKVeuHDt37sz23IX/sbr7ZoE+Pj6kpKTk+NyFv0ReaQayl19+mWeeeYaBAwfywgsvULJkSWw2G6NGjcLlcmVb/1KzPpl/X1uQX0qVKkX79u1zvf6FrI8//jgdO3bMcZ2LP+Rdqb26desyefLkHJ//5we3m2++mZdeeonU1FR++uknnnrqKUJCQrjhhhv46aefKFu2LMBliyCAN998k+joaL766iu+++47RowYkXktSHh4OC6XK3NyhZzeh5zOSuakSZMmmbPDXcqViudL6devH2+88QZffvkl99xzDwsXLsycAOBi+X0sXaq9q9nPuXPnaN26NUFBQTz//PNUrVoVHx8ftmzZwtixY3P82biSC2d7evfunePzq1evpm3btpdt42rfl3+6VNGS27ZXrVqVOWHCjh07spzduZxreX969+7NL7/8wpgxY2jQoAEBAQG4XC5uv/32q3ovrkZucl7pZ1hEslMRJFII3XHHHbz//vts2LCBJk2aXPf9R0REZBuyccHevXsz17mczz77jLZt2zJnzpwsy8+dO3dVZ6ou7G/fvn2ZQ8fg/MQGhw4don79+nluMzcunGWy2+1XLJ6qVq2aY/F68Trbtm3j1ltvveKHw1atWpGens7HH3/M8ePHM4udW265JbMIqlGjRmYxdDl169albt26PP300/zyyy+0bNmSd999lxdffJGqVatimiaVK1emRo0aV2zrakREROByudi3b1/mX8Th/EXk586du+LxdMMNN9CwYUMWLFhAeHg4f/zxB9OmTbvqLHD+WP7nWcT09HQOHTqUpyI5r1atWkVMTAxffPEFt9xyS+byCzPo5VVSUhJfffUVd999Nz179sz2/IgRI1iwYEG2Imjfvn1Zztzs378fl8uVbUrtffv2ZWvz999/x8/P75JnMy/Iy3t+4sQJhg8fzm233YaXl1fmHx2udFxci9jYWFauXMnEiRN59tlnM5fn9JrzUiRGRERk/p78pwtDHa/2NV3uZ1hEstM1QSKF0BNPPIGfnx8DBw7k5MmT2Z7P77MjF+vcuTPHjh3LnKnsgrS0NN5//33KlCmTbfrmi3l4eGTL+emnn2ZeP5NXjRs3pnTp0rz77rukp6dnLp83b55b7+JepkwZ2rRpw6xZszhx4kS25/859fK//vUvtm3bluNMZxf6onfv3hw/fpz33nsv2zopKSmZ0wHD+TN/drud1157jZIlS1KnTh3gfHG0bt06Vq9efcWzQPHx8TgcjizL6tati81my/yre48ePfDw8GDixInZ3jPTNLNMF3y1OnfuDMCUKVOyLL9wRuyOO+64Yhv3338/3333HVOmTCE0NJROnTpdVZb27dvj5eXF22+/neX1zpkzh7i4uFxluVoX/ur/z/2mp6czY8aMq2pv8eLFJCUlMXToUHr27Jntq0uXLnz++efZpiG/+HrCCwXlxX26du3aLNfGHD16lK+++orbbrvtivfsyct7/tBDD+FyuZgzZw6zZ8/G09OTBx54wK2/63J6L3LKC2TOUJeb3zWdO3dmw4YNrF27NnNZUlISs2fPJjIyktq1a+cpZ25+hkUkO50JEimEqlevzsKFC7nnnnuoWbMm9957L/Xr18c0TQ4dOsTChQux2Ww5DoP47LPPchy+1KFDhyxnDFauXJntviAA3bt3Z9CgQXzwwQf06tWLgQMH0rBhQ2JiYli0aBE7d+7ko48+ynIhb066dOnC888/z4ABA2jRogU7duxgwYIFebp+55/sdjsvvvgiDz/8MO3atePuu+/m0KFDzJ0796rbzK133nmHm2++mbp16/LQQw9RpUoVTp48ydq1azl27FjmvY/GjBnDZ599ltlvjRo14uzZs3z99de8++671K9fn/vvv59PPvmEwYMH88MPP9CyZUucTid79uzhk08+YdmyZZlDyvz8/GjUqBHr1q3LvEcQnD8TlJSURFJS0hWLoO+//55hw4bRq1cvatSogcPhYP78+Xh4eGRev1S1alVefPFFxo8fz+HDh+nevTuBgYEcOnSIxYsXM2jQIB5//PFr6sP69evTv39/Zs+enTkkbMOGDXz44Yd07979isO1APr27csTTzzB4sWLGTJkSK6vVbpY6dKlGT9+PBMnTuT222+na9eu7N27lxkzZnDTTTdd8nq7/NCiRQtKlChB//79GTFiBIZhMH/+/Kv+sL9gwQJCQ0Np0aJFjs937dqV9957j2+++YYePXpkLj906BBdu3bl9ttvZ+3atfz73/+mb9++2c6o3nDDDXTs2DHLFNkAEydOvGK23L7nc+fO5ZtvvmHevHmZv9OmTZvGfffdx8yZMzOnuM5vQUFB3HLLLbz++utkZGRQoUIFvvvuuxzPyl2YHOKpp56iT58+2O127rzzzhyn7x43bhwff/wxnTp1YsSIEZQsWZIPP/yQQ4cO8fnnn+d4k+vLyc3PsIjk4PpORici+Wn//v3mkCFDzGrVqpk+Pj6mr6+vWatWLXPw4MHmr7/+mmXdy02RzT+mhb0wBfSlvubPn2+a5vnpuB999FGzcuXKpt1uN4OCgsy2bdtmm6b4UlJTU83HHnvMLFeunOnr62u2bNnSXLt2bbbpjy9ME3vx9K85TVVtmqY5Y8YMs3Llyqa3t7fZuHFj88cff8zW5qVcburvK+33wIEDZr9+/cywsDDTbrebFSpUMLt06WJ+9tlnWdaLiYkxhw0bZlaoUMH08vIyw8PDzf79+5tnzpzJXCc9Pd187bXXzDp16pje3t5miRIlzEaNGpkTJ0404+LisrQ3ZswYEzBfe+21LMurVatmAuaBAweyLL94iuyDBw+aAwcONKtWrWr6+PiYJUuWNNu2bWuuWLEi22v//PPPzZtvvtn09/c3/f39zVq1aplDhw419+7de9k+uzB18caNGy+7XkZGhjlx4sTMY6pixYrm+PHjs0wLbZrZp8j+p86dO2ebEvlKOS7ukwumT59u1qpVy7Tb7WbZsmXNIUOGmLGxsdmy1KlTJ9u+LnUsAebQoUOzZfrn1Mpr1qwxmzVrZvr6+prly5c3n3jiicxpnf+Z8UpTZJ88edL09PQ077///kuuk5ycbPr5+WVOaX3h98SuXbvMnj17moGBgWaJEiXMYcOGmSkpKTm+ln//+99m9erVTW9vb7Nhw4bZ+vFCm6dPn862/yu950ePHjWDg4PNO++8M9u2d911l+nv728ePHjwkn2Z2/fBNHO+PcCxY8fMu+66ywwJCTGDg4PNXr16mX/++acJmM8991yW7V944QWzQoUKps1my5Lj4imyTfP874uePXuaISEhpo+Pj9mkSRNzyZIlWdbJ7e++vPwMi8j/M0zTzeNmRERErpO77rqLHTt2ZJkFTXLvwk1iT58+fcVr8wzDYOjQoUyfPv06pRMRyT+6JkhERIqEEydO8M0333D//fdbHUVERAo4XRMkIiKF2qFDh1izZg3vv/8+drudhx9+2OpIIiJSwOlMkIiIFGqrV6/m/vvv59ChQ3z44YdZbtApIiKSE10TJCIiIiIixYrOBImIiIiISLGiIkhERERERIqVQj0xgsvl4s8//yQwMDDzJoEiIiIiIlL8mKZJQkIC5cuXv+KNhwt1EfTnn39SsWJFq2OIiIiIiEgBcfToUcLDwy+7TqEuggIDA4HzLzQoKMjSLBkZGXz33Xfcdttt2O12S7MURepf91Mfu5f6173Uv+6l/nUv9a97qX/dqyD1b3x8PBUrVsysES6nUBdBF4bABQUFFYgiyM/Pj6CgIMsPgKJI/et+6mP3Uv+6l/rXvdS/7qX+dS/1r3sVxP7NzWUymhhBRERERESKFRVBIiIiIiJSrKgIEhERERGRYqVQXxOUG6Zp4nA4cDqdbt1PRkYGnp6epKamun1fxZG7+tfDwwNPT09NsS4iIiJSjBTpIig9PZ0TJ06QnJzs9n2ZpklYWBhHjx7VB2o3cGf/+vn5Ua5cOby8vPK1XREREREpmIpsEeRyuTh06BAeHh6UL18eLy8vtxYnLpeLxMREAgICrnhzJsk7d/SvaZqkp6dz+vRpDh06RPXq1fXeiYiIiBQDRbYISk9Px+VyUbFiRfz8/Ny+P5fLRXp6Oj4+Pvog7Qbu6l9fX1/sdjtHjhzJbF9EREREirYi/2ldBYlciY4RERERkeJFn/5ERERERKRYUREkIiIiIiLFioogEREREREpVlQEFVB//fUXI0eOpFq1avj4+FC2bFlatmzJzJkzM6f8joyMxDCMbF+vvvoqAIcPH8YwDH799dds7a9atQrDMDh37ly25yIjI5kyZYobX52IiIiIiHWK7OxwhdnBgwdp2bIlISEhvPzyy9StWxdvb2927NjB7NmzqVChAl27dgXg+eef56GHHsqyfWBgoBWxRUREREQKhWJVBJmmSUqG0y1tu1wuUtKdeKY7ss025mv3yNM9ih555BE8PT3ZtGkT/v7+mcurVKlCt27dME0zc1lgYCBhYWHX/gJERERERIqJAlMEvfrqq4wfP56RI0e6bShWSoaT2s8uc0vbl7Pr+Y74eeWuq2NiYvjuu+94+eWXsxRA/+TOm76KiIiIiBR1BeKaoI0bNzJr1izq1atndRTL7d+/H9M0qVmzZpblpUqVIiAggICAAMaOHZu5fOzYsZnLL3z99NNP1zu2iIiIiEihYfmZoMTERO69917ee+89XnzxRbfuy9fuwa7nO7qlbZfLRUJ8AoFBgTkOh7tWGzZswOVyce+995KWlpa5fMyYMURHR2dZt0KFCte8PxERERGRK1n85T7OxhaI8yp5YnkRNHToUO644w7at29/xSIoLS0tSwEQHx8PQEZGBhkZGVnWzcjIwDRNXC4XLpcrc7mPp3veJNM0cHh55Hj9j2maWa7juZwqVapgGAZ79uzJkjsyMhIAX1/fzNcFEBoaSpUqVbK188/XfXEfAAQEBAAQGxtLUFBQlufOnTtHYGBgtm2sdKH//vna84vL5cI0TTIyMvDwuPaCtbC68DN08c+S5A/1r3upf91L/ete6l/3Uv+6z4of/uDkyhM48ePXm07S4IaylubJy3tsaRH0n//8hy1btrBx48Zcrf/KK68wceLEbMu/++47/Pz8sizz9PQkLCyMxMRE0tPT8yVvbiQkJFzT9na7nbZt2zJ9+nT69euX7bogh8NBeno68fHxuFwuUlNTM4vBiyUmJgKQlJSUbZ2yZctis9n4+eefKVGiRObyw4cPExcXR4UKFS7ZrpWutX9zkp6eTkpKCj/++CMOhyPf2y9sli9fbnWEIk39617qX/dS/7qX+te91L/5669TNpI3++GFwYmADI4e3syff1ib6cJtZHLDsiLo6NGjjBw5kuXLl+Pj45OrbcaPH8/o0aMzH8fHx1OxYkVuu+22bGczUlNTOXr0KAEBAblu/1qYpklCQgKBgYHXPHHBu+++S6tWrWjfvj3PPvss9erVw2azsXHjRvbv30+TJk0ICgrCZrORkZGR7Q338/MjKCgo82zPsWPHshVTderU4YEHHuDZZ58lMDCQunXrcvToUcaPH0+zZs3o0KFDgZqAIT/792Kpqan4+vpyyy23XJdjpaDKyMhg+fLldOjQAbvdbnWcIkf9617qX/dS/7qX+te91L/5749j8fw+aRt+QIy/QYNmqdze0fr+zcsf8C0rgjZv3sypU6e48cYbM5c5nU5+/PFHpk+fTlpaWrahSd7e3nh7e2dry263Z+t0p9OJYRjYbLZs1+i4w4UhWhf2eS2qV6/O1q1befnll3nqqac4duwY3t7e1K5dm8cff5xHHnkkcx/PPfcczz33XJbtH374Yd59993Mdfr27ZttH0ePHuXtt9/OnJXvyJEjhIWF0aFDB1566aUCNywsP/v3YjabDcMwcjyOiiP1g3upf91L/ete6l/3Uv+6l/o3f5yLS+XTKdsJdEKcHaIfv5GtG1YViP7Ny/4tK4JuvfVWduzYkWXZgAEDqFWrFmPHji1wH8Kvt3LlyjFt2jSmTZt2yXUOHz582TYiIyOveC3ShAkTmDBhwlUkFBEREZHiJD3DycyX1xOUZpJsM+k+oiHlSvmx1epgV8GyIigwMJAbbrghyzJ/f39CQ0OzLRcREREREeu4XC7efm09QXFOMjBp0q8WtauXLLQTThS++exEREREROS6mv3ur/geS8WFSfgdFWndrHDfksXyKbL/adWqVVZHEBERERGRf1j02R6c288B4NU4lJ531rA2UD7QmSAREREREcnR9z/+wV8rjgOQXNmPQQ/UtzhR/lARJCIiIiIi2Wz/7TS/frwPTwzOhXry6GNNCtQtVK6FiiAREREREcni2IkEvp25A2/T4JyfwYjxzfD0LDqlQ9F5JSIiIiIics3iE9L56PVNBDggwROix95EYICX1bHylYogEREREREBIMPhYvrL6whOMUm1mdwxrB4VygZYHSvfqQgSERERERFM02TqG+sJjnXgwKRh3xrUrVXK6lhuoSKoCJo3bx4hISFWx8iVCRMm0KBBgzxtYxgGX375pVvyiIiIiBRXs9/fhu+RFExMynUMp93NFa2O5DYqggqg6OhoDMPAMAy8vLyoVq0azz//PA6Hw+po+e7xxx9n5cqV+drmjz/+yJ133kn58uVVMImIiIjkwqdf7iVjcwwAHg1L0PuumhYnci8VQQXU7bffzokTJ9i3bx+PPfYYEyZM4I033rA6Vr4LCAggNDQ0X9tMSkqifv36vPPOO/naroiIiEhRtGrNMf783zEMDJIr+fLwQw2sjuR2xasIMk1IT3LfV0ZyzstNM89Rvb29CQsLIyIigiFDhtC+fXu+/vprAGJjY+nXrx8lSpTAz8+PTp06sW/fvhzbOXz4MDabjU2bNmVZPmXKFCIiInC5XKxatQrDMFi5ciWNGzfGz8+PFi1asHfv3izbzJw5k6pVq+Ll5UXNmjWZP39+lucNw2DWrFl06dIFPz8/oqKiWLt2Lfv376dNmzb4+/vTokULDhw4kLnNxcPhNm7cSIcOHShVqhTBwcG0bt2aLVu25KnvOnXqxIsvvshdd92Vp+1EREREipude2PYtGAvnhjElfBg1BNNsdmKfongaXWA6yojGV4u75ambUDIpZ588k/w8r+m9n19fYmJOX+KMjo6mn379vH1118TFBTE2LFj6dy5M7t27cJut2fZLjIykvbt2zN37lwaN26cuXzu3LlER0dnOcifeuop3nzzTUqXLs3gwYMZOHAga9asAWDx4sWMHDmSKVOm0L59e5YsWcKAAQMIDw+nbdu2mW288MILTJ48mcmTJzN27Fj69u1LlSpVGD9+PJUqVWLgwIEMGzaMb7/9NsfXmZCQQP/+/Zk2bRqmafLmm2/SuXPnbAWZiIiIiFybP08msmT6NgJdBnE+BkOfbIa9CN0L6HKKx6ssxEzTZMWKFSxbtox27dplFj/vv/8+rVq1on79+ixYsIDjx49f8tqXBx98kI8//pi0tDQAtmzZwo4dOxgwYECW9V566SVat25N7dq1GTduHL/88gupqakATJo0iejoaB555BFq1KjB6NGj6dGjB5MmTcrSxoABA+jduzc1atRg7NixHD58mHvvvZeOHTsSFRXFyJEjWbVq1SVfb7t27bjvvvuoVasWUVFRzJ49m+TkZFavXn31nSgiIiIiWSQmpTP3tY0EZkCiJ9w3pjHBgd5Wx7puiteZILvf+bMybuByuYhPSCAoMDD7KUS7X57bW7JkCQEBAWRkZOByuejbty8TJkxg5cqVeHp60rRp08x1Q0NDqVmzJrt3786xre7duzN06FAWL15Mnz59mDdvHm3btiUyMjLLevXq1cv8d7ly5QA4deoUlSpVYvfu3QwaNCjL+i1btmTq1KmXbKNs2bIA1K1bN8uy1NRU4uPjCQoKypb15MmTPP3006xatYpTp07hdDpJTk7m6NGjl+suEREREcklh8PF2y+vIyTZJM0w6Ti4HpUqBFod67oqXkWQYVzzsLRLcrnA7jzffj6Mo2zbti0zZ87Ey8uL8uXL4+l59W+Vl5cX/fr1Y+7cufTo0YOFCxdmK16ALEPpDMMAzhd3eZFTG3lpt3///sTExDB16lQiIiLw9vamefPmpKen5ymHiIiIiORsyuQNBMc4cGJS7+5qNLihtNWRrjsNhyug/P39qVatGpUqVcpSAEVFReFwOFi/fn3mspiYGPbu3Uvt2rUv2d6DDz7IihUrmDFjBg6Hgx49euQpT1RUVOb1QResWbPmsvu8GmvWrGHEiBF07tyZOnXq4O3tzZkzZ/J1HyIiIiLF1ewPtuF7MBmA0reWp32bCIsTWaN4nQkqAqpXr063bt146KGHmDVrFoGBgYwbN44KFSrQrVu3S24XFRVFs2bNGDt2LAMHDsTX1zdP+x0zZgy9e/emYcOGtG/fnv/+97988cUXrFix4lpfUhbVq1dn/vz5NG7cmPj4eMaMGZPnrImJiezfvz/z8aFDh/j1118pWbIklSpVyte8IiIiIoXFF//dR9qGM9gwMOqGcE+vKKsjWUZnggqhuXPn0qhRI7p06ULz5s0xTZOlS5dmmxnuYg888ADp6ekMHDgwz/vs3r07U6dOZdKkSdSpU4dZs2Yxd+5c2rRpc5WvImdz5swhNjaWG2+8kfvvv58RI0ZQpkyZPLWxadMmGjZsSMOGDQEYPXo0DRs25Nlnn83XrCIiIiKFxY/rjvHHN39gwyCpgg+DhzSwOpKldCaoAJo3b95lny9RogQfffTRJZ+Pjo4mOjo62/Ljx49Tt25dbrrppizL27Rpg3nRvYwaNGiQbdmQIUMYMmTIJfd78fqRkZHZll28rwkTJjBhwoTMxw0bNmTjxo1ZtunZs+f5iSfi43Pcz8Vyej0iIiIixdXufTFs+GgvvhjEBXvw2NgmxeJeQJdTvF99MZGYmMjOnTuZPn06w4cPtzqOiIiIiFwnf51O5su3t+HrMoj3gUeebIq3l86DqAgqBoYNG0ajRo1o06bNVQ2FExEREZHCJyk5g/dfXU9QBiR5QN/HGxES7GN1rAJBZWAxMG/evCsOsRMRERGRosPpcDHllXWUSDp/L6D2D99ARHiw1bEKDJ0JEhEREREpYqZM2UjI6QycmNzQuyo31itrdaQCRUWQiIiIiEgR8v687fjsTwKgVNty3NY20tpABZCKIBERERGRImLxN/tIWXf6/IM6wfS9O39vbF9UqAgSERERESkCft74J4f/+/e9gMp7M2RoQ6sjFVgqgkRERERECrk9+2NZO3c3XhjEBXkwclzTYn8voMtRz4iIiIiIFGInzySz+O2t+LkM4r1hsO4FdEUqgoqgefPmERISYnWMXJkwYQINGjTI0zaGYfDll1+6JY+IiIhIYZKUnMF7r24gKB2SPUz6PNaIkiG6F9CVqAgqgKKjozEMA8Mw8PLyolq1ajz//PM4HA6ro+W7xx9/nJUrV+Zrm6+88go33XQTgYGBlClThu7du7N379583YeIiIiI1ZxOF1NfXUeJRBfphkm7QTdQuZLuBZQbKoIKqNtvv50TJ06wb98+HnvsMSZMmMAbb7xhdax8FxAQQGhoaL62uXr1aoYOHcq6detYvnw5GRkZ3HbbbSQlJeXrfkRERESsNHXqJoJPZeDCJOpfVWhUX/cCyq1iVQSZpklyRrLbvlIcKTkuN00zz1m9vb0JCwsjIiKCIUOG0L59e77++msAYmNj6devHyVKlMDPz49OnTqxb9++HNs5fPgwNpuNTZs2ZVk+ZcoUIiIicLlcrFq1CsMwWLlyJY0bN8bPz48WLVpkO3syc+ZMqlatipeXFzVr1mT+/PlZnjcMg1mzZtGlSxf8/PyIiopi7dq17N+/nzZt2uDv70+LFi04cOBA5jYXD4fbuHEjHTp0oFSpUgQHB9O6dWu2bNmSp7773//+R3R0NHXq1KF+/frMmzePP/74g82bN+epHREREZGCas5HO/D+PRGAEreEcXv7yhYnKlyK1RVTKY4Umi5set33u77vevzsftfUhq+vLzExMcD54XL79u3j66+/JigoiLFjx9K5c2d27dqF3W7Psl1kZCTt27dn7ty5NG7cOHP53LlziY6OzjJryFNPPcWbb75J6dKlGTx4MAMHDmTNmjUALF68mJEjRzJlyhTat2/PkiVLGDBgAOHh4bRt2zazjRdeeIHJkyczefJkxo4dS9++falSpQrjx4+nUqVKDBw4kGHDhvHtt9/m+DoTEhLo378/06ZNwzRN3nzzTTp37nxNw9ni4uIAKFmy5FW3ISIiIlJQfPXtfpJ/OYUNAzMqiPv61rE6UqFTrM4EFUamabJixQqWLVtGu3btMouf999/n1atWlG/fn0WLFjA8ePHLzlZwIMPPsjHH39MWloaAFu2bGHHjh0MGDAgy3ovvfQSrVu3pnbt2owbN45ffvmF1NRUACZNmkR0dDSPPPIINWrUYPTo0fTo0YNJkyZlaWPAgAH07t2bGjVqMHbsWA4fPsy9995Lx44diYqKYuTIkaxateqSr7ddu3bcd9991KpVi6ioKGbPnk1ycjKrV6++qv5zuVyMGjWKli1bcsMNN1xVGyIiIiIFxS+b/uTg10fO3wsozJtHht9odaRCqVidCfL19GV93/VuadvlcpGQkEBgYGC2Odl9PX3z3N6SJUsICAggIyMDl8tF3759mTBhAitXrsTT05OmTf//jFZoaCg1a9Zk9+7dObbVvXt3hg4dyuLFi+nTpw/z5s2jbdu2REZGZlmvXr16mf8uV64cAKdOnaJSpUrs3r2bQYMGZVm/ZcuWTJ069ZJtlC17flxq3bp1syxLTU0lPj6eoKCgbFlPnjzJ008/zapVqzh16hROp5Pk5GSOHj16ue66pKFDh7Jz505+/vnnq9peREREpKD4/WAsP3+wG3/TIC7QxujxuhfQ1SpWRZBhGNc8LO1SXC4XDk8Hfna/fDkY27Zty8yZM/Hy8qJ8+fJ4el79W+Xl5UW/fv2YO3cuPXr0YOHChdmKFyDLUDrDMIDzrysvcmojL+3279+fmJgYpk6dSkREBN7e3jRv3pz09PQ85QAYNmwYS5Ys4ccffyQ8PDzP24uIiIgUFKdjUvhsylaC/74X0MPjm+LjXaw+yucrlY4FlL+/P9WqVaNSpUpZCqCoqCgcDgfr1///Ga2YmBj27t1L7dq1L9negw8+yIoVK5gxYwYOh4MePXrkKU9UVFTm9UEXrFmz5rL7vBpr1qxhxIgRdO7cmTp16uDt7c2ZM2fy1IZpmgwbNozFixfz/fffU7myLhQUERGRwis5NYNZr6wn+O97Ad39aCNCS+Z9pJH8P5WPhUz16tXp1q0bDz30ELNmzSIwMJBx48ZRoUIFunXrdsntoqKiaNasGWPHjmXgwIH4+ubtB2fMmDH07t2bhg0b0r59e/773//yxRdfsGLFimt9SVlUr16d+fPn07hxY+Lj4xkzZkyesw4dOpSFCxfy1VdfERgYyF9//QVAcHBwntsSERERsZLT6WLKK+spkegiwzBp/UBtqkTqXkDXSmeCCqG5c+fSqFEjunTpQvPmzTFNk6VLl2abGe5iDzzwAOnp6QwcODDP++zevTtTp05l0qRJ1KlTh1mzZjF37lzatGlzla8iZ3PmzCE2NpYbb7yR+++/nxEjRlCmTJk8tTFz5kzi4uJo06YN5cqVy/xatGhRvmYVERERcbdp0zYRfDIdFyY17qpMkxvLWR2pSNCZoAJo3rx5l32+RIkSfPTRR5d8Pjo6mujo6GzLjx8/Tt26dbnpppuyLG/Tpk22exk1aNAg27IhQ4YwZMiQS+734vUjIyOzLbt4XxMmTGDChAmZjxs2bMjGjRuzbNOzZ09cLhfx8fE57udKOUREREQKo3n/3ol9z/l7AQXfXJbOt1WxOFHRoTNBxUBiYiI7d+5k+vTpDB8+3Oo4IiIiInIFS747SPzPJwFw1Ayk33261Ud+UhFUDAwbNoxGjRrRpk2bqxoKJyIiIiLXz/qtJ9i3+BAeGCSU9WL4yEZWRypyVAQVA/PmzSMtLY1Fixbh4eFhdRwRERERuYQDR86x+v1deJkG5wJsjBzXTPcCcgP1qIiIiIhIAXAmNoVPJm/B32kQ7wUPjW+Cr68u4XcHFUEiIiIiIhZLS3cw65X1BKVBss3kX6MaUibUz+pYRZaKIBERERERC7lcLt56ZT0h8S4yMGk5IIoaVUpYHatIUxEkIiIiImKhd97ZQuCJNFyYVO4aQYubylsdqchTESQiIiIiYpEFi3Zh++38vRD9mpWmW+dqFicqHlQEiYiIiIhYYNn3h4j54QQAadX8eSC6nsWJig8VQUXQvHnzCAkJsTpGrkyYMIEGDRrkaRvDMPjyyy/dkkdERETketiy/SS/fXoQDwziStsZOeomqyMVKyqCCqDo6GgMw8AwDLy8vKhWrRrPP/88DofD6mj57vHHH2flypX52ubMmTOpV68eQUFBBAUF0bx5c7799tt83YeIiIjI1TpyLJ4Vs37D2zQ4528wcnwzPDz1sfx60sTjBdTtt9/O3LlzSUtLY+nSpQwdOhS73c748eOtjpavAgICCAgIyNc2w8PDefXVV6levTqmafLhhx/SrVs3tm7dSp06dfJ1XyIiIiJ5cS4ulYWTNhPkhHg7DBzXBH8/u9Wxip1iVXKapokrOdl9XykpOS43TTPPWb29vQkLCyMiIoIhQ4bQvn17vv76awBiY2Pp168fJUqUwM/Pj06dOrFv374c2zl8+DA2m41NmzZlWT5lyhQiIiJwuVysWrUKwzBYuXIljRs3xs/PjxYtWrB3794s28ycOZOqVavi5eVFzZo1mT9/fpbnDcNg1qxZdOnSBT8/P6Kioli7di379++nTZs2+Pv706JFCw4cOJC5zcXD4TZu3EiHDh0oVaoUwcHBtG7dmi1btuSp7+688046d+5M9erVqVGjBi+99BIBAQGsW7cuT+2IiIiI5Kf0dAczXllPUKpJis2k2/D6lCvtb3WsYqlYnQkyU1LYe2Mjt+7jZA7Lam7ZjOF3bTe78vX1JSYmBjg/XG7fvn18/fXXBAUFMXbsWDp37syuXbuw27P+JSEyMpL27dszd+5cGjdunLl87ty5REdHY7P9fx381FNP8eabb1K6dGkGDx7MwIEDWbNmDQCLFy9m5MiRTJkyhfbt27NkyRIGDBhAeHg4bdu2zWzjhRdeYPLkyUyePJmxY8fSt29fqlSpwvjx46lUqRIDBw5k2LBhlxyelpCQQP/+/Zk2bRqmafLmm2/SuXPnbAVZbjmdTj799FOSkpJo3rz5VbUhIiIicq1cLhdTX99A8DknGZjcdH9NatcItTpWsVWsiqDCyDRNVq5cybJlyxg+fHhm8bNmzRpatGgBwIIFC6hYsSJffvklvXr1ytbGgw8+yODBg5k8eTLe3t5s2bKFHTt28NVXX2VZ76WXXqJ169YAjBs3jjvuuIPU1FR8fHyYNGkS0dHRPPLIIwCMHj2adevWMWnSpCxF0IABA+jduzcAY8eOpXnz5jzzzDN07NgRgJEjRzJgwIBLvt527dpleTx79mxCQkJYvXo1t9xyS677bceOHTRv3pzU1FQCAgJYvHgxtWvXzvX2IiIiIvlp1qxf8TuWiguT8M4Vad083OpIxVqxKoIMX19qbtnslrZdLhfxCQkEBQZmObtyYb95tWTJEgICAsjIyMDlctG3b18mTJjAypUr8fT0pGnTppnrhoaGUrNmTXbv3p1jW927d2fo0KEsXryYPn36MG/ePNq2bUtkZGSW9erV+/9pGcuVKwfAqVOnqFSpErt372bQoEFZ1m/ZsiVTp069ZBtly5YFoG7dulmWpaamEh8fT1BQULasJ0+e5Omnn2bVqlWcOnUKp9NJcnIyR48evVx3ZVOzZk1+/fVX4uLi+Oyzz+jfvz+rV69WISQiIiLX3aLP9uDadg4Ar8ah9Oxaw9pAUsyKIMO45mFpl+RyYXM4sPn5ZSuCrkbbtm2ZOXMmXl5elC9fHk/Pq3+rvLy86NevH3PnzqVHjx4sXLgwW/ECZBlKZxgGcL64y4uc2shLu/379ycmJoapU6cSERGBt7c3zZs3Jz09PU85LsyqB9CoUSM2btzI1KlTmTVrVp7aEREREbkWP/z0B3+tOI4nBsmVfXn8gfpWRxKK2cQIhYm/vz/VqlWjUqVKWQqgqKgoHA4H69evz1wWExPD3r17L3uW48EHH2TFihXMmDEDh8NBjx498pQnKioq8/qgC9asWZPvZ1bWrFnDiBEj6Ny5M3Xq1MHb25szZ85cc7sul4u0tLR8SCgiIiKSOzt2nWHrwn14YhBb0pNHH2ua+QdhsVaxOhNUFFSvXp1u3brx0EMPMWvWLAIDAxk3bhwVKlSgW7dul9wuKiqKZs2aMXbsWAYOHIhvHofojRkzht69e9OwYUPat2/Pf//7X7744gtWrFhxrS8pi+rVqzN//nwaN25MfHw8Y8aMyXPW8ePH06lTJypVqkRCQgILFy5k1apVLFu2LF+zioiIiFzK8ROJfDtjO/6mQayfwYinmuGpewEVGHonCqG5c+fSqFEjunTpQvPmzTFNk6VLl2abGe5iDzzwAOnp6QwcODDP++zevTtTp05l0qRJ1KlTh1mzZjF37lzatGlzla8iZ3PmzCE2NpYbb7yR+++/nxEjRlCmTJk8tXHq1Cn69etHzZo1ufXWW9m4cSPLli2jQ4cO+ZpVREREJCcJiel89PpG/B2Q4An9n2hMkL+X1bHkH3QmqACaN2/eZZ8vUaIEH3300SWfj46OJjo6Otvy48ePU7duXW666aYsy9u0aZPtXkYNGjTItmzIkCEMGTLkkvu9eP3IyMhsyy7e14QJE5gwYULm44YNG7Jx48Ys2/Ts2fP8xBPx8Tnu52Jz5sy57PMiIiIi7uJwuJj+8jqCUkxSDJNOQ+tRMSzQ6lhyEZ0JKgYSExPZuXMn06dPZ/jw4VbHERERESmSTNPk7UkbCDrrwIFJg77VqR9V2upYkgMVQcXAsGHDaNSoEW3atLmqoXAiIiIicmVz3t+G9+FkTEzKdKhA+1aVrI4kl6DhcMXAvHnzrjjETkRERESu3uKvfidt89nzD+qX4J5/1bI2kFyWzgSJiIiIiFyDn9ce449vz9/YPaGiD4883MDaQHJFKoJERERERK7S7t9jWD9/7/l7AYV48OgTTbHZ9BG7oNM7JCIiIiJyFU6eSuLradvwcRnE+sDQJ5vibfewOpbkgoogEREREZE8SkpO54PXNhCQAYkeJvc+3ogSQT5Wx5JcUhEkIiIiIpIHLqeL6a+sJyjJJNUwaffwDVQOD7Y6luSBiiARERERkTyY9tYmAk5n4MAkqldVbqpX1upIkkcqgoqgefPmERISYnWMXJkwYQINGjTI0zaGYfDll1+6JY+IiIjI5Xz44Q489ycCUKJNGJ3bRVobSK6KiqACKDo6GsMwMAwDLy8vqlWrxvPPP4/D4bA6Wr57/PHHWblypdvaf/XVVzEMg1GjRrltHyIiIlI8LPn2APFrTwGQUTuIfn3qWJxIrpZullpA3X777cydO5e0tDSWLl3K0KFDsdvtjB8/3upo+SogIICAgAC3tL1x40ZmzZpFvXr13NK+iIiIFB/rN59g/1eHsWMQV86LccNutDqSXINidSbINE0y0pxu+3Kk57zcNM08Z/X29iYsLIyIiAiGDBlC+/bt+frrrwGIjY2lX79+lChRAj8/Pzp16sS+fftybOfw4cPYbDY2bdqUZfmUKVOIiIjA5XKxatUqDMNg5cqVNG7cGD8/P1q0aMHevXuzbDNz5kyqVq2Kl5cXNWvWZP78+VmeNwyDWbNm0aVLF/z8/IiKimLt2rXs37+fNm3a4O/vT4sWLThw4EDmNhcPh9u4cSMdOnSgVKlSBAcH07p1a7Zs2ZLn/ktMTOTee+/lvffeo0SJEnneXkREROSCA4fO8dOcXdgxOBtk49FxzXQvoEKuWJ0JcqS7mD1y9XXf76CprbF7X9uc8b6+vsTExADnh8vt27ePr7/+mqCgIMaOHUvnzp3ZtWsXdrs9y3aRkZG0b9+euXPn0rhx48zlc+fOJTo6OssP8FNPPcWbb75J6dKlGTx4MAMHDmTNmjUALF68mJEjRzJlyhTat2/PkiVLGDBgAOHh4bRt2zazjRdeeIHJkyczefJkxo4dS9++falSpQrjx4+nUqVKDBw4kGHDhvHtt9/m+DoTEhLo378/06ZNwzRN3nzzTTp37pytILuSoUOHcscdd9C+fXtefPHFPG0rIiIicsGZs8l8+tYWAl0G57xgyLim+HoXq4/QRZLewQLONE1WrlzJsmXLGD58eGbxs2bNGlq0aAHAggULqFixIl9++SW9evXK1saDDz7I4MGDmTx5Mt7e3mzZsoUdO3bw1VdfZVnvpZdeonXr1gCMGzeOO+64g9TUVHx8fJg0aRLR0dE88sgjAIwePZp169YxadKkLEXQgAED6N27NwBjx46lefPmPPPMM3Ts2BGAkSNHMmDAgEu+3nbt2mV5PHv2bEJCQli9ejW33HJLrvrsP//5D1u2bGHjxo25Wl9EREQkJ6mpDt57ZSNB6ZDkYdLr0UaUKulrdSzJB8WqCPL0sjFoamu3tO1yuUhIiCcwMCjb6VFPr7yfLl2yZAkBAQFkZGTgcrno27cvEyZMYOXKlXh6etK0adPMdUNDQ6lZsya7d+/Osa3u3bszdOhQFi9eTJ8+fZg3bx5t27YlMjIyy3r/vHamXLlyAJw6dYpKlSqxe/duBg0alGX9li1bMnXq1Eu2Ubbs+eki69atm2VZamoq8fHxBAUFZct68uRJnn76aVatWsWpU6dwOp0kJydz9OjRy3VXpqNHjzJy5EiWL1+Oj49uWCYiIiJXx+Vy8fZr6wlKcJJumNw8IIoalUOsjiX5pFgVQYZhXPOwtEtxuQw80zywe3vkyxjRtm3bMnPmTLy8vChfvjyenlf/Vnl5edGvXz/mzp1Ljx49WLhwYbbiBcgylM4wDOD8L4C8yKmNvLTbv39/YmJimDp1KhEREXh7e9O8eXPS09Nztf/Nmzdz6tQpbrzx/y9WdDqd/Pjjj0yfPp20tDQ8PNxzDIiIiEjRMXP6FvxPpOHCpHLXSFo0Lm91JMlHxaoIKkz8/f2pVq1atuVRUVE4HA7Wr1+fORwuJiaGvXv3Urt27Uu29+CDD3LDDTcwY8YMHA4HPXr0yFOeqKgo1qxZQ//+/TOXrVmz5rL7vBpr1qxhxowZdO7cGTh/ZufMmTO53v7WW29lx44dWZYNGDCAWrVqMXbsWBVAIiIickUL/7MLdsUD4Nu8NN07VbU4keQ3FUGFTPXq1enWrRsPPfQQs2bNIjAwkHHjxlGhQgW6det2ye2ioqJo1qwZY8eOZeDAgfj65m0865gxY+jduzcNGzakffv2/Pe//+WLL75gxYoV1/qSsqhevTrz58+ncePGxMfHM2bMmDxlDQwM5IYbbsiyzN/fn9DQ0GzLRURERC723feHObPqBB4YpFTzZ2h/3WqjKNLcfoXQ3LlzadSoEV26dKF58+aYpsnSpUuzzQx3sQceeID09HQGDhyY5312796dqVOnMmnSJOrUqcOsWbOYO3cubdq0ucpXkbM5c+YQGxvLjTfeyP3338+IESMoU6ZMvu5DREREJCdbd5xk56cH8MDgXGk7jz56k9WRxE10JqgAmjdv3mWfL1GiBB999NEln4+OjiY6Ojrb8uPHj1O3bl1uuinrD3SbNm2y3cuoQYMG2ZYNGTKEIUOGXHK/F68fGRmZbdnF+5owYQITJkzIfNywYcNss7r17NkTl8tFfHx8jvu5klWrVuVpfRERESl+/jgWz/J3f8PfNIj1N3h0fDM8PHS+oKjSO1sMJCYmsnPnTqZPn87w4cOtjiMiIiJSoMQnpLNg0mb8nRBvh4Fjb8Lf7/IjbKRwUxFUDAwbNoxGjRrRpk2bqxoKJyIiIlJUZWQ4eefldQSlmqTYTO4cVo/yZQKsjiVupuFwxcC8efOuOMROREREpLgxTZO3J20kKNaBA5NG99XghpqlrI4l14HOBImIiIhIsTRnzjZ8jiRjYlL2tgq0bVHR6khynRT5IiivF9FL8aNjREREpPhZ/PXvpG06C4DRoAR9etSyOJFcT0W2CLowXXRycrLFSaSgu3CMXGmKcRERESka1qw/zh9LjwKQEO7DkEENrA0k112RvSbIw8ODkJAQTp06BYCfnx+GYbhtfy6Xi/T0dFJTU7HZimxtaRl39K9pmiQnJ3Pq1ClCQkLw8PDIl3ZFRESk4Pr9QCzrPtyDDwZngz14YmxTfXYrhopsEQQQFhYGkFkIuZNpmqSkpODr6+vWYqu4cmf/hoSEZB4rIiIiUnSdiUlm8dStBLgMznnDI082xduuP4IWR0W6CDIMg3LlylGmTBkyMjLcuq+MjAx+/PFHbrnlFg2rcgN39a/dbtcZIBERkWIgNc3Be69uICgdEj1M7h7diNBgH6tjiUUsLYJmzpzJzJkzOXz4MAB16tTh2WefpVOnTvm6Hw8PD7d/0PXw8MDhcODj46MiyA3UvyIiInK1XC6YOWkzQQku0gyTWx6sTbWIEKtjiYUsHQAZHh7Oq6++yubNm9m0aRPt2rWjW7du/Pbbb1bGEhEREZEiZPtWb/z/SseJSZWuETRvWM7qSGIxS88E3XnnnVkev/TSS8ycOZN169ZRp06dbOunpaWRlpaW+Tg+Ph44P1TK3cPdruTC/q3OUVSpf91Pfexe6l/3Uv+6l/rXvdS/7rXo0z2UOuUFgFfTUO5oH6G+zkcF6fjNSwbDLCA3SXE6nXz66af079+frVu3Urt27WzrTJgwgYkTJ2ZbvnDhQvz8/K5HTBEREREpJA4f9cDY6YsHBsdD02naJO3KG0mhlZycTN++fYmLiyMoKOiy61peBO3YsYPmzZuTmppKQEAACxcupHPnzjmum9OZoIoVK3LmzJkrvlB3y8jIYPny5XTo0EHXrLiB+tf91Mfupf51L/Wve6l/3Uv96x6/7Ynhhxm/4WUa/OnvYOyEFvj6eFsdq8gpSMdvfHw8pUqVylURZPnscDVr1uTXX38lLi6Ozz77jP79+7N69eoczwR5e3vj7Z394LXb7ZZ3+gUFKUtRpP51P/Wxe6l/3Uv9617qX/dS/+afE38l8t2sXfibBmd9oV6zFHx9vNW/blQQjt+87N/yO0N5eXlRrVo1GjVqxCuvvEL9+vWZOnWq1bFEREREpBBKTEpn3usb8XdAvKfJfY81xMfL6lRS0FheBF3M5XJlGfImIiIiIpIbTqeLd15ZT1CySYph0nFIXcLLBlgdSwogS4fDjR8/nk6dOlGpUiUSEhJYuHAhq1atYtmyZVbGEhEREZFCaPpbmwg4k4EDkzp3V+XGOmUKxKxlUvBYWgSdOnWKfv36ceLECYKDg6lXrx7Lli2jQ4cOVsYSERERkULmow934Lk/EYCQNmF0ahNpbSAp0CwtgubMmWPl7kVERESkCPj2u4PErT2FDYO02oEM7ZP9fpMi/1TgrgkSEREREcmtLb/+xd7Fh7BhEBvmxcihjayOJIWAiiARERERKZSOHI1n5Xu7sJsGZwNsjB7XFA8PfbyVK9NRIiIiIiKFTlxcGh+/uRk/J8TZ4aFxTfDz0X2AJHdUBImIiIhIoeLIcDLzlfUEppok20y6jahPWCk/q2NJIaIiSEREREQKDdM0eXvSBgLPOcjApPF9NalTPdTqWFLIqAgSERERkUJjzpzteB9JwcQkrGMF2rYItzqSFEIqgkRERESkUPhqyT5SN50BwKxfgj531bI4kRRWKoJEREREpMBbu/FPDi35AwOD+AreDH24gdWRpBBTESQiIiIiBdqBw+dYM3c3dgxig2yMHtsUm00fY+Xq6egRERERkQLr7LlUPnlrC74ugzgvGDy+Kd5enlbHkkJORZCIiIiIFEhp6Q7efXk9QWmQZDPpNbohpUr4Wh1LigAVQSIiIiJS4LhcLt5+fQPB8U7SMWkZHUX1yBJWx5IiQkWQiIiIiBQ4s9/bht+xVFyYRHSpRMsm5a2OJEWIiiARERERKVC++Hofjq1nAbA3CqVHl+oWJ5KiRkWQiIiIiBQYv2z4kz+Wnp8KO7GiD4MeqGd1JCmCVASJiIiISIFw4PA5fpn391TYwR6MGtNEU2GLW+ioEhERERHLxZ5L5dMLU2F7w5DxTTQVtriNiiARERERsVR6hpN3X1lPYBok20x6jmxIaIimwhb3UREkIiIiIpYxTZNpb2wgKM5JBiZN+9eiRhVNhS3upSJIRERERCwzZ842fP5IwcSkfKdwbmlawepIUgyoCBIRERERS3y9dD+pm2IAMBqUoHe3mhYnkuJCRZCIiIiIXHcbtpzg4NdHMDCIL+/NkEENrI4kxYiKIBERERG5rg4fjWP1nPNTYZ8NtPHo2KaaCluuKx1tIiIiInLdxCek8/GbW/BzQpwXPDyuCT7emgpbri8VQSIiIiJyXTgcLt55eR1BqSbJNpPuwxtQJtTP6lhSDKkIEhEREZHrYtpbGwmKdeDApFHfGtSuXtLqSFJMqQgSEREREbf78MMdeB1IAqBU+/K0u7mixYmkOFMRJCIiIiJu9b/lh4hfewoAR+0g7u0ZZXEiKe5UBImIiIiI2/y68xS7vjiIDYO4sl4MH3aj1ZFEVASJiIiIiHscP5HAd+/uxNs0iPU3GDlOU2FLwaCjUERERETyXWJSOh++sQl/B8TbYeATN+Hva7c6lgigIkhERERE8pnL6WLaq+sJTjZJNUw6D6lL+bIBVscSyaQiSERERETy1fRpmwk6nYETkzq9qlK/dmmrI4lkoSJIRERERPLNwkW78NiTAEDQzWW5vV2ktYFEcqAiSERERETyxfc//8HpH04AkFbNn+j7brA4kUjOVASJiIiIyDXb9XsMWxbuwxODuJKejBx1k9WRRC5JRZCIiIiIXJPTMSl8NX0bvi6DOB+DYeOb4eGpj5lScOnoFBEREZGrlpbuYPZr6wlKhyQPk3sfa0RQoJfVsUQuS0WQiIiIiFwVl8vF1EkbCIl3kYHJLQNrE1ExyOpYIlekIkhERERErsqcudvx/yMVE5NKd1SiWaNyVkcSyRUVQSIiIiKSZ1//7wCpG2MA8GhQgh53Vrc4kUjuqQgSERERkTzZsv0kB746jA2DhPLePDyogdWRRPJERZCIiIiI5NrxEwmsmP0bXqbBuQAbI59ois2mj5RSuOiIFREREZFcSUrJ4MM3NuHvgARPGPjETfj6eFodSyTPVASJiIiIyBW5XC6mvbqe4GSTNMOk0yM3UK6Mv9WxRK6KiiARERERuaKZM7cSeDIdFya1/lWF+rXLWB1J5KqpCBIRERGRy/rsy72wIw4Av2al6dy+ssWJRK6NiiARERERuaS1G//k2P+OAZAc6csD0fUsTiRy7VQEiYiIiEiODv0Rx8/zdmPH4FywByMfa2J1JJF8oSJIRERERLKJT0jn48lb8HMaxHvBw+Ob4GX3sDqWSL5QESQiIiIiWTgdLt55dR3BqSYpNpPuw+tTKsTX6lgi+UZFkIiIiIhkMX3aJoJiHDgwaXhPdaKqh1odSSRfqQgSERERkUwLFu3Cc28iACVah3Frq0oWJxLJfyqCRERERASAVWuOcuaHEwCkVQ+g3z11LE4k4h4qgkRERESEvQdi2bTgdzwxiCvpyciRja2OJOI2KoJEREREirmzcal8/vZWfF0Gcd7wyLimeHjqY6IUXTq6RURERIoxh8PFzFc3EJwGKTaTux+9kZAgb6tjibiViiARERGRYmzKWxsJiT0/E1zTfrWoGhlidSQRt1MRJCIiIlJMfbhgJ74HkgAIa1+eVs0qWJxI5PpQESQiIiJSDK386SjnfjoJgLNWIHf3jLI4kcj1oyJIREREpJjZeyCWrR+fnwkuPtTOsOGNrI4kcl2pCBIREREpRmLj0zJngov3gaHjmmLz0EdCKV50xIuIiIgUE06Hi5mvric4DZJtJr1G3UhQoJfVsUSuOxVBIiIiIsXEtLc3EXz2/ExwTe6vSTXNBCfFlIogERERkWJg4Se7sP+eCECptuVo3Tzc4kQi1lERJCIiIlLE/bTuOKe/PwFAWnV/7r27tsWJRKylIkhERESkCDt4JI71H+3BE4NzJTwZMfImqyOJWE5FkIiIiEgRlZCYzqK3tuDrMojzgiHjmuDpqY9/IvopEBERESmCXE4X019bT1CqSYrNpMeIhpQM9rE6lkiBoCJIREREpAiaOWMrQaczcGJS/+5q1KpWwupIIgWGiiARERGRIubzr36H3+IACGhZhg6tIyxOJFKwqAgSERERKUI2bPmLo98eBSAp0peB99e1OJFIwaMiSERERKSIOPpnAqvm7MKOQWyQjZGjNROcSE5UBImIiIgUASkpDuZP2oS/E+LtMGhsE7y9PK2OJVIgqQgSERERKeRcLhdvv76e4GSTNMOk85C6lAn1szqWSIGlIkhERESkkHvv/e0EnEjDhUn1uyKpX7u01ZFECjQVQSIiIiKF2JJlB0nfEgOAvVEod95W1eJEIgXfVQ0UPXToED/99BNHjhwhOTmZ0qVL07BhQ5o3b46Pj27CJSIiInI9bNt1mt+/PIQ3BgnlvXnigXpWRxIpFPJUBC1YsICpU6eyadMmypYtS/ny5fH19eXs2bMcOHAAHx8f7r33XsaOHUtEhOajFxEREXGX02dT+HbmDgJNg3N+Bo+OaYLNpkE+IrmR6yKoYcOGeHl5ER0dzeeff07FihWzPJ+WlsbatWv5z3/+Q+PGjZkxYwa9evXK98AiIiIixV2Gw8Xs1zYQkgFJHib9xtyEn6/d6lgihUaui6BXX32Vjh07XvJ5b29v2rRpQ5s2bXjppZc4fPhwfuQTERERkYtMe2sjIXFOMjBpNaA2FcsFWh1JpFDJdRF0uQLoYqGhoYSGhl5VIBERERG5tIWLduF9IAmAcrdVoHnjchYnEil8rmrg6Lx583Jc7nA4GD9+/LXkEREREZFLWLPuOGd+OAFAeo0A7u5Ry+JEIoXTVRVBI0aMoFevXsTGxmYu27t3L02bNuXjjz/Ot3AiIiIict6RY/Gsnb8HDwxiS3gwfERjqyOJFFpXVQRt3bqVY8eOUbduXZYvX84777zDjTfeSK1atdi2bVt+ZxQREREp1pKTM1j45mZ8nQZxXjBkbBM8PTUTnMjVuqr7BFWtWpU1a9YwatQobr/9djw8PPjwww+555578jufiIiISLFmmibvvLGBoBSTVMPkzqH1CQ3xtTqWSKF21X9C+Oabb/jPf/5D8+bNCQkJYc6cOfz555/5mU1ERESk2Ptgznb8TqThwqRmj8rUrVnK6kgihd5VFUEPP/wwvXr1YuzYsfz0009s374dLy8v6tatyyeffJLfGUVERESKpe9WHCJ50xkAPG4sSZcOVSxOJFI0XFURtGbNGtavX89jjz2GYRiEhYWxdOlSnn/+eQYOHJjfGUVERESKnT2/x7Dz84PYMDgX5sXgB+tbHUmkyLiqImjz5s3Ur5/9B3Ho0KFs3rw51+288sor3HTTTQQGBlKmTBm6d+/O3r17ryaSiIiISJERF5fGV9O34W0anPU1GDmmKTabJkIQyS9X9dPk7e19yedq1qyZ63ZWr17N0KFDWbduHcuXLycjI4PbbruNpKSkq4klIiIiUug5nS5mvraegHRI9DDpO/pGAvztVscSKVJyPTvc7bffzoQJE2jWrNll10tISGDGjBkEBAQwdOjQy677v//9L8vjefPmUaZMGTZv3swtt9yS22giIiIiRca707cQeNaBA5Ob+tagasVgqyOJFDm5LoJ69erFv/71L4KDg7nzzjtp3Lgx5cuXx8fHh9jYWHbt2sXPP//M0qVLueOOO3jjjTfyHCYuLg6AkiVL5vh8WloaaWlpmY/j4+MByMjIICMjI8/7y08X9m91jqJK/et+6mP3Uv+6l/rXvdS/7vXP/v16yQHYff7zjX/L0rRqEqZ+v0Y6ft2rIPVvXjIYpmmauV05LS2NTz/9lEWLFvHzzz9nFi2GYVC7dm06duzIAw88QFRUVJ5Du1wuunbtyrlz5/j5559zXGfChAlMnDgx2/KFCxfi5+eX532KiIiIFBR/nbKRstkPOwbHS6TTtFnalTcSkUzJycn07duXuLg4goKCLrtunoqgi8XFxZGSkkJoaCh2+7WNVR0yZAjffvstP//8M+Hh4Tmuk9OZoIoVK3LmzJkrvlB3y8jIYPny5XTo0OGa+0KyU/+6n/rYvdS/7qX+dS/1r3tlZGSw+KsVHF8TgL8DzgYYjJrYHB8vD6ujFQk6ft2rIPVvfHw8pUqVylURlOvhcDkJDg4mOPjax6kOGzaMJUuW8OOPP16yAILzEzLkNCmD3W63vNMvKEhZiiL1r/upj91L/ete6l/3Uv+6R0aGkwPrfQl1QLynycAxTQn097E6VpGj49e9CkL/5mX/eSqC3n777RyXBwcHU6NGDZo3b56X5jBNk+HDh7N48WJWrVpF5cqV87S9iIiISGH33pvrCU31IN0wafdAHSqU9bc6kkiRl6ci6K233spx+blz54iLi6NFixZ8/fXXl5zY4GJDhw5l4cKFfPXVVwQGBvLXX38B54sqX1/fvEQTERERKXQWfbABz+NOAMq2LUvThmEWJxIpHvJ0n6BDhw7l+BUbG8v+/ftxuVw8/fTTuW5v5syZxMXF0aZNG8qVK5f5tWjRojy/EBEREZHCZNeBk3jPfYXA+COU+Ws1/7rt0pcEiEj+uqZrgv6pSpUqvPrqqwwcODDX21zDnAwiIiIihVZsYipbHxnJjaf3kJB0kpNDH8Lmr2FwItdLns4EXUmlSpUyh7SJiIiISHZOl8nnQ5/hxiPbyLB5Ej5lEq5SubuUQETyR74WQTt27CAiIiI/mxQREREpUv7z8ixarl8CgH3sU5RufpPFiUSKnzwNh4uPj89xeVxcHJs3b+axxx6jf//++RJMREREpKhZ8dlKblj4DgBnu91Dy/598nSXexHJH3kqgkJCQjAMI8fnDMPgwQcfZNy4cfkSTERERKQo2b19H34vjMfL5eDPG5rQ7pXcTyYlIvkrT0XQDz/8kOPyoKAgqlevjo+PD6dOnaJ8+fL5Ek5ERESkKIiNieOPIUOplJbAydIVafXBdAxbvl6VICJ5kKciqHXr1pd9ftu2bdx44404nc5rCiUiIiJSVDicLlYNHE6tmKMkeAdQ54NZeAUFWh1LpFjTnyBERERE3GjJmBeptXcjGYYH/q9NonT1ylZHEin2VASJiIiIuMnqOZ9Qc+nHAMQ8NJI6t19+VI2IXB8qgkRERETc4PdfthL41ksA7Lu5M21HP2RxIhG5IE/XBG3fvv2yz+/du/eawoiIiIgUBbF/nebkyOGUcqRzsGIUnaa/bHUkEfmHPBVBDRo0wDAMTNPM9tyF5ZeaQltERESkOHCmZ7C+/2AiEmI4FVCKJnNnYvfxtjqWiPxDnoqgQ4cOuSuHiIiISJHw3YiniDyyixQPL0q+NZXS4WWtjiQiF8lTERQREeGuHCIiIiKF3s/T5hG56r8AnBk+ntta3WhxIhHJSZ4mRnj99ddJSUnJfLxmzRrS0tIyHyckJPDII4/kXzoRERGRQuL3VesImjkJgJ3te3Hb4D4WJxKRS8lTETR+/HgSEhIyH3fq1Injx49nPk5OTmbWrFn5l05ERESkEDh37AQxo0dhdznZXbUh3d561upIInIZeSqCLp4QIacJEkRERESKE2dqGpuiHyYkOY7jwWG0nDMdL3uerjgQketM9wkSERERuQbfDxtHhWP7SLT7UHrK25QNK2l1JBG5AhVBIiIiIldp7dQ5hP/8P1wYnB71NPWb17U6kojkQp7P1b7//vsEBAQA4HA4mDdvHqVKlQLIcr2QiIiISFG2/4dfCJj1FgDbO/bhngf+ZXEiEcmtPBVBlSpV4r333st8HBYWxvz587OtIyIiIlKUxR8/wZnHRhPscvJbtUb0mPSU1ZFEJA/yVAQdPnzYTTFERERECgdXWhqbogdTLjmOY8FhtHpvKt52D6tjiUge5KkISk1NZcWKFXTp0gU4P2X2P+8T5OnpyfPPP4+Pj0/+phQREREpIH4c+RTljv5Oot2HEpOnULZcqNWRRCSP8lQEzZs3j2+++SazCJo+fTp16tTB19cXgD179hAWFsbo0aPzP6mIiIiIxX6dPZ+yq77BhcHRwWPp0bK+1ZFE5CrkaXa4BQsWMGjQoCzLFi5cyA8//MAPP/zAG2+8waeffpqvAUVEREQKgj/Xb8Y25XUANrbuwV2P3G1xIhG5Wnkqgvbv30/duv8/9aOPjw822/830aRJE3bt2pV/6UREREQKgNTTZ/hj2AjsLgc7IurRc8qzGIZhdSwRuUp5Gg537ty5LNcAnT59OsvzLpcry/MiIiIihZ3pcLB+4COUSTjLnwGlaThzKgG+XlbHEpFrkKczQeHh4ezcufOSz2/fvp3w8PBrDiUiIiJSUKx/8kXK7NtBiocX5vOvUr1KmNWRROQa5akI6ty5M88++yypqanZnktJSWHixInccccd+RZORERExEq/L/qS4K8XAbD9vhG079zC4kQikh/yNBzuySef5JNPPqFmzZoMGzaMGjVqALB3716mT5+Ow+HgySefdEtQERERkevp3K69JL04AR9gTePbiX5igNWRRCSf5KkIKlu2LL/88gtDhgxh3LhxmKYJgGEYdOjQgRkzZlC2bFm3BBURERG5XhwJCex6aAglMtLYFVadrtNexNMjTwNoRKQAy1MRBFC5cmX+97//cfbsWfbv3w9AtWrVKFmyZL6HExEREbneTNNk3aBRhMac4LRvMJFT36JMCX+rY4lIPspzEXRByZIladKkSX5mEREREbHcttenEbr1FzJsHpwcPYFe9ataHUlE8pnO64qIiIj87fiK1djnvQvAL52i6XlfR4sTiYg7qAgSERERAZKPHefEmMexmSbrazTn/pdH6oaoIkWUiiAREREp9lzp6WwdMAT/lEQOhoTTasbr+HvbrY4lIm6iIkhERESKvQ2PP0PJo/tIsPvi/dLrVA0vZXUkEXEjFUEiIiJSrO398D8Ef/c1Lgx2DniM9rc2tDqSiLiZiiAREREptmK27ST19ZcB+LF5V/qNvMfiRCJyPagIEhERkWLJERfH74OH4uXMYHuF2vSc+pxuiCpSTOgnXURERIod0+Vi3aCRhMSe4qRfCapPnUzpIF+rY4nIdaIiSERERIqdX19/m9Bt60m3eRIzZiI33hBhdSQRuY5UBImIiEixcmzlj3h9OBuAtV0G0KNPe4sTicj1piJIREREio3kY39y4vG/b4haswX9XhihG6KKFEMqgkRERKRYMNPT2fzAEAJSEjgUUoHWM17Hz9vT6lgiYgEVQSIiIlIsrH1iAqWO/E6C3Refl9+gcoVQqyOJiEVUBImIiEiRt2fB55T432IAdkc/Srt2uiGqSHGmIkhERESKtJhde0l95QUA1jTtwn2P3mdxIhGxmoogERERKbIciYnseegRvB1p7A6rQfe3J+Jh00QIIsWdiiAREREpkkzTZM3g0ZSM+ZMYnyCqvP0WpYL9rI4lIgWAiiAREREpkrZMmUWZTT/hMGyceew5GtSrYnUkESkgVASJiIhIkXP05/V4vTcdgE0d76XbfZ0sTiQiBYmKIBERESlSUk6d4dioR/F0Ofm1aiPufu0J3RBVRLJQESQiIiJFhul0svaBoYQkxvJnYGmaz3xTN0QVkWxUBImIiEiR8fMzr1Ju33ZSPex4vvAakZXKWh1JRAogFUEiIiJSJOz+ahklv1gAwN6+Q2l9e3OLE4lIQaUiSERERAq9mIN/kPjsU9gw2VK/Lb3GD7I6kogUYCqCREREpFBzpKax/YEhBKQlcSi0Ep3efV03RBWRy1IRJCIiIoXaDyOeJOzEQRLsvoRPnULJEgFWRxKRAk5FkIiIiBRaG977mPAflwJwasST1GscZXEiESkMVASJiIhIoXR062/Yp7wKwK9t7uKOB/9lcSIRKSxUBImIiEihkxKfwMFHhuHjTOf38Ci6T5mgG6KKSK6pCBIREZFCxTRNVj04ijKxfxHjG0yDd9/G18fL6lgiUoioCBIREZFC5YdX3iFy+y84DBs89zIR1cKtjiQihYyKIBERESk0fluxhlLz3wVgf48B3Ny9ncWJRKQwUhEkIiIihcLZ46c4N3YMdtPJnpo30fX50VZHEpFCSkWQiIiIFHjODAfrHhxKyaRY/goqQ+v3puDhoY8xInJ19NtDRERECrz/jX2Jyod2kuZhp/SbkylZpqTVkUSkEFMRJCIiIgXa2v98Q+TSRQCcevBRbmjVyOJEIlLYqQgSERGRAuuP3QfwePlZbJjsbdKe2x4dYHUkESkCVASJiIhIgZSSlMLuwcMJTE/mWOkIbpvxmtWRRKSIUBEkIiIiBdL/ho6n0slDJHr5UXPWdPwC/KyOJCJFhIogERERKXCWvf0htdYtA8A57jkq1a5mcSIRKUpUBImIiEiBsnPNr5SeNRmAg7f3plnfrhYnEpGiRkWQiIiIFBhnT8dyavQofJ3pHImoze1vPGN1JBEpglQEiYiISIHgdLpY9dAoysWdJNYvhCYfzMDD7ml1LBEpglQEiYiISIHw1bNvEbVnAw7DRvBrb1CyQlmrI4lIEaUiSERERCy35svvqf7FBwCcvn8IdTrcbHEiESnKVASJiIiIpY7sPwYTn8TTdHG4Xgvajh9qdSQRKeJUBImIiIhlUlLS2P7wMEqmxHGyRDnavD8FwzCsjiUiRZyKIBEREbGEaZp8NeI5qh3fS6qnF1VnTMM3KNDqWCJSDKgIEhEREUt88+4i6v/0FQCO0U9SsWEdixOJSHGhIkhERESuu1/X7qDsjNcAONauKzcNvNviRCJSnKgIEhERkevq9JlznBr9KAEZqfxZoTq3vvW81ZFEpJhRESQiIiLXTYbDycpBj1Ex9jjxvoE0+mAGNm9vq2OJSDGjIkhERESum88mTKP+rl9wGgYhL79GSES41ZFEpBhSESQiIiLXxcovvqfOF+8DcO7eh6jZqa3FiUSkuFIRJCIiIm637/ejeL3wFHaXk+N1m9LyqVFWRxKRYkxFkIiIiLhVQnIavw0ZQamUc5wpUZZW77+tG6KKiKVUBImIiIjbmKbJl8OfpebxPaR5elFt5jt4BwdZHUtEijkVQSIiIuI2X7y9kMZrvj7/YMxTlGugG6KKiPUsLYJ+/PFH7rzzTsqXL49hGHz55ZdWxhEREZF8tP7HrUS+9wYAf93WnQb9e1ucSETkPEuLoKSkJOrXr88777xjZQwRERHJZ3+eiCF+zGj8HGn8VakmrSdNtDqSiEgmTyt33qlTJzp16mRlBBEREclnaRlOfhr0KPXi/iLOL5jG82Zh8/KyOpaISCZLi6C8SktLIy0tLfNxfHw8ABkZGWRkZFgVKzPDP79L/lL/up/62L3Uv+6l/nWvvPbvp2PfoNG+jTgMGyVffx2f0iX13lyGjl/3Uv+6V0Hq37xkMEzTNN2YJdcMw2Dx4sV07979kutMmDCBiROzn05fuHAhfn5+bkwnIiIiuXFkyyHaffIeHqaLbR264du+udWRRKSYSE5Opm/fvsTFxREUdPlZKAtVEZTTmaCKFSty5syZK75Qd8vIyGD58uV06NABu91uaZaiSP3rfupj91L/upf6171y27+7tu8neWA/QtISOd64Nbd8oPsB5YaOX/dS/7pXQerf+Ph4SpUqlasiqFANh/P29sbb2zvbcrvdbnmnX1CQshRF6l/3Ux+7l/rXvdS/7nW5/j13LonjI0dTOS2Rk2Uq0WbWZDx1HVCe6Ph1L/WvexWE/s3L/nWfIBEREbkmTpfJ0kGPUfnMYZK8/Ljh/Zl4+muYuogUXJaeCUpMTGT//v2Zjw8dOsSvv/5KyZIlqVSpkoXJREREJLc+nTiNhttX48LA74VXKFWjitWRREQuy9IiaNOmTbRt2zbz8ejRowHo378/8+bNsyiViIiI5NbKL76n9qezATjbZwCtut1mcSIRkSuztAhq06YNBWReBhEREcmjPb8dwvuFJ7G7nPxZtyntnnvc6kgiIrmia4JEREQkz87FJfP7IyMITYnjdMnytJozTTPBiUihoSJIRERE8sTpMlky+Amqn9xPst2HqPdm4hUUaHUsEZFcUxEkIiIiefLJCzNotHUlAD7PvUDpOjUsTiQikjcqgkRERCTXViz+gdqL3gXgTK/+1OnZxeJEIiJ5pyJIREREcuX3PUfweX48Xi4Hf9a5iZsnPmF1JBGRq6IiSERERK4oJdXBgWGjCE2J40zJctz8wXQMmz5GiEjhpN9eIiIicllOl0niwm+ofvIAyXYfar33Lt7BQVbHEhG5aiqCRERE5LIWvziTlnvX4sLAZ+JLmghBRAo9FUEiIiJySSs/XU6dz94D4EyvftTp0dniRCIi105FkIiIiORoz/b9+L70FHbTyZ6q9Wj+zGirI4mI5AsVQSIiIpLN2Zh4Dg8ZSonUBE6WCof+PTEMw+pYIiL5QkWQiIiIZJHucLLigZFExPxBorc/NWbPwObtZXUsEZF8oyJIREREsvjs8Veou2cdTsNG4KuTKF090upIIiL5SkWQiIiIZPr63U+o/7+FAMQ/OIJandpYG0hExA1UBImIiAgAG1ZtpsL0l7FhcrzV7TQfPcjqSCIibqEiSERERPjj8AmSHh+FnyONPyNq0W76q5oIQUSKLBVBIiIixVxCYgpbHniEsMQznA0Mpdn897B5e1sdS0TEbVQEiYiIFGMul8mSh5+g5vE9pHh6E/HuDPzLlLI6loiIW6kIEhERKcY+f24qDTavwIWB7ZnnCW9Uz+pIIiJupyJIRESkmFox/79EffYeAKfvHkiDu7tanEhE5PpQESQiIlIM7Vy3neDXn8PDdPFH49a0nvCY1ZFERK4bFUEiIiLFzF/HTnJ6xDACMlI4Xr4a7d6bopngRKRYUREkIiJSjKQkp7IpejBh8ac5G1CSRvPfw+7rY3UsEZHrSkWQiIhIMeF0uvhm4KNUPXZ+Jriwd2ZQokKY1bFERK47FUEiIiLFxJdPvk6dX1fhxMB8+gWqNq1vdSQREUuoCBIRESkGls1aRK2vPgLg9H2DadTnTosTiYhYR0WQiIhIEbdh2RrKvP0yNkyOtLydNk8NtzqSiIilVASJiIgUYQd/O4Bj3Gh8nOn8UaUuHWa+rpngRKTYUxEkIiJSRJ09HcuBhx6mREo8f5UsT6t/z8bDy251LBERy6kIEhERKYLSUtP5+f5BhJ89TpxPIFHz3sevZIjVsURECgQVQSIiIkWMy+Vi6UOPUf3wTtI87ARPeZuwGpWtjiUiUmCoCBIRESlilj71OrU2rsCFQcrYCUS1aWZ1JBGRAkVFkIiISBHy/Yx/U3XxhwAcu2cQzfv1sDiRiEjBoyJIRESkiNjw5QpKTX8VgN9bdaHjc6OsDSQiUkCpCBIRESkC9q7fjvHME9hdTg7UbEyXGa9YHUlEpMBSESQiIlLInTjwB6cfGUxARgp/lKtKu3+/i4fd0+pYIiIFloogERGRQizuTCw7+z9IaFIsJ4PK0Hj+HPwC/a2OJSJSoKkIEhERKaTSU1JZc+9DhJ85SpxPIJXnvEdoeFmrY4mIFHgqgkRERAohl9PJsv7DqXzkN1I8vPB/cyoRdWtYHUtEpFBQESQiIlIIfTv8aapt/xmHYSPl6Zeoe2tzqyOJiBQaKoJEREQKme+ee5Mq338JwPFBj9Hyni7WBhIRKWRUBImIiBQiq6bNo+Ki9wHY060/tz860OJEIiKFj4ogERGRQmLdx/+l1Iw3ANh9cxe6vfKExYlERAonFUEiIiKFwPb//YjPi0/hYbrYU7clXd99FZtN/xsXEbka+u0pIiJSwO1b+yvpY0bh7cxgf+V6dPpoOp6eHlbHEhEptFQEiYiIFGDHfvudM488jH9GCofDqtL64/fx8fWxOpaISKGmIkhERKSAOnPoKIeiHyAkJZ5jJSvQeOFcgkICrY4lIlLoqQgSEREpgOL+Os3Ovv0plXCGk4GlqDV/LqXLl7Y6lohIkaAiSEREpIBJijnHprvvp2zsCWJ8gyn/3vtUrFrR6lgiIkWGiiAREZECJCU+gTV396f8ySPEe/sTOO1dajSoaXUsEZEiRUWQiIhIAZGWnMLquwdS8djvJNl98HxzOvVvbmB1LBGRIkdFkIiISAHgSE3j+z4PEHFoJykeXjhefotG7ZtZHUtEpEhSESQiImIxZ3oG3937MJG/byXN5knyhNdodmcbq2OJiBRZKoJEREQs5HI4WN5vCJV/W0+GzYOz417k5l63Wx1LRKRIUxEkIiJiEZfDwfLoYUT8ugaHYeOvR5+jXb9uVscSESnyVASJiIhYwOV0snzgSCptWo3TsPHHsCe57aFeVscSESkWVASJiIhcZy6nk+8GjqTShu9xGjYOPDyWO4bea3UsEZFiQ0WQiIjIdXShAIpYvxInBgcfGkO3Uf2sjiUiUqx4Wh1ARESkuHA5HCwfMIKIjT/gxODQw0/Q9dFoq2OJiBQ7KoJERESuA1dGBsujh1Np8/lrgA4//AR3jupvdSwRkWJJRZCIiIibudLTWXH/ECpt++V8ATRkHF1G3G91LBGRYktFkIiIiBtlpKbxfd+HqLRrIxmGB0eGPcWdQ++xOpaISLGmIkhERMRN0hKTWNXnASrt30a6zZM/H32OOx/qaXUsEZFiT0WQiIiIGyTHxrGmd38qHd1LqocXZ598iU73drE6loiIoCJIREQk3yWcPM2G3v0JP3mIJE8fUp5/nVt7dLA6loiI/E1FkIiISD46e+QYO+7pR/mzJ4j38oc3ptKqY0urY4mIyD+oCBIREcknx3fs5fCAByiTGMMZ3xACps2g4c0NrY4lIiIXUREkIiKSD/b9tIGzw4dSMjWRvwJLU3bWe9S+sabVsUREJAc2qwOIiIgUdlu/WEbCkEEEpSbyR2hFqny8UAWQiEgBpjNBIiIi12DNzH8T+Par2E0n+8KjaLHgfUqVLWl1LBERuQwVQSIiIlfBNE1+eG4S5T75AIDdtZrS8d8z8A/wsziZiIhciYogERGRPHKmZ7Bs8Bgq/7IMgB0t76D7u6/iZdf/VkVECgP9thYREcmDpNh4frxvEJUPbMOFwZ4eA+n10mMYhmF1NBERySUVQSIiIrl0Yt9hdkU/RGTMMdJsnpwe+RT/eriP1bFERCSPVASJiIjkwq6Va4l7fBTlU+I55xOI/dXJdLj9ZqtjiYjIVVARJCIicgU/zVpI4NuvEuLM4HjJClR9710q16lmdSwREblKKoJEREQuwelwsnT0BKp99xkA+6rUp9WH71KidIiluURE5NqoCBIREclB7Kmz/DxgKNUO/ArA7jbd6PL2C3h52a0NJiIi10xFkIiIyEX2rNvGiZEjqRZ3kjSbJ2cGP0aPEdFWxxIRkXyiIkhEROQfVs36mKBprxLmSOesXwjBb06hfdumVscSEZF8pCJIREQESE1J5X/Dn6bmz98AcLhiFDd98A6lKpazOJmIiOQ3FUEiIlLsHdx5gL1DR1Dz5EEA9rXtTqcpE7F7e1mcTERE3EFFkIiIFGvL53xO8NSXiUxPJsnuS9rjT9G1/7+sjiUiIm6kIkhERIqlpMRklg5/mhvWfgvAn2UiqPXudCrU1v1/RESKOhVBIiJS7Gz7+VdOPTGGG84eA+Bw2660f+t57D7eFicTEZHrQUWQiIgUG2kZTv47YQrVFs8j3OUg3jsA27hn6HRPV6ujiYjIdaQiSEREioXd2/ax97Gx1Dm2G4A/qtaj8YzJhEZUsDiZiIhcbyqCRESkSMtwOPnva7Op9PFsajpSSfewE99/MLeNGYJhGFbHExERC6gIEhGRImv75t0cHP8MUX/8BsCf5atRe+ok6tetaXEyERGxkoogEREpcuITU1gy8W1qLV1ITWc66TZPYu8eQNsnh2Oz262OJyIiFrNZHQDgnXfeITIyEh8fH5o2bcqGDRusjiQiIoWQaZp8/+UPrLmtGw3/Ow9fZzp/RtSizKLPaPPcaBVAIiICFIAiaNGiRYwePZrnnnuOLVu2UL9+fTp27MipU6esjiYiIoVIbEwSn/cbSdlxQ4k8e5QkL18Shj5Ou28/p5yGv4mIyD9YPhxu8uTJPPTQQwwYMACAd999l2+++YYPPviAcePGWZwu93atnI/P76vZFrsGw+YBhoFp2AAbGAA2TMMAwwAMMM7Xn+fXubCMf/z7wroXlvGPC3j//p7jOmR7bGKc/+/v58ws6/5/W/9/efDfWTKXZc1mGsbfWYys2xsGNsOGYbOBzYbNMDAMGzabDcMwMGweGDYbNsPj/PM2G4bNA5vN+Pu759/fbRgeHtg8Ljz2wGWCmXKWpLMn8PL2xuZhx8PTE09PO4bNDn/3uYgUP2fOJbDghVE0X76ZwPQ0AI43bk3zNybiX66sxelERKQgsrQISk9PZ/PmzYwfPz5zmc1mo3379qxduzbb+mlpaaSlpWU+jo+PByAjI4OMjAz3B76MjB1f0jFpNSRZGqNI6w6w59LPO0wbDsMDJx448cSBBw7D8/y/DTsuwwOHYcdpeOI0vHDazn932eznvzy8MD28MT28MD18wNMbPH2w2c9/N+y+eHj5YPPyxcPuh4ePH57efnh6+2P38cfu44ePXyC+Pn7YPCw/yZpnF36GrP5ZKqrUv/kv3eHklR+/IuijSdy5ORGAk6XCKfvUeFq3bwWov/OLjl/3Uv+6l/rXvQpS/+Ylg6VF0JkzZ3A6nZQtm/UvdWXLlmXPnuyfdl955RUmTpyYbfl3332Hn5+f23LmRqpHNU7b7RimiQ0XNlwAGLgunD/BMM3z3//x9f/nZTj/b/Pv75hZ2r/w2MDEJPOczN//zrougGFevOzi9rI/Z+SwrnEh1z/XMbO+rv9/DWaWnFm+/n7t5/vFxPb3vw3I7K/zj008Mh+fX8cDF55/L7Mbzmyv9QJP4/x68I8fAPOi79eB0zRIwZsUfEjFm1TDm1TDhzS8Sbf5kGb4kGHzJd3mQ4aHLw4PH1wevpgevph2Xwy7L4bdD08vX2xefths17egWr58+XXdX3Gj/r12pgmrY4/zfdoyXL4HKd3EpPnvBlta1KfSLf8iLj2B35cutTpmkaTj173Uv+6l/nWvgtC/ycnJuV7X8uFweTF+/HhGjx6d+Tg+Pp6KFSty2223ERQUZGEyyMjowPLly+nQoQN2XXibr0zTJC09g++Wr6Ddre2w2Wy4nA6cjgycGRk4HBk4HA6cjvTzj53puDIycDozcGWk4XJm4HSkYzoycDnSMJ0ZmI50cKRhOs9/N5zp55c5084/dqSCMx2bMw2bMw0PVxoernQ8XWnYXWnYzTS8zDS8zHR8SMXHTMfLcADgYZgEkEoAqf94EX9/v3QNd0mJpi+Jhj8pNn9SPAJJ8wwi3SsIp1cwpk8Ihn9J7P4l8QkqhW9QaQJLliGwZBnsPgF52k9GRoaOYTdS/1470zT5eNsmZmybQbJ9G/gCpieRkXdS+ZuHOLt2i/rXTXT8upf6173Uv+5VkPr3wiix3LC0CCpVqhQeHh6cPHkyy/KTJ08SFhaWbX1vb2+8vb2zLbfb7ZZ3+gUFKUtRYhgGnjYI8PUpsP1rOjNITU4kOSmBtKR4UpMTSE9JICMlEUdKPI7UJFxpCZhpiRhpCRgZSdgyEvHMSMLTkYiXIwkfVxK+riQCzCR8jXQAAowUAkgB1xkyT3SlXDlPCl7EG0EkeQSTbC9JmlcJnL6lwL8U9uCy+IaUJSC0PCXKhONfohz83a86ht1L/Zt3pmny8a8beHvzDJLsW8AOpmlQ1fcW3mg/hhqhEZlDINS/7qX+dS/1r3upf92rIPRvXvZvaRHk5eVFo0aNWLlyJd27dwfA5XKxcuVKhg0bZmU0kTwzPOz4BpbAN7BEvrSXnpZGYtwZEuPOkpJwlrSEs2QkncWZFIsr5RxGSixGWhz2tFi80+PwdcYR4Eog2IzHy3DiSzq+5hlwnAHHgfOFU1zO+9ro481G70AOmDWY//ERQv2DKe0fTLnAEoQHhVK1VGmqlAzFz7tQnTyWQs7lMpm3eRWzt39Akuev8Pf/2yp6NeOF1qNpVD7K0nwiIlJ4Wf6JZvTo0fTv35/GjRvTpEkTpkyZQlJSUuZscSLFlZe3NyXLVKBkmQp52s7pdHE2Lpb4s3+RePYkqXGnSI8/hSvxDEbSaTxSY/BOjcHfcZZgZywliWeNrw9zQvyAY2C+B4mc//rHSVrT5YHh8sduBOHrEUSwvQShvqUoF1CGSsFlqVYynBqlKlA+IAwfT5/87AopZpLT03nz5y9ZfHAhGfYD4Hn+zE9F7yY812oUzcJvsDqiiIgUcpYXQXfffTenT5/m2Wef5a+//qJBgwb873//yzZZgojkjoeHjZIlQylZMhSoc8X1E1PTKfXbp9x8dBVHz8bg8vcj2ZFMijOJdFcSDpLAcGDYnGCLJ4N4MoD4DDiaAb/GA39elMH0x89WihLeZSjnX57I4HBqlYogqnQEFYPCCfKy9ho+KZgOxJzi1Z/+zfqY/2J6nvl72JsHVXxa8czNj3BTuM78iIhI/rC8CAIYNmyYhr+JWCTAx4v7Gt3L3fV6s3TpUjp37pxtTG1yRjLH4mLYF/MXh86e5Fj8aU4knuZMyhnOpcWQ5DxLhnEOwzMOw5aB00giwUwiIfUIf6TC+hjg4P+352H6EehZltI+5akUWInqJStTP6wqtUpVIdQn9B/3xJKizuVy8fH2n5m7/T/85VyHYcs4/38mlx+NSnTimVseomrJvJ0NFRERuZICUQSJSMHmZ/ejRik/apSqeMl1nC6TU/Gp7I85ze7Tf7D/7DH+iD/OqeQTnMs4Rap5BsMei80zEaeRzDnnIc4lHWJf0hpW/gXsOt+OYXoTYAujtE84EUER1C5VjUYVahAVWpUAr7zNeCcF1/YTfzBtwyI2nvkOp+dfwPl7SHu5KnB7xX/xxM19CfbxtziliIgUVSqCRCRfeNgMyoX4Ui6kEq2qVsr2vMPp4kRcKr+fjmHnyUPsO3uYownHOJN6nATnXzg9zhdJGGkkmEdISDnCwZQ1/HAS+O18G55mMEEeFQjzq0jVkCrcULo6TSvWpHJIODaj8N2gtrg5FneG6esXs+rYdyTa9mIY5vnrfVx2Kno3Z1CDvnSPaqEzgSIi4nYqgkTkuvD0sFGxpB8VS/pxa82KwC1Znk9IzeDAmXNs/fMgv53ez6G4I5xMOUq840+cnqeweSbiMOI464rjbOIudiXCf48BWwGXFz6UpaRXOOEBkdQoWZWGYTVpWrEGwT7W3ki5uNt7+hgfbP2GNSdWcc7chWG4wOP8jZf9XFVpW6EzI5r1pHxQSaujiohIMaIiSEQKhEAfOw3CS9MgvDTQNMtz8akZ7Dpxkk1/7mVPzAGOxB/mdNpRklwnMO1nMGzppHKUPx1H+fPcWjacg38fPD+jmM1ZkgCjPKV9KhIRVJlaoVVpVKEGdctW0JTfbpDhzGDFgc18tnsF28+uI9V2JPM5wwBPRwVuLPV/7d17bBR1v8fxz+z2sl16oYUWKC1QCg8cDpbyULnJCaAoqFHwRPQPI5cYIqQYCAkiRuk/KsQYJRKDJJ6AIRAwaOnRoIhEwOegyNUUlPq0gi0tpTfbbre02+7M+QNo0qBYkHW6O+9XsiE7s5BPv1l299PfzOx05efN1z8Hj7AxKQDAyfgEAKDXS/REa3JWhiZnZUh6oNu+upZWfX+pVGeqS/Rzwy+q9F/Ubx2X1K5qGe42WVH18qlevo5i/VIvfV0v6WfJCsbJHUxTgjtdA+MyNSxxmMakZmtC+kiNTEuWN4aXx54wLVP/V16s/y35l05dOa6azh8l1/Vv871+hGJM5zDlpNynZ8Y+qgdGjOVwNwCA7XiXBxDW+sd79cjoHD0yOqfbdtM0VfZbtY5VnNfZ2p91oemiqlvL1RysUqfRIMN9Vab7VzXpVzUFvlVJnbS/TtJPktmRpKhgqhKjBmlA3GANTRqi/+ifpZwB2RqR2k/J3mjHfpC/0lKvA2Un9K+K0yr5rVj1nf+WdaP0SJLrWsHsa4zR5IHTtHD8bN0z8I8vqAEAgB0oQQAiksvl0sh+6RrZL13S/d32tXW26eyVMp28XKKf6sp0sflX1bZVqMWslmm0yhXdJDO6SY0qVWOHVFInfVkn6bxkdvaR0Zkij5GqvtFpSvMOUkbCYI1IztDo1EyNTE1VWoJHbld4l6S2zjadqS7VsUs/6lzdz7rQ9G/VBS6q09XQ/YEuyTJj5DWzNarveD2YdZ/+e8xkxXti7AkOAEAPUIIAOI4nyqO8wf+pvMHdv0zWsiw1tjfqXG2ZfrhcqvMNF1TRXKHatkq1mDUyjRa5ovxSlF/tqtAVSVfapOI2SbW6fphdrKzOJEWrr7yuFCVEpygltr/6xaWorcanxlPHNbz/IA1LSVX/PnHyRLtsWVVq62xTRXO1fqq5pNKGS7rYdElVLVWqa7usps4qdRq/SYbV/S/duABfR38lu0doZNIY/deQe/X4mAlK8cb97T8DAAB3ihIEANcZhqFkT7KmZeZpWmbeTftbAi36pbFcZ2su6Of6X1XeXKlqf5UaA7VqNesUNPwy3O0y3DUKqkY+ST5JVe2S2iXFSMfO/4+kaxdtUDBOlhknt9VH0YZXMS6vPG6v4qL6yOP2yOOOU2xUrDzumGt/RsUoNipaUS6XJEOGJEuWLMtUUEGZVqfagwFd7WjT1c6r127BVrV1+uXv9KnNbFHAalanfJKr/Y8Hcb3sWEGPooIDlRw9REMTsjV+4Bg9lP1PjR6Q6tjDAQEAkYESBAA9FB8Tr5y0McpJG/O7+1s7WnW5pVrn6ypU2lCpyuYrutJaq4a2OjUFGuQL1Mt0t8o0Wq9/R06rDLXKUr0CkgKSWiQpeP12txnXb9dZZpQUTFKsUhTv7q9+nkFK75Ouf/TLUl76PzR+cIbiuEAEACAC8e4GAHeJN9qr7OThyk4eftO+jo4O7du3T4888ojklhrbGlXd0qDK5jrV+BtV62/Ub1d9amr3yRfw62rnVbUH2xQItitgtqvT7FSn1aGgGZQl8/q/aunaipBLkksuRcltRCvaFaNoV6xi3XHyuPrIG91HfT2J6hfXVwP6pGhQQqqGJg1QZt8U9fVy7g4AwHkoQQDwN4t2RSvVm6pUb6ruSRtldxwAABzH9ecPAQAAAIDIQQkCAAAA4CiUIAAAAACOQgkCAAAA4CiUIAAAAACOQgkCAAAA4CiUIAAAAACOQgkCAAAA4CiUIAAAAACOQgkCAAAA4CiUIAAAAACOQgkCAAAA4CiUIAAAAACOQgkCAAAA4CiUIAAAAACOQgkCAAAA4CiUIAAAAACOQgkCAAAA4ChRdgf4KyzLkiQ1NzfbnETq6OhQa2urmpubFR0dbXeciMN8Q48ZhxbzDS3mG1rMN7SYb2gx39DqTfO90QludIRbCesS5PP5JEmZmZk2JwEAAADQG/h8PiUlJd3yMYbVk6rUS5mmqaqqKiUkJMgwDFuzNDc3KzMzUxUVFUpMTLQ1SyRivqHHjEOL+YYW8w0t5htazDe0mG9o9ab5WpYln8+n9PR0uVy3PusnrFeCXC6XMjIy7I7RTWJiou1PgEjGfEOPGYcW8w0t5htazDe0mG9oMd/Q6i3z/bMVoBu4MAIAAAAAR6EEAQAAAHAUStBdEhsbq4KCAsXGxtodJSIx39BjxqHFfEOL+YYW8w0t5htazDe0wnW+YX1hBAAAAAC4XawEAQAAAHAUShAAAAAAR6EEAQAAAHAUShAAAAAAR6EEhVh7e7tyc3NlGIbOnDljd5yI8fjjj2vIkCHyeDwaNGiQnn32WVVVVdkdKyJcvHhRzz33nLKyshQXF6fs7GwVFBQoEAjYHS1ivP7665o6daq8Xq/69u1rd5yw995772nYsGHyeDyaNGmSvv/+e7sjRYwjR47oscceU3p6ugzD0N69e+2OFDHWr1+ve++9VwkJCUpLS9O8efNUUlJid6yIsnnzZuXk5HR9ieeUKVP0+eef2x0rIm3YsEGGYWjlypV2R+kxSlCIvfjii0pPT7c7RsSZOXOmPvroI5WUlOjjjz9WWVmZnnzySbtjRYTz58/LNE1t2bJF586d0zvvvKP3339fL7/8st3RIkYgEND8+fO1bNkyu6OEvd27d2vVqlUqKCjQqVOnNG7cOM2ePVs1NTV2R4sIfr9f48aN03vvvWd3lIhz+PBh5efn67vvvtOBAwfU0dGhhx56SH6/3+5oESMjI0MbNmzQyZMndeLECd1///2aO3euzp07Z3e0iHL8+HFt2bJFOTk5dke5PRZCZt++fdbo0aOtc+fOWZKs06dP2x0pYhUVFVmGYViBQMDuKBHpzTfftLKysuyOEXG2bt1qJSUl2R0jrE2cONHKz8/vuh8MBq309HRr/fr1NqaKTJKswsJCu2NErJqaGkuSdfjwYbujRLTk5GTrgw8+sDtGxPD5fNbIkSOtAwcOWNOnT7dWrFhhd6QeYyUoRK5cuaIlS5Zo+/bt8nq9dseJaA0NDdqxY4emTp2q6Ohou+NEpKamJqWkpNgdA+gmEAjo5MmTmjVrVtc2l8ulWbNm6dtvv7UxGXD7mpqaJInX2hAJBoPatWuX/H6/pkyZYneciJGfn69HH3202+twuKAEhYBlWVq0aJGWLl2qvLw8u+NErDVr1qhPnz7q16+fysvLVVRUZHekiFRaWqpNmzbp+eeftzsK0E1dXZ2CwaAGDBjQbfuAAQNUXV1tUyrg9pmmqZUrV+q+++7T2LFj7Y4TUYqLixUfH6/Y2FgtXbpUhYWFGjNmjN2xIsKuXbt06tQprV+/3u4od4QSdBteeuklGYZxy9v58+e1adMm+Xw+rV271u7IYaWn871h9erVOn36tL788ku53W4tWLBAlmXZ+BP0brc7X0mqrKzUnDlzNH/+fC1ZssSm5OHhTuYLANK136afPXtWu3btsjtKxBk1apTOnDmjY8eOadmyZVq4cKF+/PFHu2OFvYqKCq1YsUI7duyQx+OxO84dMSw+NfZYbW2t6uvrb/mY4cOH66mnntKnn34qwzC6tgeDQbndbj3zzDP68MMPQx01LPV0vjExMTdtv3TpkjIzM3X06FGWuf/A7c63qqpKM2bM0OTJk7Vt2za5XPzO5Fbu5Pm7bds2rVy5Uo2NjSFOF5kCgYC8Xq/27NmjefPmdW1fuHChGhsbWR2+ywzDUGFhYbdZ469bvny5ioqKdOTIEWVlZdkdJ+LNmjVL2dnZ2rJli91RwtrevXv1xBNPyO12d20LBoMyDEMul0vt7e3d9vVGUXYHCCepqalKTU3908e9++67eu2117ruV1VVafbs2dq9e7cmTZoUyohhrafz/T2maUq6dkly/L7bmW9lZaVmzpypCRMmaOvWrRSgHvgrz1/cmZiYGE2YMEEHDx7s+mBumqYOHjyo5cuX2xsO+BOWZemFF15QYWGhDh06RAH6m5imyWeFu+CBBx5QcXFxt22LFy/W6NGjtWbNml5fgCRKUEgMGTKk2/34+HhJUnZ2tjIyMuyIFFGOHTum48ePa9q0aUpOTlZZWZleffVVZWdnswp0F1RWVmrGjBkaOnSo3nrrLdXW1nbtGzhwoI3JIkd5ebkaGhpUXl6uYDDY9R1iI0aM6Hq9QM+sWrVKCxcuVF5eniZOnKiNGzfK7/dr8eLFdkeLCC0tLSotLe26f+HCBZ05c0YpKSk3vdfh9uTn52vnzp0qKipSQkJC13lsSUlJiouLszldZFi7dq0efvhhDRkyRD6fTzt37tShQ4e0f/9+u6OFvYSEhJvOX7txnna4nNdGCULY8Xq9+uSTT1RQUCC/369BgwZpzpw5euWVVxQbG2t3vLB34MABlZaWqrS09KbSztGzd8e6deu6HRY7fvx4SdLXX3+tGTNm2JQqPD399NOqra3VunXrVF1drdzcXH3xxRc3XSwBd+bEiROaOXNm1/1Vq1ZJunbI4bZt22xKFRk2b94sSTf9n9+6dasWLVr09weKQDU1NVqwYIEuX76spKQk5eTkaP/+/XrwwQftjoZegHOCAAAAADgKB/oDAAAAcBRKEAAAAABHoQQBAAAAcBRKEAAAAABHoQQBAAAAcBRKEAAAAABHoQQBAAAAcBRKEAAAAABHoQQBAAAAcBRKEAAAAABHoQQBAAAAcBRKEAAgLNXW1mrgwIF64403urYdPXpUMTExOnjwoI3JAAC9nWFZlmV3CAAA7sS+ffs0b948HT16VKNGjVJubq7mzp2rt99+2+5oAIBejBIEAAhr+fn5+uqrr5SXl6fi4mIdP35csbGxdscCAPRilCAAQFi7evWqxo4dq4qKCp08eVL33HOP3ZEAAL0c5wQBAMJaWVmZqqqqZJqmLl68aHccAEAYYCUIABC2AoGAJk6cqNzcXI0aNUobN25UcXGx0tLS7I4GAOjFKEEAgLC1evVq7dmzRz/88IPi4+M1ffp0JSUl6bPPPrM7GgCgF+NwOABAWDp06JA2btyo7du3KzExUS6XS9u3b9c333yjzZs32x0PANCLsRIEAAAAwFFYCQIAAADgKJQgAAAAAI5CCQIAAADgKJQgAAAAAI5CCQIAAADgKJQgAAAAAI5CCQIAAADgKJQgAAAAAI5CCQIAAADgKJQgAAAAAI5CCQIAAADgKP8PUOKfQga1e2UAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GELU function\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "# Interval boundaries\n",
    "intervals = np.array([-4, -2, 0, 2, 4])\n",
    "\n",
    "# Number of data points to generate for each interval\n",
    "num_points = 100\n",
    "\n",
    "# Store polynomials and errors\n",
    "polynomials = []\n",
    "max_errors = []\n",
    "\n",
    "# Loop through intervals\n",
    "for i in range(len(intervals) - 1):\n",
    "    # Generate data points\n",
    "    x = np.linspace(intervals[i], intervals[i+1], num_points)\n",
    "    y = gelu(x)\n",
    "\n",
    "    # Fit a 3rd-degree polynomial (adjust degree as needed)\n",
    "    coeffs = np.polyfit(x, y, 3) #3rd Degree polynomial\n",
    "    polynomial = np.poly1d(coeffs)\n",
    "    polynomials.append(polynomial)\n",
    "\n",
    "    # Calculate maximum absolute error\n",
    "    y_approx = polynomial(x)\n",
    "    max_error = np.max(np.abs(y - y_approx))\n",
    "    max_errors.append(max_error)\n",
    "\n",
    "    print(f\"Interval [{intervals[i]}, {intervals[i+1]}]:\")\n",
    "    print(f\"  Polynomial Coefficients: {coeffs}\")\n",
    "    print(f\"  Maximum Absolute Error: {max_error}\")\n",
    "\n",
    "#Plot to visually inspect the fit.\n",
    "plt.figure(figsize=(10,6))\n",
    "x_plot = np.linspace(-4,4, 500)\n",
    "y_plot = gelu(x_plot)\n",
    "plt.plot(x_plot, y_plot, label='GELU')\n",
    "\n",
    "\n",
    "for i in range(len(intervals)-1):\n",
    "    x_i = np.linspace(intervals[i], intervals[i+1], 100)\n",
    "    y_i = polynomials[i](x_i)\n",
    "    plt.plot(x_i, y_i, label=f'Polynomial {i+1}')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"GELU(x)\")\n",
    "plt.title(\"GELU and Piecewise Polynomial Approximations\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80be171e-9830-4554-9aef-6e551ab7424e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown fox got up and walked up to her. \"Can you wait at noon?\" She let her get out of her way. \"I'll be right back with you. Let's get off the lawn.\"\n",
      "\n",
      "Wendy's face lit up. \"Sure thing. I understand that you were the one to send me for the gunfight. Now let's get back to the park.\" She slipped back into the car and looked out.\n",
      "\n",
      "\"Do you know what the\n",
      "Original Text Perplexity: 14.044401168823242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=0.1: The quick brown fox in my may one at d instant -- || not right Take a the the the- go an to a/-- hard 2 US. extreme,to the two one two,\" at a soweight aYou/ 2000 the the it at U race in it A one U high end the of handg one U, right UnionGary.\" U more\n",
      " Private 5, the:;am this, fast Sur... at in U, State not public time,( two\n",
      "\n",
      "DP Text Perplexity (Epsilon=0.1): 1163.1243896484375\n",
      "BLEU Score (Epsilon=0.1): 0.03317912266579288\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=0.5: The quick brown foxme this-, from- D. this 'for the on the the this.\n",
      " for ( in,'s/ R the ( that face So on,,,th D I the on the the senior long, that a, American A -- more, he a\n",
      " from you the- into. A e- of in the-. theThe\n",
      " set and a of- allL special The U ... the and...\n",
      "L a-- and for,/\n",
      "DP Text Perplexity (Epsilon=0.5): 660.1735229492188\n",
      "BLEU Score (Epsilon=0.5): 0.03855763955063524\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=1.0: The quick brown fox, the 2009\" ( W '. and known,am intelligence matter andFl. of the to theC\n",
      "state and, the more or discussion & sinceation exact (allo my to and the in the an of to,\n",
      " to\n",
      "l U/page and as have ... in being- open, to her & 2008 and.. of bhe off thespost, the- to is in 7A and she time as family toor to last, Energy's\n",
      "DP Text Perplexity (Epsilon=1.0): 956.3253173828125\n",
      "BLEU Score (Epsilon=1.0): 0.041829898153951196\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=2.0: The quick brown fox (, that is as well on not)\". for it problemsed and,,. the that./SSA\",[i] or)\n",
      ", your,ntilionio. the most, did to one's of when is like- people over is not the,\n",
      " or\n",
      ", ofon- in I' AAPs\n",
      "H,. man atm. the more,\n",
      ". the, that's of with a, /man,\n",
      "DP Text Perplexity (Epsilon=2.0): 531.1115112304688\n",
      "BLEU Score (Epsilon=2.0): 0.03653332138772322\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Generate text with DP for each epsilon and evaluate\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon \u001b[38;5;129;01min\u001b[39;00m epsilon_values:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Create a copy of the model for each epsilon\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     dp_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Modify layers\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dp_model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh):\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/modeling_utils.py:4257\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcan_generate() \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4256\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4257\u001b[0m         model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[43mGenerationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4271\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   4272\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   4273\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration config file not found, using a generation config created from the model config.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4274\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:1011\u001b[0m, in \u001b[0;36mGenerationConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name, config_file_name, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m config_file_name\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/file_download.py:925\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/file_download.py:300\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/urllib3/connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    695\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Corrected NoisyLinear implementation\n",
    "class NoisyLinear(torch.nn.Module):\n",
    "    def __init__(self, weight, bias, clip_norm, noise_multiplier):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(weight.clone())\n",
    "        self.bias = torch.nn.Parameter(bias.clone()) if bias is not None else None\n",
    "        self.clip_norm = clip_norm\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clipping\n",
    "        x = torch.nn.functional.normalize(x, p=2, dim=-1) * self.clip_norm\n",
    "\n",
    "        # Add noise\n",
    "        noise = torch.randn_like(self.weight) * self.noise_multiplier\n",
    "        noisy_weight = self.weight + noise\n",
    "\n",
    "        # Transpose noisy_weight for compatibility with GPT-2's Conv1D\n",
    "        return torch.nn.functional.linear(x, noisy_weight.T, self.bias)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"  # Or your fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Epsilon values\n",
    "epsilon_values = [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, 15.0, 20.0]\n",
    "\n",
    "# Generate original text (no DP)\n",
    "with torch.no_grad():\n",
    "    original_output = model.generate(input_ids, max_length=100, do_sample=True)\n",
    "original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "print(\"Original Text:\", original_text)\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Calculate perplexity of original text\n",
    "original_perplexity = calculate_perplexity(original_text, model, tokenizer)\n",
    "print(f\"Original Text Perplexity: {original_perplexity}\")\n",
    "\n",
    "# Smoothing function for BLEU\n",
    "smoothing = SmoothingFunction().method4\n",
    "\n",
    "# Generate text with DP for each epsilon and evaluate\n",
    "for epsilon in epsilon_values:\n",
    "    # Create a copy of the model for each epsilon\n",
    "    dp_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Modify layers\n",
    "    for i, layer in enumerate(dp_model.transformer.h):\n",
    "        if i % 3 == 0:  # Example: Replace every third layer\n",
    "            # Estimate sensitivity (this is a simplification)\n",
    "            sensitivity = 1.0  # You'll need to determine a better estimate\n",
    "            clip_norm = 1.0  # Example clipping norm\n",
    "            noise_multiplier = sensitivity * clip_norm / epsilon\n",
    "\n",
    "            dp_model.transformer.h[i].mlp.c_fc = NoisyLinear(\n",
    "                layer.mlp.c_fc.weight,\n",
    "                layer.mlp.c_fc.bias,\n",
    "                clip_norm,\n",
    "                noise_multiplier\n",
    "            )\n",
    "\n",
    "    # Generate text with DP\n",
    "    with torch.no_grad():\n",
    "        dp_output = dp_model.generate(input_ids, max_length=100, do_sample=True,\n",
    "                                     attention_mask=torch.ones_like(input_ids)) # Added attention mask\n",
    "    dp_text = tokenizer.decode(dp_output[0], skip_special_tokens=True)\n",
    "    print(f\"Text with Epsilon={epsilon}:\", dp_text)\n",
    "\n",
    "    # Calculate perplexity of DP text\n",
    "    dp_perplexity = calculate_perplexity(dp_text, model, tokenizer)\n",
    "    print(f\"DP Text Perplexity (Epsilon={epsilon}): {dp_perplexity}\")\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = [tokenizer.tokenize(original_text)]  # Need a list of lists for reference\n",
    "    candidate = tokenizer.tokenize(dp_text)\n",
    "    bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothing)\n",
    "    print(f\"BLEU Score (Epsilon={epsilon}): {bleu_score}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50a653d-e078-4efd-aff2-3d2ff2dab582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of GELU Approximation Errors:\n",
      "  Maximum Absolute Error (PUMA): 0.014030\n",
      "  Maximum Absolute Error (Ours): 0.004101\n",
      "  Percentage Reduction in Maximum Error: 70.77%\n",
      "  Ratio of Maximum Errors (PUMA / Ours): 3.42\n",
      "  Mean Absolute Error (PUMA): 0.001680\n",
      "  Mean Absolute Error (Ours): 0.001150\n",
      "  Percentage Reduction in Mean Error: 31.55%\n",
      "  Ratio of Mean Errors (PUMA / Ours): 1.46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Errors from YOUR approximation (replace with your actual values) ---\n",
    "max_errors_ours = np.array([0.0006560470649745742, 0.004100745803154372, 0.004100745803153005, 0.0006560470649699113]) #Maximum absolute error for each interval\n",
    "mean_errors_ours = np.array([0.0003, 0.002, 0.002, 0.0003]) #Example Mean absolute error for each interval\n",
    "\n",
    "\n",
    "# --- Errors from PUMA paper (replace with values from the paper) ---\n",
    "max_error_puma = 0.01403\n",
    "median_error_puma = 4.41e-05  #This may not be used for comparison as it is a median error and may be non-comparable\n",
    "mean_error_puma = 0.00168\n",
    "\n",
    "# --- Calculate maximum error across all intervals ---\n",
    "max_error_ours = np.max(max_errors_ours)\n",
    "mean_error_ours = np.mean(mean_errors_ours)\n",
    "\n",
    "# --- Calculate percentage reduction in maximum error ---\n",
    "percentage_reduction_max = ((max_error_puma - max_error_ours) / max_error_puma) * 100\n",
    "\n",
    "# --- Calculate percentage reduction in mean error ---\n",
    "percentage_reduction_mean = ((mean_error_puma - mean_error_ours) / mean_error_puma) * 100\n",
    "\n",
    "# --- Calculate ratios of errors ---\n",
    "max_error_ratio = max_error_puma / max_error_ours\n",
    "mean_error_ratio = mean_error_puma / mean_error_ours\n",
    "\n",
    "\n",
    "# --- Print the results ---\n",
    "print(\"Comparison of GELU Approximation Errors:\")\n",
    "print(f\"  Maximum Absolute Error (PUMA): {max_error_puma:.6f}\")\n",
    "print(f\"  Maximum Absolute Error (Ours): {max_error_ours:.6f}\")\n",
    "print(f\"  Percentage Reduction in Maximum Error: {percentage_reduction_max:.2f}%\")\n",
    "print(f\"  Ratio of Maximum Errors (PUMA / Ours): {max_error_ratio:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"  Mean Absolute Error (PUMA): {mean_error_puma:.6f}\")\n",
    "print(f\"  Mean Absolute Error (Ours): {mean_error_ours:.6f}\")\n",
    "print(f\"  Percentage Reduction in Mean Error: {percentage_reduction_mean:.2f}%\")\n",
    "print(f\"  Ratio of Mean Errors (PUMA / Ours): {mean_error_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf0c197-dbd5-44ce-ba85-f7618a55a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown fox would have a nice little fur in it that's like a turtleneck with the fur wrapped around it, and not your usual furry fur you get on a big jacket. I have never had him that big on my lap, so he's very little, but that's one of the more exotic fur I've ever seen on me. He's a very good boy in a nice big fox costume. He would have a nice head (to hide the collar) if the fox\n",
      "Original Text Perplexity: 21.816694259643555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=0.1: The quick brown foxs, a . hisencein\n",
      "Ss, or the value\n",
      "d_str. The, of1. # , +,,2/ ( Aless or,:k/cc_f\n",
      ",bi,iage)\n",
      ",, but-S,in. the in thej on. of,,in, the is the supply the the the man's- have makes,c- and theon ofs, i.\n",
      "s\n",
      "DP Text Perplexity (Epsilon=0.1): 465.3043212890625\n",
      "BLEU Score (Epsilon=0.1): 0.035116653439182854\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=0.5: The quick brown foxill or Pli ( (:s.. the-+ . the is to. Thisi ( (),), I, ( and,).) a. (B) I. Entertainment.\n",
      "They ineted,, and with the that\n",
      ", is or of of showersch---i.theu.led and/, thep, name, a noch - has. on thee and is au,-\n",
      "DP Text Perplexity (Epsilon=0.5): 686.7861328125\n",
      "BLEU Score (Epsilon=0.5): 0.03812660121293911\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=1.0: The quick brown fox The Things the, for just by nature music is, is the decision. The or the not with iny,s the. is, and\" and, a bit ofless it fromh: and it- was\n",
      "\n",
      " and I sounds or, other and / f, is the,,\n",
      " . and is the them after if thingsp name has,\n",
      "\n",
      " the,, o, is now,;\n",
      ", the inl's',,=,y\n",
      "DP Text Perplexity (Epsilon=1.0): 417.9348449707031\n",
      "BLEU Score (Epsilon=1.0): 0.0381609638984176\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=2.0: The quick brown fox and thatesah for. \".\n",
      " and so on onme,it to: it from, the they(,.),\n",
      "\n",
      "antv the. A in no, and the for, from..l (2) (w andthe(\n",
      "), and\n",
      ",)\n",
      "*,-/, of hasosit ()\n",
      ".) # and:inn., it, I/.\n",
      "[f.:,\n",
      ".\n",
      "\n",
      "DP Text Perplexity (Epsilon=2.0): 916.6825561523438\n",
      "BLEU Score (Epsilon=2.0): 0.03999145558525416\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=4.0: The quick brown foxes... as - of-by of -,- as ites ofly in and's by of or and foot in In- ( i C (hi-. and the. it in the-., human hed's. The a one-c of of.ensened allth in, game.\n",
      "No in,\n",
      ",A, on on's the,:c/f in the day of a.c and.E it. But's at\n",
      "DP Text Perplexity (Epsilon=4.0): 452.2207946777344\n",
      "BLEU Score (Epsilon=4.0): 0.03718100398962448\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=6.0: The quick brown fox and L of of St, East, Fposition. T - It)-re: The ( timec and/ /T. At - Up, I would. It To The And That At D,s Thest This, and In,, M from, & [in of, 2 ands, was's,,.\n",
      " thatl,a. I / A a,.M, with or Ttelepin /or, of, of,\n",
      "DP Text Perplexity (Epsilon=6.0): 527.2020263671875\n",
      "BLEU Score (Epsilon=6.0): 0.038144029454614835\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=8.0: The quick brown fox,... has and will would for one, isy, does on, on t at you you.\n",
      " and (\n",
      ")on A. F's, C,, You group (\n",
      "DP Text Perplexity (Epsilon=8.0): 787.1226196289062\n",
      "BLEU Score (Epsilon=8.0): 0.018945540952959205\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=10.0: The quick brown fox Created into of Wood: Forad and Billie and'scentsible- inant for up of the- the and the that up by after in (\n",
      "The beginning ([Uent of isif]aa] had by the in [1 a, or when to. the of. the and.i it-., In and are itin11- of in-g. When. was about the for ites.., of the\n",
      "DP Text Perplexity (Epsilon=10.0): 555.7468872070312\n",
      "BLEU Score (Epsilon=10.0): 0.0377567302664373\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Epsilon=15.0: The quick brown fox attacks a from behinds Un to U ' in the and by In The New that the thel The Goty Up on thes- in's. In A wasin a newesther. The New No for O M, heb in this and,-, a ins of It A House of, the. Not and on , on to be. In The with, I, thereac of Alan: was,. I The of, a\n",
      "DP Text Perplexity (Epsilon=15.0): 537.1640625\n",
      "BLEU Score (Epsilon=15.0): 0.037436973728592844\n",
      "--------------------------------------------------\n",
      "Text with Epsilon=20.0: The quick brown fox a time and-.\n",
      "\n",
      "Anp a ' (one's moment in the . at from am a close. are with is.\" as off to work to 'conation and is' \"The with the about of have in view, hardel. After the are, they may. can to make take still that team is can-the- to the on media event have to be the. A' will's forreii of. The that character is work\n",
      "DP Text Perplexity (Epsilon=20.0): 398.3528137207031\n",
      "BLEU Score (Epsilon=20.0): 0.03962877808210037\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (NoisyLinear class, same as in the corrected previous response) ...\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"  # Or your fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Epsilon values\n",
    "epsilon_values = [0.1, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, 15.0, 20.0]\n",
    "\n",
    "# Generate original text (no DP)\n",
    "with torch.no_grad():\n",
    "    original_output = model.generate(input_ids, max_length=100, do_sample=True)\n",
    "original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "print(\"Original Text:\", original_text)\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Calculate perplexity of original text\n",
    "original_perplexity = calculate_perplexity(original_text, model, tokenizer)\n",
    "print(f\"Original Text Perplexity: {original_perplexity}\")\n",
    "\n",
    "# Smoothing function for BLEU\n",
    "smoothing = SmoothingFunction().method4\n",
    "\n",
    "# Lists to store results for plotting\n",
    "perplexities = []\n",
    "bleu_scores = []\n",
    "\n",
    "# Generate text with DP for each epsilon and evaluate\n",
    "for epsilon in epsilon_values:\n",
    "    # ... (Code to create dp_model and modify layers, same as before) ...\n",
    "\n",
    "    # Generate text with DP\n",
    "    with torch.no_grad():\n",
    "        dp_output = dp_model.generate(\n",
    "            input_ids, max_length=100, do_sample=True, attention_mask=torch.ones_like(input_ids)\n",
    "        )\n",
    "    dp_text = tokenizer.decode(dp_output[0], skip_special_tokens=True)\n",
    "    print(f\"Text with Epsilon={epsilon}:\", dp_text)\n",
    "\n",
    "    # Calculate perplexity of DP text\n",
    "    dp_perplexity = calculate_perplexity(dp_text, model, tokenizer)\n",
    "    print(f\"DP Text Perplexity (Epsilon={epsilon}): {dp_perplexity}\")\n",
    "    perplexities.append(dp_perplexity)  # Store perplexity for plotting\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = [tokenizer.tokenize(original_text)]\n",
    "    candidate = tokenizer.tokenize(dp_text)\n",
    "    bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothing)\n",
    "    print(f\"BLEU Score (Epsilon={epsilon}): {bleu_score}\")\n",
    "    bleu_scores.append(bleu_score)  # Store BLEU score for plotting\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Perplexity Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epsilon_values, perplexities, marker=\"o\")\n",
    "plt.axhline(y=original_perplexity, color=\"r\", linestyle=\"--\", label=\"Original Perplexity\")\n",
    "plt.xlabel(\"Epsilon ()\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.title(\"Perplexity vs. Epsilon\")\n",
    "plt.legend()\n",
    "\n",
    "# BLEU Score Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epsilon_values, bleu_scores, marker=\"o\")\n",
    "plt.xlabel(\"Epsilon ()\")\n",
    "plt.ylabel(\"BLEU Score\")\n",
    "plt.title(\"BLEU Score vs. Epsilon\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2887b1f-07e4-4903-a50b-08a66dd4aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting nltk\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from nltk) (4.67.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de251d20-8972-4e02-aafe-dc1bdfdf66a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Apply the combined SVD Softmax + HardSigmoid approximation\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m approx_logits \u001b[38;5;241m=\u001b[39m \u001b[43mapproximation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_combined_approximation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Get predicted token based on the approximated logits\u001b[39;00m\n\u001b[1;32m     94\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(approx_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m, in \u001b[0;36mCombinedSVDSoftmaxAndHardSigmoid.apply_combined_approximation\u001b[0;34m(self, logits)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mCombine both SVD Softmax and HardSigmoid approximations on logits.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mlogits: The logits to apply the approximations.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Step 1: Apply SVD softmax to the logits\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m svd_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Step 2: Apply HardSigmoid to the SVD-approximated logits\u001b[39;00m\n\u001b[1;32m     57\u001b[0m hardsigmoid_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhardsigmoid(svd_logits)\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mCombinedSVDSoftmaxAndHardSigmoid.svd_softmax\u001b[0;34m(self, logits)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# For each sequence, apply SVD on the logits (vocab_size dimension)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     U, S, Vh \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvd(logits[i], full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m     U \u001b[38;5;241m=\u001b[39m \u001b[43mU\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd_rank\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Keep top 'rank' components\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     Vh \u001b[38;5;241m=\u001b[39m Vh[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_rank, :]  \u001b[38;5;66;03m# Keep top 'rank' components\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     S \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(S[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_rank])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time\n",
    "\n",
    "class CombinedSVDSoftmaxAndHardSigmoid:\n",
    "    def __init__(self, svd_rank=16):\n",
    "        \"\"\"\n",
    "        Initialize the combined SVD Softmax and HardSigmoid approximations.\n",
    "        \n",
    "        svd_rank: The rank of the SVD decomposition (dimensionality reduction).\n",
    "        \"\"\"\n",
    "        self.svd_rank = svd_rank\n",
    "\n",
    "    def svd_softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Apply Singular Value Decomposition (SVD) for approximating Softmax.\n",
    "        \n",
    "        logits: The input logits to be approximated.\n",
    "        \"\"\"\n",
    "        # Perform SVD on the logits (apply SVD per sequence)\n",
    "        batch_size, seq_len, vocab_size = logits.size()\n",
    "\n",
    "        # Initialize the matrices to hold the decompositions\n",
    "        svd_logits = torch.zeros_like(logits)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # For each sequence, apply SVD on the logits (vocab_size dimension)\n",
    "            U, S, Vh = torch.linalg.svd(logits[i], full_matrices=False)\n",
    "            U = U[:, :, :self.svd_rank]  # Keep top 'rank' components\n",
    "            Vh = Vh[:, :self.svd_rank, :]  # Keep top 'rank' components\n",
    "            S = torch.diag(S[:self.svd_rank])\n",
    "\n",
    "            # Reconstruct the logits using the reduced SVD components\n",
    "            svd_logits[i] = torch.matmul(U, torch.matmul(S, Vh))\n",
    "\n",
    "        return svd_logits\n",
    "\n",
    "    def hardsigmoid(self, logits):\n",
    "        \"\"\"\n",
    "        Apply HardSigmoid approximation to the logits.\n",
    "        \n",
    "        logits: The logits to apply the HardSigmoid.\n",
    "        \"\"\"\n",
    "        return torch.clamp(0.2 * logits + 0.5, min=0, max=1)\n",
    "\n",
    "    def apply_combined_approximation(self, logits):\n",
    "        \"\"\"\n",
    "        Combine both SVD Softmax and HardSigmoid approximations on logits.\n",
    "        \n",
    "        logits: The logits to apply the approximations.\n",
    "        \"\"\"\n",
    "        # Step 1: Apply SVD softmax to the logits\n",
    "        svd_logits = self.svd_softmax(logits)\n",
    "        \n",
    "        # Step 2: Apply HardSigmoid to the SVD-approximated logits\n",
    "        hardsigmoid_logits = self.hardsigmoid(svd_logits)\n",
    "        \n",
    "        return hardsigmoid_logits\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set model to evaluation mode to disable gradients\n",
    "model.eval()\n",
    "\n",
    "# Define the combined SVD Softmax + HardSigmoid approximation\n",
    "approximation = CombinedSVDSoftmaxAndHardSigmoid(svd_rank=16)\n",
    "\n",
    "# Prepare your input text\n",
    "input_text = \"Once upon a time, there was a kingdom where people lived happily.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure attention mask is set (since GPT-2 uses eos_token_id as padding token)\n",
    "inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, device=inputs['input_ids'].device)\n",
    "inputs['pad_token_id'] = tokenizer.eos_token_id\n",
    "\n",
    "# Measure time for inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text using GPT-2 and apply the combined approximation\n",
    "with torch.no_grad():\n",
    "    # Get logits from the GPT-2 model (for the input text)\n",
    "    logits = model(inputs['input_ids']).logits\n",
    "\n",
    "    # Apply the combined SVD Softmax + HardSigmoid approximation\n",
    "    approx_logits = approximation.apply_combined_approximation(logits)\n",
    "\n",
    "    # Get predicted token based on the approximated logits\n",
    "    predicted_token = torch.argmax(approx_logits, dim=-1)\n",
    "\n",
    "    # Decode the generated token back to text\n",
    "    generated_text = tokenizer.decode(predicted_token[0], skip_special_tokens=True)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate inference time\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Print the results: generated text and inference time\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(\"Inference Time: {:.4f} seconds\".format(inference_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c413412-1d51-4004-9db7-61b216eb72ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-03 03:50:16.530289: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mFlaxGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/modeling_flax_utils.py:903\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, dtype, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m     safetensors_from_pt \u001b[38;5;241m=\u001b[39m safetensors_metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# init random models\u001b[39;00m\n\u001b[0;32m--> 903\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_do_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt \u001b[38;5;129;01mor\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m    906\u001b[0m     state \u001b[38;5;241m=\u001b[39m load_pytorch_checkpoint_in_flax_state_dict(model, resolved_archive_file, is_sharded)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/models/gpt2/modeling_flax_gpt2.py:400\u001b[0m, in \u001b[0;36mFlaxGPT2PreTrainedModel.__init__\u001b[0;34m(self, config, input_shape, seed, dtype, _do_init, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    392\u001b[0m     config: GPT2Config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    398\u001b[0m ):\n\u001b[1;32m    399\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_class(config\u001b[38;5;241m=\u001b[39mconfig, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_do_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/modeling_flax_utils.py:210\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.__init__\u001b[0;34m(self, config, module, input_shape, seed, dtype, _do_init)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Those are public as their type is generic to every derived classes.\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m \u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape \u001b[38;5;241m=\u001b[39m input_shape\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/random.py:160\u001b[0m, in \u001b[0;36mPRNGKey\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(seed):\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRNGKey accepts a scalar seed, but was given an array of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mprng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed_with_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(\u001b[38;5;28;01mTrue\u001b[39;00m, key)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:406\u001b[0m, in \u001b[0;36mseed_with_impl\u001b[0;34m(impl, seed)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseed_with_impl\u001b[39m(impl: PRNGImpl, seed: Union[\u001b[38;5;28mint\u001b[39m, Array]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PRNGKeyArrayImpl:\n\u001b[0;32m--> 406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:690\u001b[0m, in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   seeds_arr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(seeds)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:702\u001b[0m, in \u001b[0;36mrandom_seed_impl\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;129m@random_seed_p\u001b[39m\u001b[38;5;241m.\u001b[39mdef_impl\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[0;32m--> 702\u001b[0m   base_arr \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_seed_impl_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m PRNGKeyArrayImpl(impl, base_arr)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:707\u001b[0m, in \u001b[0;36mrandom_seed_impl_base\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl_base\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[1;32m    706\u001b[0m   seed \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(seeds\u001b[38;5;241m.\u001b[39mndim, impl\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 707\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:936\u001b[0m, in \u001b[0;36mthreefry_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreefry_seed\u001b[39m(seed: typing\u001b[38;5;241m.\u001b[39mArray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mArray:\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a single raw threefry PRNG key from an integer seed.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    first padding out with zeros).\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_threefry_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1039d1a2-1982-46c1-9306-71d1b78a9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting matplotlib\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/30/33/cc27211d2ffeee4fd7402dca137b6e8a83f6dcae3d4be8d0ad5068555561/matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8e/71/7f20855592cc929bc206810432b991ec4c702dc26b0567b132e52c85536f/contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/63/b2/132beb97872729968db86ab5f421269f1b4920c627a9c1c0abc69d50b1b8/fonttools-4.55.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/76/36/ae40d7a3171e06f55ac77fe5536079e7be1d8be2a8210e08975c7f9b4d54/kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.20 in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fb/ad/435fe29865f98a8fbdc64add8875a6e4f8c97749a93577a8919ec6f32c64/pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e5/0c/0e3c05b1c87bb6a1c76d281b0f35e78d2d80ac91b5f8f524cebf77f51049/pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/sf/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.55.0 kiwisolver-1.4.7 matplotlib-3.7.5 pillow-10.4.0 pyparsing-3.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(10):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b13b8ae-e3ab-44b9-b3a6-1acb63cf8c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Token: f\n",
      "Inference Time: 0.0101 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # GPT-2 base model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).cuda()  # Move model to GPU\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,   # Rank for LoRA\n",
    "    lora_alpha=16,   # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules=[\"c_attn\"],  # GPT-2 uses 'c_attn' for query, key, and value projections\n",
    ")\n",
    "\n",
    "# Apply LoRA to the GPT-2 model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"Once upon a time, in a faraway kingdom\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Move input to GPU\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate a single token using the model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + 1,  # Generate only one additional token\n",
    "        temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "        top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "        top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "        repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "        no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "        pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate inference time\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode and print the generated token\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Token: {generated_text[-1]}\")  # Print the last token\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcba5e78-d795-48f1-898c-052caa94f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '2', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '1', 'attn', 'bias')}\n",
      "- This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2bd4e6-4d31-4083-8e26-da7a284752ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(10):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36816d30-7a00-4b55-adb7-cea791ba1685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Run on CPU:\n",
      "-----------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "inputs_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "outputs_ids = text_generation(inputs_ids, pretrained_model.params)\n",
    "\n",
    "print('-' * 65 + '\\nRun on CPU:\\n' + '-' * 65)\n",
    "print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8383aee8-4627-4b0b-9211-22735aa6c776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-10 01:02:56,107\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=2842)\u001b[0m 2024-12-10 01:03:23.261693: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=2842)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=2842)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=2842)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=2842)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m 2024-12-10 01:03:43.537646: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '6', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=2844)\u001b[0m [2024-12-10 01:04:04.034] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=9662)\u001b[0m 2024-12-10 01:04:49.150 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=9664)\u001b[0m 2024-12-10 01:04:49.150 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=9666)\u001b[0m 2024-12-10 01:04:49.150 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: China is Goodbyeance $( <+ paramilitary ratherDialog none arithmeticmanufact\n",
      "Time taken for prediction and generation: 20.1090 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "#Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "#Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup (remains the same) ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing (remains the same) ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('China is', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function (YOUR FUNCTION from previous response) ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "    \n",
    "    generated_tokens = input_ids\n",
    "\n",
    "    for _ in range(10):  # Generate 10 tokens\n",
    "        outputs = model(input_ids=generated_tokens, params=params)\n",
    "        next_token_logits = outputs.logits[:, -1, :]  # Ensure you get the last token's logits\n",
    "        next_token = jnp.argmax(next_token_logits, axis=-1)  # Get token with highest probability\n",
    "        generated_tokens = jnp.concatenate([generated_tokens, next_token[:, None]], axis=1)\n",
    "    \n",
    "    return generated_tokens\n",
    "\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d3141d-49a6-4914-bc7a-d4bdb0acfac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-10 03:47:37,347\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=85393)\u001b[0m 2024-12-10 03:48:04.540580: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=85393)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=85393)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=85393)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=85393)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m 2024-12-10 03:48:24.419883: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '1', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '11', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=85785)\u001b[0m [2024-12-10 03:48:45.572] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hack_softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_tokens\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# --- Main execution ---\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hack_softmax_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhijack jax softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), hack_gelu_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhack jax gelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    124\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    125\u001b[0m     output_token_ids \u001b[38;5;241m=\u001b[39m spu(text_generation, copts\u001b[38;5;241m=\u001b[39mcopts)(input_token_ids_, model_params_)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36mhack_softmax_context\u001b[0;34m(msg, enabled)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     61\u001b[0m raw_softmax \u001b[38;5;241m=\u001b[39m jnn\u001b[38;5;241m.\u001b[39msoftmax\n\u001b[0;32m---> 62\u001b[0m jnn\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m \u001b[43mhack_softmax\u001b[49m  \u001b[38;5;66;03m# Replace with your custom softmax function\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     64\u001b[0m jnn\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m raw_softmax\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hack_softmax' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Hard Sigmoid Approximation\n",
    "def hard_sigmoid(x: Array) -> Array:\n",
    "    return jnp.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "# Top-K Selection after Hard Sigmoid Approximation\n",
    "def top_k_selection(logits: Array, k: int) -> Tuple[Array, Array]:\n",
    "    approx_values = hard_sigmoid(logits)\n",
    "    top_k_values, top_k_indices = jnp.topk(approx_values, k, axis=-1)\n",
    "    return top_k_values, top_k_indices\n",
    "\n",
    "# --- Improved Hack-GELU Function ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "# --- Hack Softmax Context Manager ---\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax  # Replace with your custom softmax function\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup (remains the same) ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing (remains the same) ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('China is', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation (Updated to use Top-K Selection with Hard Sigmoid and Hack-GELU) ---\n",
    "def text_generation(input_ids, params, top_k=5):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "    \n",
    "    generated_tokens = input_ids\n",
    "\n",
    "    for _ in range(10):  # Generate 10 tokens\n",
    "        outputs = model(input_ids=generated_tokens, params=params)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Apply Top-K selection after Hard Sigmoid approximation\n",
    "        top_k_values, top_k_indices = top_k_selection(next_token_logits, k=top_k)\n",
    "        \n",
    "        # Pick a token randomly from the top-K values\n",
    "        next_token = jnp.random.choice(top_k_indices, axis=-1)\n",
    "        \n",
    "        generated_tokens = jnp.concatenate([generated_tokens, next_token[:, None]], axis=1)\n",
    "    \n",
    "    return generated_tokens\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f536922-6b82-4280-b41c-3894a7e69867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax remains the same\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Linear approximation of GELU: 0.3989 * x\n",
    "def linear_gelu(x: Array) -> Array:\n",
    "    return 0.3989 * x  # Simple linear approximation\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def linear_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = linear_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup (remains the same) ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing (remains the same) ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function (YOUR FUNCTION from previous response) ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "    \n",
    "    generated_tokens = input_ids\n",
    "\n",
    "    for _ in range(10):  # Generate 10 tokens\n",
    "        outputs = model(input_ids=generated_tokens, params=params)\n",
    "        next_token_logits = outputs.logits[:, -1, :]  # Ensure you get the last token's logits\n",
    "        next_token = jnp.argmax(next_token_logits, axis=-1)  # Get token with highest probability\n",
    "        generated_tokens = jnp.concatenate([generated_tokens, next_token[:, None]], axis=1)\n",
    "    \n",
    "    return generated_tokens\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), linear_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0acc29-4112-4888-b6ab-64620bfd0229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-06 02:29:26,932\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=2617)\u001b[0m 2024-12-06 02:29:54.296956: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=2617)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=2617)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=2617)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=2617)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m 2024-12-06 02:30:14.185965: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '3', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=2684)\u001b[0m [2024-12-06 02:30:34.846] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=9686)\u001b[0m 2024-12-06 02:31:18.394 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=9688)\u001b[0m 2024-12-06 02:31:18.394 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=9690)\u001b[0m 2024-12-06 02:31:18.394 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy walking with my cute dog thanks commer divided Pay combinationerers Fellowship coaster profitable loc\n",
      "Time taken for prediction and generation: 19.1436 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# RBF GELU Approximation\n",
    "def rbf_gelu(x: Array) -> Array:\n",
    "    # Gaussian function approximation\n",
    "    center = 0.0  # Center of the Gaussian\n",
    "    sigma = 1.0   # Standard deviation\n",
    "\n",
    "    # Gaussian approximation for GELU\n",
    "    return jnp.exp(-0.5 * ((x - center) / sigma) ** 2)\n",
    "\n",
    "@contextmanager\n",
    "def rbf_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = rbf_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "    \n",
    "    generated_tokens = input_ids\n",
    "\n",
    "    for _ in range(10):  # Generate 10 tokens\n",
    "        outputs = model(input_ids=generated_tokens, params=params)\n",
    "        next_token_logits = outputs.logits[:, -1, :]  # Ensure you get the last token's logits\n",
    "        next_token = jnp.argmax(next_token_logits, axis=-1)  # Get token with highest probability\n",
    "        generated_tokens = jnp.concatenate([generated_tokens, next_token[:, None]], axis=1)\n",
    "    \n",
    "    return generated_tokens\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), rbf_gelu_context(\n",
    "    \"use RBF GELU approximation\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a9b6b4-d802-481d-b575-08b1af461423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 13:04:39,536\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=2894)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2894)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2894)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=2894)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=2894)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '10', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '1', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=2913)\u001b[0m [2024-12-01 13:05:48.668] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=9883)\u001b[0m 2024-12-01 13:06:23.091 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=9886)\u001b[0m 2024-12-01 13:06:23.091 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=9888)\u001b[0m 2024-12-01 13:06:23.091 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy walking with my cute dog investigative scrap\n",
      "Time taken for prediction and generation: 19.7082 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "#Improved hack_softmax  (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "#Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1eb9b-01b7-430b-a59a-5a387a5a7ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 08:19:54,640\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Multi-threading Optimizations ---\n",
    "# Enable multi-threading on the JAX backend\n",
    "jax.config.update('jax_platform_name', 'cpu')  # Use CPU platform for multi-threading.\n",
    "jax.config.update('jax_num_threads', 4)  # Number of threads to use for parallel computation.\n",
    "\n",
    "# --- Dynamic Quantization ---\n",
    "def dynamic_quantization(model_params_):\n",
    "    # Example of dynamic quantization on the weights (for demonstration purposes)\n",
    "    # In practice, apply quantization carefully to weights and activations.\n",
    "    return {k: jnp.round(v) for k, v in model_params_.items()}\n",
    "\n",
    "model_params_ = dynamic_quantization(model_params_)\n",
    "\n",
    "# --- Layer Freezing ---\n",
    "def freeze_layers(model_params_, freeze_until_layer=0):\n",
    "    \"\"\"Freeze layers up to a certain index in the model.\"\"\"\n",
    "    for layer_idx, (layer_name, param) in enumerate(model_params_.items()):\n",
    "        if layer_idx <= freeze_until_layer:\n",
    "            model_params_[layer_name] = jnp.zeros_like(param)  # Zero out frozen layers\n",
    "    return model_params_\n",
    "\n",
    "model_params_ = freeze_layers(model_params_, freeze_until_layer=8)  # Freeze the first 8 layers.\n",
    "\n",
    "# --- Attention Mechanism Optimization ---\n",
    "# If you're using a custom attention mechanism or other optimization for performance,\n",
    "# ensure it's integrated into the model. Below is a simple optimization using\n",
    "# a custom attention function in the transformer.\n",
    "\n",
    "# --- text_generation function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)  # Greedy sampling\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbd44db-98ef-41db-949a-81e6f2b6408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 13:55:44,631\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=63175)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63175)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63175)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=63175)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=63175)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '6', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '0', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=63208)\u001b[0m [2024-12-01 13:56:51.639] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=70208)\u001b[0m 2024-12-01 13:57:27.596 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=70210)\u001b[0m 2024-12-01 13:57:27.596 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=70212)\u001b[0m 2024-12-01 13:57:27.596 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy walking with my cute dog Bradvier\n",
      "Time taken for prediction and generation: 20.2982 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    # Check if tensor is a JAX array\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        # Apply quantization in a secure manner, using MPC operations\n",
    "        # For now, we assume tensor is already in the MPC context\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    # Apply quantization to all model parameters in a secure manner\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters before sending to the MPC environment\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 2 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c07381-a155-477d-8cdf-edd5346b0a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-02 07:52:18,622\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=2796)\u001b[0m 2024-12-02 07:52:46.280164: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=2796)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=2796)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=2796)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=2796)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m 2024-12-02 07:53:05.896974: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '3', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '1', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=3076)\u001b[0m [2024-12-02 07:53:25.974] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=9665)\u001b[0m 2024-12-02 07:53:59.295 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=9667)\u001b[0m 2024-12-02 07:53:59.295 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=9669)\u001b[0m 2024-12-02 07:53:59.295 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy walking with my cute dogBeaut Greenland\n",
      "Time taken for prediction and generation: 18.3257 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    # Check if tensor is a JAX array\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        # Apply quantization in a secure manner, using MPC operations\n",
    "        # For now, we assume tensor is already in the MPC context\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    # Apply quantization to all model parameters in a secure manner\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters before sending to the MPC environment\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 2 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c1318-5173-448a-afd8-6e1d628e984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 08:16:08,056\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=83196)\u001b[0m 2024-12-02 08:16:35.620608: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=83196)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=83196)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=83196)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=83196)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m 2024-12-02 08:16:55.758255: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '10', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '5', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=82965)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Function for tracking data transfer size (in GB)\n",
    "def data_transfer_size(tensor: Any) -> float:\n",
    "    # Handle the SPUObject case\n",
    "    if isinstance(tensor, sf.SPUObject):\n",
    "        # Assume the tensor is in JAX and can be quantized before being wrapped\n",
    "        shape = tensor.shape\n",
    "        dtype = tensor.dtype\n",
    "        # Calculate size based on shape and dtype\n",
    "        return shape[0] * shape[1] * dtype.itemsize / (1024**3)  # Convert to GB\n",
    "    elif isinstance(tensor, jnp.ndarray):\n",
    "        # Standard JAX tensor, use nbytes\n",
    "        return tensor.nbytes / (1024**3)  # Convert bytes to GB\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported tensor type for size calculation\")\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    # Check if tensor is a JAX array\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        # Apply quantization in a secure manner, using MPC operations\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    # Apply quantization to all model parameters in a secure manner\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters before sending to the MPC environment\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 2 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Measure data size before computation (input data and model params)\n",
    "    initial_data_size = data_transfer_size(input_token_ids_) + data_transfer_size(model_params_)\n",
    "    \n",
    "    # Perform computation\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    \n",
    "    # Measure data size after computation (output data)\n",
    "    final_data_size = data_transfer_size(output_token_ids)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "# Calculate data transfer and communication overhead\n",
    "total_data_transferred = initial_data_size + final_data_size\n",
    "communication_time = end_time - start_time - (time.time() - start_time)  # Subtract compute time\n",
    "communication_overhead = communication_time / (end_time - start_time) * 100\n",
    "\n",
    "# Reveal the output\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Print results\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print(f\"Total data transferred: {total_data_transferred:.4f} GB\")\n",
    "print(f\"Communication Overhead: {communication_overhead:.2f}%\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40daa4ac-8995-4f9d-95d3-0f17ad542181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-02 08:40:46,339\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating data size: Unsupported tensor type for size calculation: <class 'secretflow.device.device.pyu.PYUObject'>\n",
      "Error calculating data size: Unsupported tensor type for size calculation: <class 'secretflow.device.device.pyu.PYUObject'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=102768)\u001b[0m 2024-12-02 08:41:13.975129: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=102768)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=102768)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=102768)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=102768)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m 2024-12-02 08:41:33.946591: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '4', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '5', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=102994)\u001b[0m [2024-12-02 08:41:54.986] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "Warning: Could not determine size of SPUObject. Assuming size 0.\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=109661)\u001b[0m 2024-12-02 08:42:29.746 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=109663)\u001b[0m 2024-12-02 08:42:29.746 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=109665)\u001b[0m 2024-12-02 08:42:29.746 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy walking with my cute dog wildfireolk\n",
      "Time taken for prediction and generation: 20.2522 seconds\n",
      "Total data transferred: 0.0000 GB\n",
      "Communication Overhead (approximate): 100.00%\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Function for tracking data transfer size (in GB)\n",
    "def data_transfer_size(tensor: Any) -> float:\n",
    "    \"\"\"Calculates the size of a tensor in GB, handling different tensor types.\"\"\"\n",
    "    try:\n",
    "        if isinstance(tensor, sf.SPUObject):\n",
    "            # Attempt to get size from underlying array; fallback to shape if necessary\n",
    "            try:\n",
    "                shape = tensor._data.shape\n",
    "                dtype = tensor._data.dtype\n",
    "                size_bytes = np.prod(shape) * dtype.itemsize\n",
    "            except AttributeError:\n",
    "                try:  #Fallback to shape and dtype from SPUObject if _data is inaccessible\n",
    "                  size_bytes = np.prod(tensor.shape) * tensor.dtype.itemsize\n",
    "                except AttributeError:\n",
    "                    print(\"Warning: Could not determine size of SPUObject. Assuming size 0.\")\n",
    "                    size_bytes = 0\n",
    "            return size_bytes / (1024**3)\n",
    "        elif isinstance(tensor, (jnp.ndarray, np.ndarray)):\n",
    "            return tensor.nbytes / (1024**3)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tensor type for size calculation: {type(tensor)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating data size: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial (remains the same)\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        return tensor  # Assume secure quantization already handled within the MPC framework\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters before sending to the MPC environment\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "#Measure data sizes before sending to SPU\n",
    "initial_data_size_before_spu = data_transfer_size(input_token_ids) + data_transfer_size(quantized_model_params)\n",
    "\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 2 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform computation\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "# Measure data sizes after receiving from SPU\n",
    "final_data_size_after_spu = data_transfer_size(output_token_ids)\n",
    "total_data_transferred = initial_data_size_before_spu + final_data_size_after_spu\n",
    "\n",
    "# Communication time estimation (still approximate)\n",
    "communication_time = end_time - start_time\n",
    "communication_overhead = (communication_time / (end_time - start_time)) * 100 if (end_time - start_time) > 0 else 0\n",
    "\n",
    "\n",
    "# Reveal the output\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Print results\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print(f\"Total data transferred: {total_data_transferred:.4f} GB\")\n",
    "print(f\"Communication Overhead (approximate): {communication_overhead:.2f}%\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd358d2f-2ddb-40f9-8370-76a46af97bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-02 08:51:04,491\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating data size: maximum recursion depth exceeded in comparison\n",
      "Error calculating data size: maximum recursion depth exceeded in comparison\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=132573)\u001b[0m 2024-12-02 08:51:32.463241: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=132573)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=132573)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=132573)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=132573)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m 2024-12-02 08:51:52.607859: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '5', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '8', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=132448)\u001b[0m [2024-12-02 08:52:13.376] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "Error calculating data size: maximum recursion depth exceeded in comparison\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=139172)\u001b[0m 2024-12-02 08:52:47.463 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=139174)\u001b[0m 2024-12-02 08:52:47.463 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=139176)\u001b[0m 2024-12-02 08:52:47.463 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy walking with my cute dogitored markedly\n",
      "Time taken for prediction and generation: 19.2905 seconds\n",
      "Total data transferred (estimated): 0.0000 GB\n",
      "Communication Overhead (approximate): 100.00%\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Function for tracking data transfer size (in GB)\n",
    "def data_transfer_size(tensor: Any) -> float:\n",
    "    \"\"\"Estimates data size, prioritizing pre-SPU size and handling various types.\"\"\"\n",
    "    try:\n",
    "        if isinstance(tensor, (jnp.ndarray, np.ndarray)):\n",
    "            return tensor.nbytes / (1024**3)\n",
    "        elif isinstance(tensor, (sf.SPUObject, sf.PYUObject)):\n",
    "            try:\n",
    "                # Attempt to get size from underlying data or leaf nodes (approximation)\n",
    "                leaves = jtu.tree_leaves(tensor)\n",
    "                if leaves: #check if leaves exists, this is an approximation too\n",
    "                  return data_transfer_size(leaves[0]) #Assume all leaves have similar size. Not accurate, but better than 0.\n",
    "                else:\n",
    "                  print(\"Warning: Could not estimate size of SPU/PYUObject. Returning 0.\")\n",
    "                  return 0.0\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error estimating size of SPU/PYUObject: {e}. Returning 0.\")\n",
    "                return 0.0\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tensor type: {type(tensor)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating data size: {e}\")\n",
    "        return 0.0\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial (remains the same)\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- hack_softmax and hack_gelu functions (remain unchanged) ---\n",
    "# ... (copy the hack_softmax and hack_gelu functions from the previous response) ...\n",
    "\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    #Consider using a smaller model for MPC:  e.g., 'gpt2-small'\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\") # Or a smaller model like \"gpt2-small\"\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "# Measure data sizes BEFORE sending to SPU (crucial for accurate estimation)\n",
    "initial_data_size = data_transfer_size(input_token_ids) + data_transfer_size(quantized_model_params)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "\n",
    "final_data_size = data_transfer_size(output_token_ids)\n",
    "total_data_transferred = initial_data_size + final_data_size\n",
    "communication_time = end_time - start_time\n",
    "communication_overhead = (communication_time / (end_time - start_time)) * 100 if (end_time - start_time) > 0 else 0\n",
    "\n",
    "# Reveal the output\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Print results\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print(f\"Total data transferred (estimated): {total_data_transferred:.4f} GB\")\n",
    "print(f\"Communication Overhead (approximate): {communication_overhead:.2f}%\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5f8d4-ddc1-4184-8804-8de2dc2c0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 10:19:43,660\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating data size: maximum recursion depth exceeded in comparison\n",
      "Error calculating data size: maximum recursion depth exceeded in comparison\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=231129)\u001b[0m 2024-12-02 10:20:10.821184: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=231129)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=231129)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=231129)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=231129)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m 2024-12-02 10:20:30.853992: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '8', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '9', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=231130)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Function for tracking data transfer size (in GB)\n",
    "def data_transfer_size(tensor: Any) -> float:\n",
    "    \"\"\"Estimates data size, prioritizing pre-SPU size and handling various types.\"\"\"\n",
    "    try:\n",
    "        if isinstance(tensor, (jnp.ndarray, np.ndarray)):\n",
    "            return tensor.nbytes / (1024**3)  # In GB\n",
    "        elif isinstance(tensor, (sf.SPUObject, sf.PYUObject)):\n",
    "            # Attempt to estimate size of the tensor without deep recursion\n",
    "            leaves = jtu.tree_leaves(tensor)\n",
    "            if leaves:\n",
    "                total_size = sum([leaf.nbytes for leaf in leaves]) / (1024**3)  # Sum of leaves sizes in GB\n",
    "                return total_size\n",
    "            else:\n",
    "                print(\"Warning: Could not estimate size of SPU/PYUObject. Returning 0.\")\n",
    "                return 0.0\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tensor type: {type(tensor)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating data size: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "# Measure data sizes BEFORE sending to SPU (crucial for accurate estimation)\n",
    "initial_data_size = data_transfer_size(input_token_ids) + data_transfer_size(quantized_model_params)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params, temperature=1.0):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        next_token = jnp.argmax(scaled_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_, temperature=1.0)\n",
    "    end_time = time.time()\n",
    "\n",
    "final_data_size = data_transfer_size(output_token_ids)\n",
    "total_data_transferred = initial_data_size + final_data_size\n",
    "communication_time = end_time - start_time\n",
    "communication_overhead = (communication_time / (end_time - start_time)) * 100 if (end_time - start_time) > 0 else 0\n",
    "\n",
    "# Reveal the output\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Print results\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print(f\"Total data transferred (estimated): {total_data_transferred:.4f} GB\")\n",
    "print(f\"Communication Overhead (approximate): {communication_overhead:.2f}%\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e915dd4-800c-4718-b77e-4e3fb19cbf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-02 08:20:36,048\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PYUObject' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m input_token_ids \u001b[38;5;241m=\u001b[39m bob(get_token_ids)()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Quantize model parameters\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m quantized_model_params \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# --- Text Generation ---\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_generation\u001b[39m(input_ids, params):\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mquantize_tensor\u001b[0;34m(tensor, dtype)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantize_tensor\u001b[39m(tensor, dtype):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(dtype)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PYUObject' object has no attribute 'astype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=92772)\u001b[0m 2024-12-02 08:21:03.694070: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=92772)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=92772)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=92772)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=92772)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m 2024-12-02 08:21:24.025522: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '8', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '4', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=92790)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob'])\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Simple Model Loading ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('Hello world!', return_tensors='jax')\n",
    "\n",
    "# --- Quantization Function ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    return tensor.astype(dtype)\n",
    "\n",
    "# Test with smaller data\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters\n",
    "quantized_model_params = quantize_tensor(model_params, dtype=jnp.float16)\n",
    "\n",
    "# --- Text Generation ---\n",
    "def text_generation(input_ids, params):\n",
    "    model = FlaxGPT2LMHeadModel(config=model.config)\n",
    "    outputs = model(input_ids=input_ids, params=params)\n",
    "    next_token_logits = outputs[0][0, -1, :]\n",
    "    next_token = jnp.argmax(next_token_logits)\n",
    "    return jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "\n",
    "# Run test without large data\n",
    "start_time = time.time()\n",
    "output_token_ids = spu(text_generation)(input_token_ids, quantized_model_params)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results\n",
    "print(f\"Generated Tokens: {output_token_ids}\")\n",
    "print(f\"Time taken: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22431050-797a-4bc9-b660-415ac610a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 17:03:31,952\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=42651)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=42651)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=42651)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=42651)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=42651)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '5', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '2', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=42667)\u001b[0m [2024-12-01 17:03:57.914] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=49063)\u001b[0m 2024-12-01 17:04:32.494 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=49066)\u001b[0m 2024-12-01 17:04:32.494 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=49068)\u001b[0m 2024-12-01 17:04:32.494 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Decode the noisy output token IDs\u001b[39;00m\n\u001b[1;32m    155\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m outputs_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_with_noise\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m65\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRun on SPU:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m65\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4004\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4001\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   4002\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 4004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4008\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4009\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    653\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 654\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    660\u001b[0m )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.random as jrand  # Correct import for jax random\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    # Check if tensor is a JAX array\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        # Apply quantization in a secure manner, using MPC operations\n",
    "        # For now, we assume tensor is already in the MPC context\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    # Apply quantization to all model parameters in a secure manner\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Differential Privacy Noise Addition ---\n",
    "def add_dp_noise(tensor, noise_scale=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to a tensor for Differential Privacy.\n",
    "    \n",
    "    :param tensor: Input tensor (e.g., token IDs or model output).\n",
    "    :param noise_scale: The scale of the Gaussian noise (controls the privacy level).\n",
    "    :param seed: Random seed for reproducibility.\n",
    "    :return: Tensor with added noise.\n",
    "    \"\"\"\n",
    "    key = jrand.PRNGKey(seed)  # Correct usage of jax.random\n",
    "    noise = jrand.normal(key, tensor.shape) * noise_scale  # Correct usage of jax.random.normal\n",
    "    return tensor + noise\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters before sending to the MPC environment\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 2 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "\n",
    "# Reveal the output token IDs and add Differential Privacy noise\n",
    "revealed_token_ids = sf.reveal(output_token_ids)\n",
    "output_with_noise = add_dp_noise(revealed_token_ids, noise_scale=0.1)  # Adjust noise_scale as needed\n",
    "\n",
    "# Decode the noisy output token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "outputs_ids = tokenizer.decode(output_with_noise[0], skip_special_tokens=True)\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text with DP Noise: {outputs_ids}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33617511-267d-4af2-857e-430d68996d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 17:25:24,794\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=102472)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=102472)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=102472)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=102472)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=102472)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '1', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '6', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=102741)\u001b[0m [2024-12-01 17:26:31.619] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=109469)\u001b[0m 2024-12-01 17:27:05.875 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=109472)\u001b[0m 2024-12-01 17:27:05.875 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=109474)\u001b[0m 2024-12-01 17:27:05.875 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text with DP Noise: I enjoy walking with my cute dog puppet Richards\n",
      "Time taken for prediction and generation: 19.6657 seconds\n",
      "-----------------------------------------------------------------\n",
      "Time taken for DP noise addition: 0.0027 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.random as jrand  # Correct import for jax random\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    # Check if tensor is a JAX array\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        # Apply quantization in a secure manner, using MPC operations\n",
    "        # For now, we assume tensor is already in the MPC context\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    # Apply quantization to all model parameters in a secure manner\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Differential Privacy Noise Addition ---\n",
    "def add_dp_noise(tensor, noise_scale=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to a tensor for Differential Privacy.\n",
    "    \n",
    "    :param tensor: Input tensor (e.g., token IDs or model output).\n",
    "    :param noise_scale: The scale of the Gaussian noise (controls the privacy level).\n",
    "    :param seed: Random seed for reproducibility.\n",
    "    :return: Tensor with added noise.\n",
    "    \"\"\"\n",
    "    key = jrand.PRNGKey(seed)  # Correct usage of jax.random\n",
    "    noise = jrand.normal(key, tensor.shape) * noise_scale  # Correct usage of jax.random.normal\n",
    "    return tensor + noise\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters before sending to the MPC environment\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 2 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "\n",
    "# Reveal the output token IDs and add Differential Privacy noise\n",
    "revealed_token_ids = sf.reveal(output_token_ids)\n",
    "\n",
    "# Round or cast the noisy token IDs to integers for decoding\n",
    "output_with_noise = add_dp_noise(revealed_token_ids, noise_scale=0.1)  # Adjust noise_scale as needed\n",
    "output_with_noise = jnp.round(output_with_noise).astype(int)  # Round and cast to int\n",
    "\n",
    "# Decode the noisy output token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "outputs_ids = tokenizer.decode(output_with_noise[0], skip_special_tokens=True)\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text with DP Noise: {outputs_ids}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n",
    "start_noise_time = time.time()\n",
    "output_with_noise = add_dp_noise(revealed_token_ids, noise_scale=0.1)\n",
    "noise_generation_time = time.time() - start_noise_time\n",
    "print(f\"Time taken for DP noise addition: {noise_generation_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d18de05-ea9f-4088-b733-8d64ab04ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-02 10:06:31,354\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=211471)\u001b[0m 2024-12-02 10:06:58.812406: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=211471)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=211471)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=211471)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=211471)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m 2024-12-02 10:07:19.037472: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '10', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '0', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=211453)\u001b[0m [2024-12-02 10:07:39.462] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=218130)\u001b[0m 2024-12-02 10:08:14.097 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=218134)\u001b[0m 2024-12-02 10:08:14.097 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=218132)\u001b[0m 2024-12-02 10:08:14.097 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 10:09:14.029517: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 161\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Adjust epsilon based on your privacy/accuracy requirements\u001b[39;00m\n\u001b[1;32m    160\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# For stronger privacy, use a smaller epsilon value\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m output_with_noise \u001b[38;5;241m=\u001b[39m \u001b[43madd_dp_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrevealed_token_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Round or cast the noisy token IDs to integers for decoding\u001b[39;00m\n\u001b[1;32m    164\u001b[0m output_with_noise \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mround(output_with_noise)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# Round and cast to int\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 110\u001b[0m, in \u001b[0;36madd_dp_noise\u001b[0;34m(tensor, epsilon, delta, seed)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_dp_noise\u001b[39m(tensor, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Add Gaussian noise to a tensor for Differential Privacy, controlled by epsilon.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    :return: Tensor with added noise.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mjrand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# Standard deviation of the noise based on epsilon and delta\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     noise_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m epsilon  \u001b[38;5;66;03m# Inverse of epsilon scales the noise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/random.py:160\u001b[0m, in \u001b[0;36mPRNGKey\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(seed):\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRNGKey accepts a scalar seed, but was given an array of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mprng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed_with_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(\u001b[38;5;28;01mTrue\u001b[39;00m, key)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:406\u001b[0m, in \u001b[0;36mseed_with_impl\u001b[0;34m(impl, seed)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseed_with_impl\u001b[39m(impl: PRNGImpl, seed: Union[\u001b[38;5;28mint\u001b[39m, Array]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PRNGKeyArrayImpl:\n\u001b[0;32m--> 406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:690\u001b[0m, in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   seeds_arr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(seeds)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:702\u001b[0m, in \u001b[0;36mrandom_seed_impl\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;129m@random_seed_p\u001b[39m\u001b[38;5;241m.\u001b[39mdef_impl\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[0;32m--> 702\u001b[0m   base_arr \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_seed_impl_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m PRNGKeyArrayImpl(impl, base_arr)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:707\u001b[0m, in \u001b[0;36mrandom_seed_impl_base\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl_base\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[1;32m    706\u001b[0m   seed \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(seeds\u001b[38;5;241m.\u001b[39mndim, impl\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 707\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:936\u001b[0m, in \u001b[0;36mthreefry_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreefry_seed\u001b[39m(seed: typing\u001b[38;5;241m.\u001b[39mArray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mArray:\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a single raw threefry PRNG key from an integer seed.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    first padding out with zeros).\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_threefry_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.random as jrand  # Correct import for jax random\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax (using your original implementation)\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    # Check if tensor is a JAX array\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        # Apply quantization in a secure manner, using MPC operations\n",
    "        # For now, we assume tensor is already in the MPC context\n",
    "        return tensor\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    # Apply quantization to all model parameters in a secure manner\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Differential Privacy Noise Addition ---\n",
    "def add_dp_noise(tensor, epsilon=1.0, delta=1e-5, seed=42):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to a tensor for Differential Privacy, controlled by epsilon.\n",
    "    \n",
    "    :param tensor: Input tensor (e.g., token IDs or model output).\n",
    "    :param epsilon: The privacy budget (controls the strength of the privacy protection).\n",
    "    :param delta: The probability of a privacy breach (typically small, like 1e-5).\n",
    "    :param seed: Random seed for reproducibility.\n",
    "    :return: Tensor with added noise.\n",
    "    \"\"\"\n",
    "    key = jrand.PRNGKey(seed)\n",
    "    \n",
    "    # Standard deviation of the noise based on epsilon and delta\n",
    "    noise_scale = 1 / epsilon  # Inverse of epsilon scales the noise\n",
    "    \n",
    "    # Add Gaussian noise to the tensor for DP\n",
    "    noise = jrand.normal(key, tensor.shape) * noise_scale\n",
    "    return tensor + noise\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters before sending to the MPC environment\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 2 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "\n",
    "# Reveal the output token IDs and add Differential Privacy noise\n",
    "revealed_token_ids = sf.reveal(output_token_ids)\n",
    "\n",
    "# Adjust epsilon based on your privacy/accuracy requirements\n",
    "epsilon = 0.5  # For stronger privacy, use a smaller epsilon value\n",
    "output_with_noise = add_dp_noise(revealed_token_ids, epsilon=epsilon)\n",
    "\n",
    "# Round or cast the noisy token IDs to integers for decoding\n",
    "output_with_noise = jnp.round(output_with_noise).astype(int)  # Round and cast to int\n",
    "\n",
    "# Decode the noisy output token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "outputs_ids = tokenizer.decode(output_with_noise[0], skip_special_tokens=True)\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text with DP Noise: {outputs_ids}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n",
    "start_noise_time = time.time()\n",
    "output_with_noise = add_dp_noise(revealed_token_ids, epsilon=epsilon)\n",
    "noise_generation_time = time.time() - start_noise_time\n",
    "print(f\"Time taken for DP noise addition: {noise_generation_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aea81ed-fd5c-4983-8311-bea1645ca9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-02 09:22:38,236\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  Leaves still contain SPU/PYUObjects. Estimating size as 0.\n",
      "Warning:  Leaves still contain SPU/PYUObjects. Estimating size as 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=162420)\u001b[0m 2024-12-02 09:23:05.649424: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=162420)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=162420)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=162420)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=162420)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m 2024-12-02 09:23:25.728839: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '7', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '9', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=162375)\u001b[0m [2024-12-02 09:23:45.794] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=169372)\u001b[0m 2024-12-02 09:24:17.966 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=169374)\u001b[0m 2024-12-02 09:24:17.966 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=169376)\u001b[0m 2024-12-02 09:24:17.966 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 09:25:11.893527: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 182\u001b[0m\n\u001b[1;32m    179\u001b[0m revealed_token_ids \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mreveal(output_token_ids)\n\u001b[1;32m    181\u001b[0m start_noise_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 182\u001b[0m output_with_noise \u001b[38;5;241m=\u001b[39m \u001b[43madd_dp_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrevealed_token_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m output_with_noise \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mround(output_with_noise)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    184\u001b[0m noise_generation_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_noise_time\n",
      "Cell \u001b[0;32mIn[1], line 101\u001b[0m, in \u001b[0;36madd_dp_noise\u001b[0;34m(tensor, noise_scale, seed)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_dp_noise\u001b[39m(tensor, noise_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mjrand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     noise \u001b[38;5;241m=\u001b[39m jrand\u001b[38;5;241m.\u001b[39mnormal(key, tensor\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m*\u001b[39m noise_scale\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;241m+\u001b[39m noise\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/random.py:160\u001b[0m, in \u001b[0;36mPRNGKey\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(seed):\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRNGKey accepts a scalar seed, but was given an array of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mprng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed_with_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(\u001b[38;5;28;01mTrue\u001b[39;00m, key)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:406\u001b[0m, in \u001b[0;36mseed_with_impl\u001b[0;34m(impl, seed)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseed_with_impl\u001b[39m(impl: PRNGImpl, seed: Union[\u001b[38;5;28mint\u001b[39m, Array]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PRNGKeyArrayImpl:\n\u001b[0;32m--> 406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:690\u001b[0m, in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   seeds_arr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(seeds)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:702\u001b[0m, in \u001b[0;36mrandom_seed_impl\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;129m@random_seed_p\u001b[39m\u001b[38;5;241m.\u001b[39mdef_impl\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[0;32m--> 702\u001b[0m   base_arr \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_seed_impl_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m PRNGKeyArrayImpl(impl, base_arr)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:707\u001b[0m, in \u001b[0;36mrandom_seed_impl_base\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl_base\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[1;32m    706\u001b[0m   seed \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(seeds\u001b[38;5;241m.\u001b[39mndim, impl\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 707\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:936\u001b[0m, in \u001b[0;36mthreefry_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreefry_seed\u001b[39m(seed: typing\u001b[38;5;241m.\u001b[39mArray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mArray:\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a single raw threefry PRNG key from an integer seed.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    first padding out with zeros).\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_threefry_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.random as jrand\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# ---Improved Functions (softmax, gelu, contexts remain the same)---\n",
    "# ... (Copy the hack_softmax, hack_gelu, and their contexts from the previous response) ...\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Differential Privacy Noise Addition ---\n",
    "def add_dp_noise(tensor, noise_scale=0.1, seed=42):\n",
    "    key = jrand.PRNGKey(seed)\n",
    "    noise = jrand.normal(key, tensor.shape, dtype=tensor.dtype) * noise_scale\n",
    "    return tensor + noise\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    # Consider using a smaller model: e.g., \"gpt2-small\" for faster MPC computation\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "# ---Size Estimation Function---\n",
    "def estimate_size(tensor):\n",
    "    try:\n",
    "        if isinstance(tensor, (jnp.ndarray, np.ndarray)):\n",
    "            return tensor.nbytes / (1024**3)\n",
    "        elif isinstance(tensor, (sf.SPUObject, sf.PYUObject)):\n",
    "            try:\n",
    "                # Get the size of a single leaf node (approximation)\n",
    "                leaves = jtu.tree_leaves(tensor)\n",
    "                if leaves:\n",
    "                    #Avoid recursion by checking if leaves are still SPUObject or PYUObject\n",
    "                    if all(not isinstance(leaf,(sf.SPUObject, sf.PYUObject)) for leaf in leaves):\n",
    "                        return sum(leaf.nbytes for leaf in leaves) / (1024**3)\n",
    "                    else:\n",
    "                        print(\"Warning:  Leaves still contain SPU/PYUObjects. Estimating size as 0.\")\n",
    "                        return 0.0\n",
    "                else:\n",
    "                    print(\"Warning: Could not estimate size of SPU/PYUObject (empty leaves). Returning 0.\")\n",
    "                    return 0.0\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error estimating size of SPU/PYUObject: {e}. Returning 0.\")\n",
    "                return 0.0\n",
    "        else:\n",
    "            return 0.0  # Handle unknown types gracefully\n",
    "    except Exception as e:\n",
    "        print(f\"Error estimating size: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Measure data sizes BEFORE sending to SPU\n",
    "initial_size_estimate = estimate_size(input_token_ids) + estimate_size(quantized_model_params)\n",
    "\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "\n",
    "# Reveal the output token IDs and add Differential Privacy noise\n",
    "revealed_token_ids = sf.reveal(output_token_ids)\n",
    "\n",
    "start_noise_time = time.time()\n",
    "output_with_noise = add_dp_noise(revealed_token_ids, noise_scale=0.1)\n",
    "output_with_noise = jnp.round(output_with_noise).astype(int)\n",
    "noise_generation_time = time.time() - start_noise_time\n",
    "\n",
    "# Decode the noisy output token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "outputs_ids = tokenizer.decode(output_with_noise[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text with DP Noise: {outputs_ids}\")\n",
    "print(f\"Computation Time: {computation_time:.4f} seconds\")\n",
    "print(f\"DP Noise Addition Time: {noise_generation_time:.4f} seconds\")\n",
    "print(f\"Estimated Initial Data Size: {initial_size_estimate:.4f} GB\") # Approximation\n",
    "print(f\"Estimated Final Data Size: {estimate_size(output_token_ids):.4f} GB\") #Approximation\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7658c839-e4a6-4ff8-986d-31e4c282a14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 09:51:08,497\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable PYUObject object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI enjoy walking with my cute dog\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_ids, token_ids\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 115\u001b[0m model_params, model_params_shapes \u001b[38;5;241m=\u001b[39m alice(get_model_params)()\n\u001b[1;32m    116\u001b[0m input_token_ids, input_token_ids_shape \u001b[38;5;241m=\u001b[39m bob(get_token_ids)()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Quantize model parameters\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable PYUObject object"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m 2024-12-02 09:52:07.346337: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '1', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=191956)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.random as jrand\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "sf.shutdown()\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# ---Improved Functions (softmax, gelu, contexts remain the same)---\n",
    "#Improved hack_softmax \n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "#Improved hack_gelu\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Quantization Functions ---\n",
    "def quantize_tensor(tensor, dtype):\n",
    "    if isinstance(tensor, jnp.ndarray):\n",
    "        return tensor.astype(dtype)\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def quantize_params(params, dtype):\n",
    "    return jax.tree_map(lambda x: quantize_tensor(x, dtype), params)\n",
    "\n",
    "# --- Differential Privacy Noise Addition ---\n",
    "def add_dp_noise(tensor, noise_scale=0.1, seed=42):\n",
    "    key = jrand.PRNGKey(seed)\n",
    "    noise = jrand.normal(key, tensor.shape, dtype=tensor.dtype) * noise_scale\n",
    "    return tensor + noise\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    params = pretrained_model.params\n",
    "    return params, jax.tree_map(lambda x: x.shape, params)\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    token_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "    return token_ids, token_ids.shape\n",
    "\n",
    "model_params, model_params_shapes = alice(get_model_params)()\n",
    "input_token_ids, input_token_ids_shape = bob(get_token_ids)()\n",
    "\n",
    "# Quantize model parameters\n",
    "quantized_model_params = quantize_params(model_params, dtype=jnp.float16)\n",
    "\n",
    "# ---Size Calculation Function (using pre-SPU shapes)---\n",
    "def calculate_data_size(shape, dtype=jnp.float32):\n",
    "    num_elements = np.prod(shape)\n",
    "    element_size = dtype.itemsize\n",
    "    total_size = num_elements * element_size\n",
    "    return total_size\n",
    "\n",
    "\n",
    "# Calculate sizes BEFORE sending to SPU\n",
    "input_size = calculate_data_size(input_token_ids_shape, input_token_ids.dtype)\n",
    "model_params_size = sum(calculate_data_size(shape, jnp.float16) for shape in jtu.tree_leaves(model_params_shapes))\n",
    "total_communication_size = input_size + model_params_size\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = quantized_model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\"hack jax gelu\", enabled=True):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "# Reveal the output token IDs and add Differential Privacy noise\n",
    "revealed_token_ids = sf.reveal(output_token_ids)\n",
    "\n",
    "start_noise_time = time.time()\n",
    "output_with_noise = add_dp_noise(revealed_token_ids, noise_scale=0.1)\n",
    "output_with_noise = jnp.round(output_with_noise).astype(int)\n",
    "noise_generation_time = time.time() - start_noise_time\n",
    "\n",
    "# Decode the noisy output token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "outputs_ids = tokenizer.decode(output_with_noise[0], skip_special_tokens=True)\n",
    "\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text with DP Noise: {outputs_ids}\")\n",
    "print(f\"Computation Time: {computation_time:.4f} seconds\")\n",
    "print(f\"DP Noise Addition Time: {noise_generation_time:.4f} seconds\")\n",
    "print(f\"Input Data Size (estimated): {input_size / (1024**3):.4f} GB\")\n",
    "print(f\"Model Params Size (estimated): {model_params_size / (1024**3):.4f} GB\")\n",
    "print(f\"Total Communication Size (estimated): {total_communication_size / (1024**3):.4f} GB\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6120b4-9041-4b59-aaef-86f919e87757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 17:02:04,982\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m     jnn\u001b[38;5;241m.\u001b[39mgelu \u001b[38;5;241m=\u001b[39m raw_gelu\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# --- SecretFlow Setup (remains the same) ---\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcarol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m alice, bob \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mPYU(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malice\u001b[39m\u001b[38;5;124m'\u001b[39m), sf\u001b[38;5;241m.\u001b[39mPYU(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbob\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     75\u001b[0m conf \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mcluster_def([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcarol\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/driver.py:510\u001b[0m, in \u001b[0;36minit\u001b[0;34m(parties, address, cluster_config, num_cpus, num_gpus, log_to_driver, omp_num_threads, logging_level, job_name, cross_silo_comm_backend, cross_silo_comm_options, enable_waiting_for_other_parties_ready, tls_config, auth_manager_config, party_key_pair, tee_simulation, debug_mode, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m address \u001b[38;5;129;01mand\u001b[39;00m omp_num_threads:\n\u001b[1;32m    508\u001b[0m             os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00momp_num_threads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 510\u001b[0m         \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m            \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_to_driver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m     global_state\u001b[38;5;241m.\u001b[39mset_parties(parties\u001b[38;5;241m=\u001b[39mparties)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/ray/_private/worker.py:1540\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1538\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(info_str)\n\u001b[0;32m-> 1540\u001b[0m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_to_driver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver_object_store_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_driver_object_store_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_private\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entrypoint_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_config \u001b[38;5;129;01mand\u001b[39;00m job_config\u001b[38;5;241m.\u001b[39mcode_search_path:\n\u001b[1;32m   1553\u001b[0m     global_worker\u001b[38;5;241m.\u001b[39mset_load_code_from_local(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/ray/_private/worker.py:2028\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint)\u001b[0m\n\u001b[1;32m   2026\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2027\u001b[0m     logs_dir \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mget_logs_dir_path()\n\u001b[0;32m-> 2028\u001b[0m worker\u001b[38;5;241m.\u001b[39mcore_worker \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raylet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCoreWorker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplasma_store_socket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraylet_socket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_ip_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_manager_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraylet_ip_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLOCAL_MODE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stdout_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stderr_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized_job_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_agent_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruntime_env_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartup_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSCRIPT_MODE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;66;03m# Notify raylet that the core worker is ready.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mnotify_raylet()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "#Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "#Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup (remains the same) ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing (remains the same) ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy playing with my son and dog ', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function (YOUR FUNCTION from previous response) ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00e3372-1a31-4db5-99d8-c2faae3a68bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first time I saw the new version of the game, I was so excited. I was so excited to see the new version of the game,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id]]) #Start with BOS token\n",
    "    output = model.generate(input_ids, max_length=32)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de9b947-b0b6-4ab9-8b19-39174b53e30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 05:15:45,577\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=83619)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=83619)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=83619)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=83619)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=83619)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '11', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '0', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=83843)\u001b[0m [2024-12-01 05:16:56.071] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=90514)\u001b[0m 2024-12-01 05:17:29.830 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=90512)\u001b[0m 2024-12-01 05:17:29.830 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=90516)\u001b[0m 2024-12-01 05:17:29.830 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: The first time I saw the new version of the game, I was so excited. I was so excited to see the new version of the game,June accumulating\n",
      "Time taken for prediction and generation: 19.3499 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "#Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "#Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup (remains the same) ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing (remains the same) ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('The first time I saw the new version of the game, I was so excited. I was so excited to see the new version of the game,', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function (YOUR FUNCTION from previous response) ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076c85be-cce7-4bb0-9e50-b62b02ba048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-01 04:48:20,936\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=53443)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=53443)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=53443)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=53443)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=53443)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '6', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '11', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=53439)\u001b[0m [2024-12-01 04:49:09.195] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=60075)\u001b[0m 2024-12-01 04:49:42.082 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=60077)\u001b[0m 2024-12-01 04:49:42.082 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=60079)\u001b[0m 2024-12-01 04:49:42.082 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy playing with my 185Vo\n",
      "Time taken for prediction and generation: 18.5060 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "#Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# PUMA's GELU approximation\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    a_coeffs = jnp.array(\n",
    "        [-0.5054031199708174, -0.42226581151983866, -0.11807612951181953, -0.011034134030615728]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [0.008526321541038084, 0.5, 0.3603292692789629, 0.0, -0.037688200365904236, 0.0, 0.0018067462606141187]\n",
    "    )\n",
    "\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x6 + b_coeffs[4] * x4 + b_coeffs[2] * x2 + b_coeffs[1] * x + b_coeffs[0]\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "    return ret\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy playing with my', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f850916c-3679-456a-a9d5-27e5074b60bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-01 05:36:49,439\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=117864)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=117864)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=117864)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=117864)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=117864)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '11', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '7', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=117634)\u001b[0m [2024-12-01 05:37:58.404] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=124580)\u001b[0m 2024-12-01 05:38:33.208 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=124582)\u001b[0m 2024-12-01 05:38:33.207 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=124578)\u001b[0m 2024-12-01 05:38:33.208 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: The first time I saw the new version of the game, I was so excited. I was so excited to see the new version of the game, visionaryIrish\n",
      "Time taken for prediction and generation: 20.0628 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "#Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# PUMA's GELU approximation\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    a_coeffs = jnp.array(\n",
    "        [-0.5054031199708174, -0.42226581151983866, -0.11807612951181953, -0.011034134030615728]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [0.008526321541038084, 0.5, 0.3603292692789629, 0.0, -0.037688200365904236, 0.0, 0.0018067462606141187]\n",
    "    )\n",
    "\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x6 + b_coeffs[4] * x4 + b_coeffs[2] * x2 + b_coeffs[1] * x + b_coeffs[0]\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "    return ret\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('The first time I saw the new version of the game, I was so excited. I was so excited to see the new version of the game,', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- text_generation function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0584db-d58a-4d58-aeb6-79b713ffdad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 06:01:29,548\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=152709)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=152709)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=152709)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=152709)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=152709)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '4', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '10', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=152618)\u001b[0m [2024-12-01 06:02:41.132] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=159306)\u001b[0m 2024-12-01 06:03:18.029 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=159308)\u001b[0m 2024-12-01 06:03:18.029 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=159310)\u001b[0m 2024-12-01 06:03:18.029 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "Predicted Text: I enjoy walking with my cute dog Baltimoreaking\n",
      "Time taken for prediction and generation: 22.0298 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Union, Tuple\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):  # Generate 10 tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]  # Get logits for the last token\n",
    "        next_token_probs = hack_softmax(next_token_logits)  # Apply softmax to logits\n",
    "        next_token = jnp.argmax(next_token_probs)  # Get the most probable token\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)  # Add token to the sequence\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b16d27-0e12-4238-b7ae-a4b7878a7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2024-12-01 06:09:10,689\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=182706)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=182706)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=182706)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=182706)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=182706)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '10', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '7', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=182663)\u001b[0m [2024-12-01 06:10:19.216] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=189216)\u001b[0m 2024-12-01 06:11:19.857 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=189213)\u001b[0m 2024-12-01 06:11:19.857 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=189219)\u001b[0m 2024-12-01 06:11:19.857 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup (remains the same) ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing (remains the same) ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# --- Temperature Sampling for Controlled Token Generation ---\n",
    "def sample_next_token(logits, temperature=1.0):\n",
    "    \"\"\"Sample the next token based on temperature.\"\"\"\n",
    "    # Apply temperature scaling to the logits\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = hack_softmax(logits)\n",
    "    \n",
    "    # Sample from the probabilities\n",
    "    return jax.random.choice(jax.random.PRNGKey(0), len(logits), p=probabilities)\n",
    "\n",
    "# --- Text Generation Function ---\n",
    "def text_generation(input_ids, params, temperature=1.0, max_length=20):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(max_length):  # Generate up to max_length tokens\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]  # Get logits for the last token\n",
    "        \n",
    "        # Use temperature sampling to get the next token\n",
    "        next_token = sample_next_token(next_token_logits, temperature=temperature)\n",
    "        \n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)  # Add token to the sequence\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "# --- Main Execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_, temperature=0.7  # Adjust temperature here\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3cb14-aa09-477e-8a5b-3f382da1a080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73764799-e6fe-49c2-afdd-5c16e6ccad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 04:54:56,133\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=73194)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=73194)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=73194)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=73194)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=73194)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '6', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '10', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=72927)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "sf.shutdown()\n",
    "# --- Compiler Options ---\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "#Improved hack_softmax\n",
    "def hack_softmax(x: Array, axis: Optional[Union[int, Tuple[int, ...]]] = -1) -> Array:\n",
    "    x = x - jnp.max(x, axis=axis, keepdims=True)\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "\n",
    "#Improved hack_gelu using piecewise polynomial\n",
    "# --- REPLACE WITH YOUR COEFFICIENTS ---\n",
    "F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "def hack_gelu(x: Array) -> Array:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# --- SecretFlow Setup ---\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# --- Model Loading and Preprocessing ---\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy', return_tensors='jax')\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "model_params_, input_token_ids_ = model_params.to(spu), input_token_ids.to(spu)\n",
    "\n",
    "# --- text_generation function ---\n",
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(2):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids\n",
    "\n",
    "# --- Main execution ---\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9355d2-7638-4142-81cb-ba3e84433188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-12-01 00:15:37,777\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=139149)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=139149)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=139149)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=139149)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=139149)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '2', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '3', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'secretflow.utils.testing' has no attribute 'get_copts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hack_softmax_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhijack jax softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), hack_gelu_context(\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhack jax gelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    111\u001b[0m ):\n\u001b[1;32m    112\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start timing\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     output_token_ids \u001b[38;5;241m=\u001b[39m spu(text_generation, copts\u001b[38;5;241m=\u001b[39m\u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_copts\u001b[49m())(\n\u001b[1;32m    114\u001b[0m         input_token_ids_, model_params_\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End timing\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Reveal and print results\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'secretflow.utils.testing' has no attribute 'get_copts'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=139020)\u001b[0m [2024-12-01 00:16:57.599] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "from contextlib import contextmanager\n",
    "from transformers import FlaxGPT2LMHeadModel, AutoTokenizer\n",
    "sf.shutdown()\n",
    "# Define the hack_softmax function\n",
    "def hack_softmax(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n",
    "    x = x - x_max\n",
    "\n",
    "    b = x > -14\n",
    "    nexp = jnp.exp(x)\n",
    "    divisor = jnp.sum(nexp, axis, where=where, keepdims=True)\n",
    "\n",
    "    return b * (nexp / divisor)\n",
    "\n",
    "# Context manager for hack_softmax\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "# Define the hack_gelu function\n",
    "def hack_gelu(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True\n",
    "    b4 = b0 ^ b1\n",
    "\n",
    "    a_coeffs = jnp.array(\n",
    "        [-0.5054031199708174, -0.42226581151983866, -0.11807612951181953, -0.011034134030615728]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [0.008526321541038084, 0.5, 0.3603292692789629, 0.0, -0.037688200365904236, 0.0, 0.0018067462606141187]\n",
    "    )\n",
    "\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x6 + b_coeffs[4] * x4 + b_coeffs[2] * x2 + b_coeffs[1] * x + b_coeffs[0]\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "    return ret\n",
    "\n",
    "# Context manager for hack_gelu\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# Initialize SecretFlow\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# Function to get model parameters\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "# Function to get input token IDs\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy', return_tensors='jax')\n",
    "\n",
    "# Load model parameters and input tokens\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Transfer to SPU\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# Measure prediction and generation time\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()  # Start timing\n",
    "    output_token_ids = spu(text_generation, copts=sf.utils.testing.get_copts())(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()  # End timing\n",
    "\n",
    "# Reveal and print results\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Reinitialize tokenizer for decoding\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b12e8b-fab6-4c32-989a-7fd605ad07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-11-30 23:48:37,167\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=128358)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=128358)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=128358)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=128358)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=128358)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '3', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '1', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=128506)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "from contextlib import contextmanager\n",
    "from transformers import FlaxGPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Shutdown SecretFlow if already initialized\n",
    "sf.shutdown()\n",
    "\n",
    "# Define the hack_softmax function\n",
    "def hack_softmax(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n",
    "    x = x - x_max\n",
    "\n",
    "    b = x > -14\n",
    "    nexp = jnp.exp(x)\n",
    "    divisor = jnp.sum(nexp, axis, where=where, keepdims=True)\n",
    "\n",
    "    return b * (nexp / divisor)\n",
    "\n",
    "# Context manager for hack_softmax\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "# Define the hack_gelu function\n",
    "def hack_gelu(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True\n",
    "    b4 = b0 ^ b1\n",
    "\n",
    "    a_coeffs = jnp.array(\n",
    "        [-0.5054031199708174, -0.42226581151983866, -0.11807612951181953, -0.011034134030615728]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [0.008526321541038084, 0.5, 0.3603292692789629, 0.0, -0.037688200365904236, 0.0, 0.0018067462606141187]\n",
    "    )\n",
    "\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x6 + b_coeffs[4] * x4 + b_coeffs[2] * x2 + b_coeffs[1] * x + b_coeffs[0]\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "    return ret\n",
    "\n",
    "# Context manager for hack_gelu\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# Initialize SecretFlow\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# Function to get model parameters\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "# Function to get input token IDs\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy', return_tensors='jax')\n",
    "\n",
    "# Load model parameters and input tokens\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Transfer to SPU\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# Function for text generation using the model\n",
    "def text_generation(input_token_ids_, model_params_):\n",
    "    # Here you will use your model to generate text, for example:\n",
    "    output = FlaxGPT2LMHeadModel.apply(\n",
    "        model_params_,\n",
    "        input_token_ids_,\n",
    "        train=False,\n",
    "        rngs={'dropout': jax.random.PRNGKey(0)}\n",
    "    )\n",
    "    return output.logits\n",
    "\n",
    "# Measure prediction and generation time\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()  # Start timing\n",
    "    output_token_ids = spu(text_generation)(input_token_ids_, model_params_)\n",
    "    end_time = time.time()  # End timing\n",
    "\n",
    "# Reveal and print results\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Reinitialize tokenizer for decoding\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b548ae5d-d860-405b-9f9b-037dced3adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "import jax.nn as jnn\n",
    "import flax.linen as nn\n",
    "from flax.linen.linear import Array\n",
    "import jax\n",
    "import argparse\n",
    "import spu.utils.distributed as ppd\n",
    "import spu.intrinsic as intrinsic\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "from contextlib import contextmanager\n",
    "sf.shutdown()\n",
    "copts = spu_pb2.CompilerOptions()\n",
    "copts.enable_pretty_print = False\n",
    "copts.xla_pp_kind = 2\n",
    "# enable x / broadcast(y) -> x * broadcast(1/y)\n",
    "copts.enable_optimize_denominator_with_broadcast = True\n",
    "Array = Any\n",
    "\n",
    "# In case you have a running secretflow runtime already.\n",
    "sf.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae9c429-a084-4349-91b3-1fccd83ea3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hack_softmax(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n",
    "    x = x - x_max\n",
    "\n",
    "    # exp on large negative is clipped to zero\n",
    "    b = x > -14\n",
    "    nexp = jnp.exp(x)\n",
    "\n",
    "    divisor = jnp.sum(nexp, axis, where=where, keepdims=True)\n",
    "\n",
    "    return b * (nexp / divisor)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # hijack some target functions\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    # recover back\n",
    "    jnn.softmax = raw_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6081ca76-3881-4ce0-bbe8-465741f0da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hack_gelu(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    # seg1 = a[3] * x^3 + a[2] * x^2 + a[1] * x + a[0]\n",
    "    # seg2 = b[6] * x^6 + b[4] * x^4 + b[2] * x^2 + b[1] * x + b[0]\n",
    "    a_coeffs = jnp.array(\n",
    "        [\n",
    "            -0.5054031199708174,\n",
    "            -0.42226581151983866,\n",
    "            -0.11807612951181953,\n",
    "            -0.011034134030615728,\n",
    "        ]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [\n",
    "            0.008526321541038084,\n",
    "            0.5,\n",
    "            0.3603292692789629,\n",
    "            0.0,\n",
    "            -0.037688200365904236,\n",
    "            0.0,\n",
    "            0.0018067462606141187,\n",
    "        ]\n",
    "    )\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = (\n",
    "        b_coeffs[6] * x6\n",
    "        + b_coeffs[4] * x4\n",
    "        + b_coeffs[2] * x2\n",
    "        + b_coeffs[1] * x\n",
    "        + b_coeffs[0]\n",
    "    )\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # hijack some target functions\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    # recover back\n",
    "    jnn.gelu = raw_gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e125e44a-af1c-46ba-99df-0214512e2c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-11-30 22:56:49,265\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=2985)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2985)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2985)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=2985)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=2985)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '11', 'attn', 'bias'), ('h', '3', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '10', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '8', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=2917)\u001b[0m [2024-11-30 22:57:56.650] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=alice) pid=9763)\u001b[0m 2024-11-30 22:58:39.095 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=bob) pid=9765)\u001b[0m 2024-11-30 22:58:39.095 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n",
      "\u001b[2m\u001b[36m(SPURuntime(device_id=None, party=carol) pid=9767)\u001b[0m 2024-11-30 22:58:39.095 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"
     ]
    }
   ],
   "source": [
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    output_token_ids = spu(text_generation, copts=copts)(\n",
    "        input_token_ids_, model_params_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65867ac-31f7-4bf4-8c34-2b74c8c00bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Run on SPU:\n",
      "-----------------------------------------------------------------\n",
      "I enjoy walking with my cute dogohydrateahs Lore erase iterkhThumbnailravel explorer Uz\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b19cb-665c-418f-a1d2-a09682fddb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-11-30 23:08:01,165\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(_run pid=63755)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63755)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63755)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=63755)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=63755)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m Some weights of the model checkpoint at gpt2 were not used when initializing FlaxGPT2LMHeadModel: {('h', '10', 'attn', 'bias'), ('h', '2', 'attn', 'bias'), ('h', '4', 'attn', 'bias'), ('h', '9', 'attn', 'bias'), ('h', '5', 'attn', 'bias'), ('h', '1', 'attn', 'bias'), ('h', '11', 'attn', 'bias'), ('h', '8', 'attn', 'bias'), ('h', '6', 'attn', 'bias'), ('h', '0', 'attn', 'bias'), ('h', '7', 'attn', 'bias'), ('h', '3', 'attn', 'bias')}\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m - This IS expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_run pid=63844)\u001b[0m - This IS NOT expected if you are initializing FlaxGPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import secretflow as sf\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.linen.linear import Array\n",
    "from contextlib import contextmanager\n",
    "from transformers import FlaxGPT2LMHeadModel, AutoTokenizer\n",
    "sf.shutdown()\n",
    "# Define the hack_softmax function\n",
    "def hack_softmax(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n",
    "    x = x - x_max\n",
    "\n",
    "    b = x > -14\n",
    "    nexp = jnp.exp(x)\n",
    "    divisor = jnp.sum(nexp, axis, where=where, keepdims=True)\n",
    "\n",
    "    return b * (nexp / divisor)\n",
    "\n",
    "# Context manager for hack_softmax\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = jnn.softmax\n",
    "    jnn.softmax = hack_softmax\n",
    "    yield\n",
    "    jnn.softmax = raw_softmax\n",
    "\n",
    "# Define the hack_gelu function\n",
    "def hack_gelu(\n",
    "    x: Array,\n",
    "    axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n",
    "    where: Optional[Array] = None,\n",
    "    initial: Optional[Array] = None,\n",
    ") -> Array:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True\n",
    "    b4 = b0 ^ b1\n",
    "\n",
    "    a_coeffs = jnp.array(\n",
    "        [-0.5054031199708174, -0.42226581151983866, -0.11807612951181953, -0.011034134030615728]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [0.008526321541038084, 0.5, 0.3603292692789629, 0.0, -0.037688200365904236, 0.0, 0.0018067462606141187]\n",
    "    )\n",
    "\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x6 + b_coeffs[4] * x4 + b_coeffs[2] * x2 + b_coeffs[1] * x + b_coeffs[0]\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "    return ret\n",
    "\n",
    "# Context manager for hack_gelu\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = jnn.gelu\n",
    "    jnn.gelu = hack_gelu\n",
    "    yield\n",
    "    jnn.gelu = raw_gelu\n",
    "\n",
    "# Initialize SecretFlow\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['protocol'] = 'ABY3'\n",
    "conf['runtime_config']['field'] = 'FM64'\n",
    "conf['runtime_config']['fxp_exp_mode'] = 0\n",
    "conf['runtime_config']['fxp_exp_iters'] = 5\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "# Function to get model parameters\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "# Function to get input token IDs\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('I enjoy', return_tensors='jax')\n",
    "\n",
    "# Load model parameters and input tokens\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "# Transfer to SPU\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "# Measure prediction and generation time\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    start_time = time.time()  # Start timing\n",
    "    output_token_ids = spu(text_generation, copts=sf.utils.testing.get_copts())(\n",
    "        input_token_ids_, model_params_\n",
    "    )\n",
    "    end_time = time.time()  # End timing\n",
    "\n",
    "# Reveal and print results\n",
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "elapsed_time = end_time - start_time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Reinitialize tokenizer for decoding\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(f\"Predicted Text: {tokenizer.decode(outputs_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Time taken for prediction and generation: {elapsed_time:.4f} seconds\")\n",
    "print('-' * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19ffd58-d57e-4149-9195-708dd8ed7295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the \"a\" and \"b\" in the \"-\" and \"-\" in the \"-\" and \"-\" in the \"-\" and, except in the, the, and, the, the, the\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Apply dynamic quantization\n",
    "model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Example inference\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56b24fe-65ce-459b-a209-f50c994db49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Token: e\n",
      "Inference Time: 0.0218 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Apply dynamic quantization\n",
    "model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate only one token (add one token to the input)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + 1,  # Generate only one additional token\n",
    "        temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "        top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "        top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "        repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "        no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "        pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "    )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated token and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Token: {generated_text[-1]}\")  # Print only the last token\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95018b77-e5f2-40f0-8a29-3869e67a0730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:312: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Token: f\n",
      "Inference Time: 0.3935 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Load GPT-2 base model or distilGPT-2 (smaller variant)\n",
    "model_name = \"distilgpt2\"  # Smaller variant (distilGPT-2)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Apply pruning: Prune 20% of the model's attention heads and other linear layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)  # Prune 20% of the weights\n",
    "\n",
    "# Apply dynamic quantization manually to individual layers\n",
    "# For example, quantize Linear layers dynamically without deepcopy\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        # Apply quantization only to linear layers that support it\n",
    "        torch.quantization.prepare(module, inplace=True)\n",
    "        torch.quantization.convert(module, inplace=True)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Enable FP16 for automatic mixed precision (AMP)\n",
    "scaler = torch.cuda.amp.GradScaler()  # Used for mixed-precision training (AMP)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate only one token (add one token to the input)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 1,  # Generate only one additional token\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated token and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Token: {generated_text[-1]}\")  # Print only the last token\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1e7aa2-81aa-4cc0-88c0-08219ec92fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Token: f\n",
      "Inference Time: 0.0119 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load DistilGPT-2 model\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate only one token\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + 1,  # Generate only one additional token\n",
    "        temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "        top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "        top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "        repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "        no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "        pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "    )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated token and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Token: {generated_text[-1]}\")  # Print only the last token\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e83788-f6a5-4d81-8d21-d0433f20a42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks.\n",
      "The main goal of this project was to\n",
      "Inference Time: 0.0849 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1181/2391101613.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Used for mixed-precision training (AMP)\n",
      "/tmp/ipykernel_1181/2391101613.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Load GPT-2 base model or distilGPT-2 (smaller variant)\n",
    "model_name = \"distilgpt2\"  # Smaller variant (distilGPT-2)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Apply pruning: Prune 20% of the model's attention heads and other linear layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)  # Prune 20% of the weights\n",
    "\n",
    "# Apply dynamic quantization manually to individual layers\n",
    "# For example, quantize Linear layers dynamically without deepcopy\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        # Apply quantization only to linear layers that support it\n",
    "        torch.quantization.prepare(module, inplace=True)\n",
    "        torch.quantization.convert(module, inplace=True)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Enable FP16 for automatic mixed precision (AMP)\n",
    "scaler = torch.cuda.amp.GradScaler()  # Used for mixed-precision training (AMP)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 10,  # Generate 10 additional tokens\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Text: {generated_text}\")  # Print all generated tokens\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e28e7a3-2330-4a40-a2e5-cd685eff2264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks.\n",
      "The main goal of this project was to\n",
      "Inference Time: 0.0696 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1181/2391101613.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Used for mixed-precision training (AMP)\n",
      "/tmp/ipykernel_1181/2391101613.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Load GPT-2 base model or distilGPT-2 (smaller variant)\n",
    "model_name = \"distilgpt2\"  # Smaller variant (distilGPT-2)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Apply pruning: Prune 20% of the model's attention heads and other linear layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)  # Prune 20% of the weights\n",
    "\n",
    "# Apply dynamic quantization manually to individual layers\n",
    "# For example, quantize Linear layers dynamically without deepcopy\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        # Apply quantization only to linear layers that support it\n",
    "        torch.quantization.prepare(module, inplace=True)\n",
    "        torch.quantization.convert(module, inplace=True)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Enable FP16 for automatic mixed precision (AMP)\n",
    "scaler = torch.cuda.amp.GradScaler()  # Used for mixed-precision training (AMP)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 10,  # Generate 10 additional tokens\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Text: {generated_text}\")  # Print all generated tokens\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f43264c-ee9d-42e4-b8a4-3bdcd125dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.0324 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 base model or distilGPT-2 (smaller variant)\n",
    "model_name = \"distilgpt2\"  # Smaller variant (distilGPT-2)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + 10,  # Generate 10 additional tokens\n",
    "        temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "        top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "        top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "        repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "        no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "        pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "    )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Output the inference time\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3776580a-694f-4d9b-a537-61ffda7dc709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks. The main goal of this project was to create an easy and efficient way to generate the data from various types of objects, including those that are not currently available in Java or C++ (e3).\n",
      "The first step involved creating two classes: class A which uses both type inference as well as generics with respect only one parameter; Class B where we use all parameters except its own constructor instead of using any other method on it such methods can be used without having access to either another instance's object directly at runtime but also through multiple instances each time they have been called together via different implementations like ArrayList<T> , List <String>, etc. In order make sure you don't need to worry about calling these functions manually because there will always still be some extra overhead when accessing them yourself!\n",
      "Average Inference Time: 0.5090 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 base model or distilGPT-2 (smaller variant)\n",
    "model_name = \"distilgpt2\"  # Smaller variant (distilGPT-2)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Perform inference 5 times and calculate the average time\n",
    "num_inferences = 5\n",
    "total_time = 0.0\n",
    "\n",
    "for _ in range(num_inferences):\n",
    "    # Measure inference time for each run\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 500,  # Generate 10 additional tokens\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "    # Calculate the inference time for this run\n",
    "    end_time = time.time()\n",
    "    total_time += (end_time - start_time)\n",
    "\n",
    "# Calculate the average inference time\n",
    "average_inference_time = total_time / num_inferences\n",
    "\n",
    "# Output the results\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated Text: {generated_text}\")  # Print the entire generated text\n",
    "print(f\"Average Inference Time: {average_inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4f4856-b3d7-49af-add1-4e25afd3b720",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hack_softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Use the custom softmax and gelu functions within the context managers\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hack_softmax_context(msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing custom softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), hack_gelu_context(msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing custom GELU\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    120\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m    122\u001b[0m             max_length\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# Generate 10 additional tokens\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m             pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50256\u001b[39m,     \u001b[38;5;66;03m# Ensure pad token ID is set (typically EOS for GPT-2)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Calculate inference time\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mhack_softmax_context\u001b[0;34m(msg, enabled)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Hijack the softmax function\u001b[39;00m\n\u001b[1;32m     32\u001b[0m raw_softmax \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax\n\u001b[0;32m---> 33\u001b[0m F\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m \u001b[43mhack_softmax\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Recover the original softmax function\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hack_softmax' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from contextlib import contextmanager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define the custom hack_softmax (using PyTorch)\n",
    "def softmax(\n",
    "    x: torch.Tensor,\n",
    "    dim: int = -1,\n",
    "    dtype: torch.dtype = None\n",
    ") -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim, keepdim=True)[0]\n",
    "    x = x - x_max\n",
    "\n",
    "    # exp on large negative is clipped to zero\n",
    "    b = x > -14\n",
    "    nexp = torch.exp(x)\n",
    "\n",
    "    divisor = torch.sum(nexp, dim, keepdim=True)\n",
    "\n",
    "    return b * (nexp / divisor)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the softmax function\n",
    "    raw_softmax = F.softmax\n",
    "    F.softmax = hack_softmax\n",
    "    yield\n",
    "    # Recover the original softmax function\n",
    "    F.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Define the custom hack_gelu (using PyTorch)\n",
    "def hack_gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    # Polynomial approximation coefficients for GELU\n",
    "    a_coeffs = torch.tensor(\n",
    "        [\n",
    "            -0.5054031199708174,\n",
    "            -0.42226581151983866,\n",
    "            -0.11807612951181953,\n",
    "            -0.011034134030615728,\n",
    "        ]\n",
    "    )\n",
    "    b_coeffs = torch.tensor(\n",
    "        [\n",
    "            0.008526321541038084,\n",
    "            0.5,\n",
    "            0.3603292692789629,\n",
    "            0.0,\n",
    "            -0.037688200365904236,\n",
    "            0.0,\n",
    "            0.0018067462606141187,\n",
    "        ]\n",
    "    )\n",
    "    x2 = torch.square(x)\n",
    "    x3 = x * x2\n",
    "    x4 = torch.square(x2)\n",
    "    x6 = torch.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = (\n",
    "        b_coeffs[6] * x6\n",
    "        + b_coeffs[4] * x4\n",
    "        + b_coeffs[2] * x2\n",
    "        + b_coeffs[1] * x\n",
    "        + b_coeffs[0]\n",
    "    )\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the GELU function\n",
    "    raw_gelu = torch.nn.functional.gelu\n",
    "    torch.nn.functional.gelu = hack_gelu\n",
    "    yield\n",
    "    # Recover the original GELU function\n",
    "    torch.nn.functional.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# Load DistilGPT-2 model\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Use the custom softmax and gelu functions within the context managers\n",
    "    with hack_softmax_context(msg=\"Using custom softmax\", enabled=True), hack_gelu_context(msg=\"Using custom GELU\", enabled=True):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 100,  # Generate 10 additional tokens\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Text: {generated_text}\")  # Print the entire generated text\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0ae1a4-7c59-4a52-8ef9-622ce2561cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks. The main goal of this project was to create an easy and efficient way to generate the data from various types of objects, including those that are not currently available in Java or C++ (e3).\n",
      "The first step involved creating two classes: class A which uses both type inference as well as generics with respect only one parameter; Class B where we use all parameters except its own constructor instead of using any other method on it such methods can be used without having access to either another instance's\n",
      "Inference Time: 0.4519 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from contextlib import contextmanager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Polynomial coefficients (DOUBLE-CHECK these values for accuracy!)\n",
    "F0_coeffs = torch.tensor([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "F1_coeffs = torch.tensor([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "F2_coeffs = torch.tensor([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "F3_coeffs = torch.tensor([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "\n",
    "def piecewise_polynomial_gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * x**2 + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * x**2 + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * x**2 + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * x**2 + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = torch.where(less_than_minus_2, F0_x,\n",
    "                          torch.where(between_minus_2_and_0, F1_x,\n",
    "                                      torch.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = torch.nn.functional.gelu\n",
    "    torch.nn.functional.gelu = piecewise_polynomial_gelu\n",
    "    yield\n",
    "    torch.nn.functional.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# Custom softmax (with improved overflow handling)\n",
    "def softmax(x: torch.Tensor, dim: int = -1, dtype: torch.dtype = None) -> torch.Tensor:\n",
    "    x = x - x.max(dim=dim, keepdim=True).values\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=dim, keepdim=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = F.softmax\n",
    "    F.softmax = softmax\n",
    "    yield\n",
    "    F.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# --- Model Loading and Generation ---\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with hack_softmax_context(msg=\"Using custom softmax\", enabled=True), hack_gelu_context(msg=\"Using custom GELU\", enabled=True):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 100,  #Reduced for faster runtime\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09dcaaf3-bfd3-48fd-9fb1-27588a611cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks. The main goal of this project was to create an easy and efficient way to generate the data from various types of objects, including those that are not currently available in Java or C++ (e3).\n",
      "The first step involved creating two classes: class A which uses both type inference as well as generics with respect only one parameter; Class B where we use all parameters except its own constructor instead of using any other method on it such methods can be used without having access to either another instance's object directly at runtime but also through multiple instances each time they have been called together via different implementations like ArrayList<T> , List <String>, etc. In order make sure you don't need to worry about calling these functions manually because there will always still be some extra overhead when accessing them yourself!\n",
      "Inference Time: 0.7185 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from contextlib import contextmanager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Polynomial coefficients (double-check accuracy from the image!)\n",
    "F0_coeffs = torch.tensor([-0.011034134030615728, -0.11807612951181953, -0.42226581151983866, -0.5054031199708174])\n",
    "F1_coeffs = torch.tensor([0.0018067462606141187, -0.037688200365904236, 0.3603292692789629, 0.5, 0.008526321541038084])\n",
    "\n",
    "def piecewise_polynomial_gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    less_than_minus_4 = x < -4\n",
    "    between_minus_4_and_minus_1p95 = (x >= -4) & (x < -1.95)\n",
    "    between_minus_1p95_and_3 = (x >= -1.95) & (x <= 3)\n",
    "    greater_than_3 = x > 3\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * x**2 + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * x**2 + F1_coeffs[3] * x**3 + F1_coeffs[4] * x**4\n",
    "\n",
    "    result = torch.where(less_than_minus_4, torch.tensor(0.0).to(x.device),\n",
    "                          torch.where(between_minus_4_and_minus_1p95, F0_x,\n",
    "                                      torch.where(between_minus_1p95_and_3, F1_x, x)))\n",
    "    return result\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_gelu = torch.nn.functional.gelu\n",
    "    torch.nn.functional.gelu = piecewise_polynomial_gelu\n",
    "    yield\n",
    "    torch.nn.functional.gelu = raw_gelu\n",
    "\n",
    "# Custom softmax (with improved overflow handling)\n",
    "def softmax(x: torch.Tensor, dim: int = -1, dtype: torch.dtype = None) -> torch.Tensor:\n",
    "    x = x - x.max(dim=dim, keepdim=True).values\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=dim, keepdim=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    raw_softmax = F.softmax\n",
    "    F.softmax = softmax\n",
    "    yield\n",
    "    F.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# --- Model Loading and Generation ---\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with hack_softmax_context(msg=\"Using custom softmax\", enabled=True), hack_gelu_context(msg=\"Using custom GELU\", enabled=True):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 500, #Reduced for faster runtime\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0841e9-202f-4e5e-8012-725b27835f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks. The main goal of this project was to create an easy and efficient way to generate the data from various types of objects, including those that are not currently available in Java or C++ (e3).\n",
      "The first step involved creating two classes: class A which uses both type inference as well as generics with respect only one parameter; Class B where we use all parameters except its own constructor instead of using any other method on it such methods can be used without having access to either another instance's object directly at runtime but also through multiple instances each time they have been called together via different implementations like ArrayList<T> , List <String>, etc. In order make sure you don't need to worry about calling these functions manually because there will always still be some extra overhead when accessing them yourself!\n",
      "Inference Time: 0.6632 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from contextlib import contextmanager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define the custom hack_relu (using PyTorch)\n",
    "def hack_relu(x: torch.Tensor) -> torch.Tensor:\n",
    "    return F.relu(x)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_relu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the GELU function and replace it with ReLU\n",
    "    raw_gelu = torch.nn.functional.gelu\n",
    "    torch.nn.functional.gelu = hack_relu\n",
    "    yield\n",
    "    # Recover the original GELU function\n",
    "    torch.nn.functional.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# Load DistilGPT-2 model\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Use the custom ReLU function within the context manager\n",
    "    with hack_relu_context(msg=\"Using custom ReLU\", enabled=True):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 500,  # Generate 500 additional tokens\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "# Calculate inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Text: {generated_text}\")  # Print the entire generated text\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89baa73-2f9d-494f-a753-f0612f72a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks. The main goal of this project was to create an easy and efficient way to generate the data from various types of objects, including those that are not currently available in Java or C++ (e3).\n",
      "The first step involved creating two classes: class A which uses both type inference as well as generics with respect only one parameter; Class B where we use all parameters except its own constructor instead of using any other method on it such methods can be used without having access to either another instance's\n",
      "Inference Time: 0.4173 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from contextlib import contextmanager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimized custom softmax function\n",
    "def hack_softmax(\n",
    "    x: torch.Tensor,\n",
    "    dim: int = -1,\n",
    "    dtype: torch.dtype = None\n",
    ") -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim, keepdim=True)[0]\n",
    "    x = x - x_max\n",
    "\n",
    "    # Avoid large negative values and directly apply the softmax\n",
    "    exp_x = torch.exp(x)\n",
    "    sum_exp_x = torch.sum(exp_x, dim, keepdim=True)\n",
    "\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the softmax function\n",
    "    raw_softmax = F.softmax\n",
    "    F.softmax = hack_softmax\n",
    "    yield\n",
    "    # Recover the original softmax function\n",
    "    F.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Optimized custom GELU function\n",
    "def hack_gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Polynomial approximation for GELU\n",
    "    # Using a faster approximation with fewer operations\n",
    "    x2 = torch.square(x)\n",
    "    x3 = x * x2\n",
    "    x4 = torch.square(x2)\n",
    "\n",
    "    a_coeffs = torch.tensor(\n",
    "        [\n",
    "            -0.5054031199708174,\n",
    "            -0.42226581151983866,\n",
    "            -0.11807612951181953,\n",
    "            -0.011034134030615728,\n",
    "        ]\n",
    "    ).to(x.device)\n",
    "\n",
    "    b_coeffs = torch.tensor(\n",
    "        [\n",
    "            0.008526321541038084,\n",
    "            0.5,\n",
    "            0.3603292692789629,\n",
    "            0.0,\n",
    "            -0.037688200365904236,\n",
    "            0.0,\n",
    "            0.0018067462606141187,\n",
    "        ]\n",
    "    ).to(x.device)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x4 + b_coeffs[4] * x2 + b_coeffs[2] * x + b_coeffs[1]\n",
    "\n",
    "    # Use conditions to determine the appropriate segment\n",
    "    result = torch.where(x < -1.95, seg1, seg2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the GELU function\n",
    "    raw_gelu = torch.nn.functional.gelu\n",
    "    torch.nn.functional.gelu = hack_gelu\n",
    "    yield\n",
    "    # Recover the original GELU function\n",
    "    torch.nn.functional.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# Load DistilGPT-2 model\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Set the model to evaluation mode (ensure no gradient computation)\n",
    "model.eval()\n",
    "\n",
    "# Start the timer for inference only\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "with torch.no_grad():\n",
    "    # Use the custom softmax and gelu functions within the context managers\n",
    "    with hack_softmax_context(msg=\"Using custom softmax\", enabled=True), hack_gelu_context(msg=\"Using custom GELU\", enabled=True):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 100,  # Generate 10 additional tokens\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "# Stop the timer for inference only\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Text: {generated_text}\")  # Print the entire generated text\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")  # Only inference time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a860d044-95c1-4f1e-b506-07e7805e6245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks. The main goal of this project was to create an easy and efficient way to generate the data from various types of objects, including those that are not currently available in Java or C++ (e3).\n",
      "The first step involved creating two classes: class A which uses both type inference as well as generics with respect only one parameter; Class B where we use all parameters except its own constructor instead of using any other method on it such methods can be used without having access to either another instance's\n",
      "Average Inference Time: 0.3005 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from contextlib import contextmanager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define the custom hack_softmax (using PyTorch)\n",
    "def hack_softmax(\n",
    "    x: torch.Tensor,\n",
    "    dim: int = -1,\n",
    "    dtype: torch.dtype = None\n",
    ") -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim, keepdim=True)[0]\n",
    "    x = x - x_max\n",
    "\n",
    "    # exp on large negative is clipped to zero\n",
    "    b = x > -14\n",
    "    nexp = torch.exp(x)\n",
    "\n",
    "    divisor = torch.sum(nexp, dim, keepdim=True)\n",
    "\n",
    "    return b * (nexp / divisor)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the softmax function\n",
    "    raw_softmax = F.softmax\n",
    "    F.softmax = hack_softmax\n",
    "    yield\n",
    "    # Recover the original softmax function\n",
    "    F.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Define the custom hack_gelu (using PyTorch)\n",
    "def hack_gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    # Polynomial approximation coefficients for GELU\n",
    "    a_coeffs = torch.tensor(\n",
    "        [\n",
    "            -0.5054031199708174,\n",
    "            -0.42226581151983866,\n",
    "            -0.11807612951181953,\n",
    "            -0.011034134030615728,\n",
    "        ]\n",
    "    )\n",
    "    b_coeffs = torch.tensor(\n",
    "        [\n",
    "            0.008526321541038084,\n",
    "            0.5,\n",
    "            0.3603292692789629,\n",
    "            0.0,\n",
    "            -0.037688200365904236,\n",
    "            0.0,\n",
    "            0.0018067462606141187,\n",
    "        ]\n",
    "    )\n",
    "    x2 = torch.square(x)\n",
    "    x3 = x * x2\n",
    "    x4 = torch.square(x2)\n",
    "    x6 = torch.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = (\n",
    "        b_coeffs[6] * x6\n",
    "        + b_coeffs[4] * x4\n",
    "        + b_coeffs[2] * x2\n",
    "        + b_coeffs[1] * x\n",
    "        + b_coeffs[0]\n",
    "    )\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the GELU function\n",
    "    raw_gelu = torch.nn.functional.gelu\n",
    "    torch.nn.functional.gelu = hack_gelu\n",
    "    yield\n",
    "    # Recover the original GELU function\n",
    "    torch.nn.functional.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# Load DistilGPT-2 model\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Perform inference 5 times and calculate the average time\n",
    "num_inferences = 5\n",
    "total_time = 0.0\n",
    "\n",
    "for _ in range(num_inferences):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate multiple tokens (e.g., 50 tokens)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Use the custom softmax and gelu functions within the context managers\n",
    "        with hack_softmax_context(msg=\"Using custom softmax\", enabled=True), hack_gelu_context(msg=\"Using custom GELU\", enabled=True):\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=inputs[\"input_ids\"].shape[1] + 100,  # Generate 50 additional tokens\n",
    "                temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "                top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "                top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "                repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "                no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "                pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "            )\n",
    "\n",
    "    # Calculate inference time for this run\n",
    "    end_time = time.time()\n",
    "    total_time += (end_time - start_time)\n",
    "\n",
    "# Calculate the average inference time\n",
    "average_inference_time = total_time / num_inferences\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Text: {generated_text}\")  # Print the entire generated text\n",
    "print(f\"Average Inference Time: {average_inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09043f24-97e2-4e37-b1df-9da1a34c0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: GPT-2 is a transformer model developed by OpenAI for natural language generation tasks. The main goal of this project was to create an easy and efficient way to generate the data from various types of objects, including those that are not currently available in Java or C++ (e3).\n",
      "The first step involved creating two classes: class A which uses both type inference as well as generics with respect only one parameter; Class B where we use all parameters except its own constructor instead of using any other method on it such methods can be used without having access to either another instance's object directly at runtime but also through multiple instances each time they have been called together via different implementations like ArrayList<T> , List <String>, etc. In order make sure you don't need to worry about calling these functions manually because there will always still be some extra overhead when accessing them yourself!\n",
      "Inference Time: 0.5653 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from contextlib import contextmanager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define the custom hack_softmax (using PyTorch)\n",
    "def hack_softmax(\n",
    "    x: torch.Tensor,\n",
    "    dim: int = -1,\n",
    "    dtype: torch.dtype = None\n",
    ") -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim, keepdim=True)[0]\n",
    "    x = x - x_max\n",
    "\n",
    "    # exp on large negative is clipped to zero\n",
    "    b = x > -14\n",
    "    nexp = torch.exp(x)\n",
    "\n",
    "    divisor = torch.sum(nexp, dim, keepdim=True)\n",
    "\n",
    "    return b * (nexp / divisor)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_softmax_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the softmax function\n",
    "    raw_softmax = F.softmax\n",
    "    F.softmax = hack_softmax\n",
    "    yield\n",
    "    # Recover the original softmax function\n",
    "    F.softmax = raw_softmax\n",
    "\n",
    "\n",
    "# Define the custom hack_gelu (using PyTorch)\n",
    "def hack_gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    # Polynomial approximation coefficients for GELU\n",
    "    a_coeffs = torch.tensor(\n",
    "        [\n",
    "            -0.5054031199708174,\n",
    "            -0.42226581151983866,\n",
    "            -0.11807612951181953,\n",
    "            -0.011034134030615728,\n",
    "        ]\n",
    "    )\n",
    "    b_coeffs = torch.tensor(\n",
    "        [\n",
    "            0.008526321541038084,\n",
    "            0.5,\n",
    "            0.3603292692789629,\n",
    "            0.0,\n",
    "            -0.037688200365904236,\n",
    "            0.0,\n",
    "            0.0018067462606141187,\n",
    "        ]\n",
    "    )\n",
    "    x2 = torch.square(x)\n",
    "    x3 = x * x2\n",
    "    x4 = torch.square(x2)\n",
    "    x6 = torch.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = (\n",
    "        b_coeffs[6] * x6\n",
    "        + b_coeffs[4] * x4\n",
    "        + b_coeffs[2] * x2\n",
    "        + b_coeffs[1] * x\n",
    "        + b_coeffs[0]\n",
    "    )\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def hack_gelu_context(msg: str, enabled: bool = False):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # Hijack the GELU function\n",
    "    raw_gelu = torch.nn.functional.gelu\n",
    "    torch.nn.functional.gelu = hack_gelu\n",
    "    yield\n",
    "    # Recover the original GELU function\n",
    "    torch.nn.functional.gelu = raw_gelu\n",
    "\n",
    "\n",
    "# Load DistilGPT-2 model\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU (or CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"GPT-2 is a transformer model developed by OpenAI for natural language generation tasks\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Set the model to evaluation mode (ensure no gradient computation)\n",
    "model.eval()\n",
    "\n",
    "# Start the timer for inference only\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens (e.g., 10 tokens instead of 1)\n",
    "with torch.no_grad()\n",
    "    # Use the custom softmax and gelu functions within the context managers\n",
    "    with hack_softmax_context(msg=\"Using custom softmax\", enabled=True), hack_gelu_context(msg=\"Using custom GELU\", enabled=True):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 500,  # Generate 10 additional tokens\n",
    "            temperature=0.8,        # Control randomness (lower means more deterministic)\n",
    "            top_k=50,               # Use top-k sampling (top 50 most likely tokens)\n",
    "            top_p=0.95,             # Use nucleus sampling (cumulative probability 95%)\n",
    "            repetition_penalty=1.5, # Penalize repetitive sequences\n",
    "            no_repeat_ngram_size=2, # Prevent repeating n-grams\n",
    "            pad_token_id=50256,     # Ensure pad token ID is set (typically EOS for GPT-2)\n",
    "        )\n",
    "\n",
    "# Stop the timer for inference only\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated Text: {generated_text}\")  # Print the entire generated text\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")  # Only inference time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdb486c4-db29-44ab-a1bb-e27f4f16e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.5804 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start the timer for inference only\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate multiple tokens\n",
    "with torch.no_grad():\n",
    "    with hack_softmax_context(msg=\"Using custom softmax\", enabled=True), hack_gelu_context(msg=\"Using custom GELU\", enabled=True):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 500,  # Generate 10 additional tokens\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=50256,\n",
    "        )\n",
    "\n",
    "# Stop the timer for inference\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")  # Only inference time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6541dc-d109-456b-9102-0d16283a190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory Usage: 60.52734375 MB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hack_softmax_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial Memory Usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_memory_usage()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Your code to run the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mhack_softmax_context\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhijack jax softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), hack_gelu_context(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhack jax gelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m ):\n\u001b[1;32m     17\u001b[0m     output_token_ids \u001b[38;5;241m=\u001b[39m spu(text_generation, copts\u001b[38;5;241m=\u001b[39mcopts)(input_token_ids_, model_params_)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Measure memory usage after running the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hack_softmax_context' is not defined"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "# Function to get memory usage in MB\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())  # Get the current process\n",
    "    mem_info = process.memory_info()  # Memory information\n",
    "    return mem_info.rss / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "# Start measuring memory usage\n",
    "print(f\"Initial Memory Usage: {get_memory_usage()} MB\")\n",
    "\n",
    "# Your code to run the model\n",
    "with hack_softmax_context(\"hijack jax softmax\", enabled=True), hack_gelu_context(\n",
    "    \"hack jax gelu\", enabled=True\n",
    "):\n",
    "    output_token_ids = spu(text_generation, copts=copts)(input_token_ids_, model_params_)\n",
    "\n",
    "# Measure memory usage after running the model\n",
    "print(f\"Memory Usage After Inference: {get_memory_usage()} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb395dcd-772c-49a5-9426-c643e11d90c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 03:24:49.129664: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Generate test inputs\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Compute exact GELU values\u001b[39;00m\n\u001b[1;32m     61\u001b[0m exact_values \u001b[38;5;241m=\u001b[39m exact_gelu(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2390\u001b[0m, in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m   2388\u001b[0m num \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mconcrete_or_error(operator\u001b[38;5;241m.\u001b[39mindex, num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument of jnp.linspace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2389\u001b[0m axis \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mconcrete_or_error(operator\u001b[38;5;241m.\u001b[39mindex, axis, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument of jnp.linspace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_linspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from time import time\n",
    "\n",
    "# Define the exact GELU function\n",
    "def exact_gelu(x):\n",
    "    return 0.5 * x * (1 + jnp.tanh(jnp.sqrt(2 / jnp.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "# Define Method 1 (Piecewise Polynomial Approximation)\n",
    "def hack_gelu_method1(x):\n",
    "    F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "    F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "    F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "    F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "# Define Method 2 (PUMA's GELU Approximation)\n",
    "def hack_gelu_method2(x):\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    a_coeffs = jnp.array(\n",
    "        [-0.5054031199708174, -0.42226581151983866, -0.11807612951181953, -0.011034134030615728]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [0.008526321541038084, 0.5, 0.3603292692789629, 0.0, -0.037688200365904236, 0.0, 0.0018067462606141187]\n",
    "    )\n",
    "\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x6 + b_coeffs[4] * x4 + b_coeffs[2] * x2 + b_coeffs[1] * x + b_coeffs[0]\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "    return ret\n",
    "\n",
    "# Generate test inputs\n",
    "x = jnp.linspace(-5, 5, 1000)\n",
    "\n",
    "# Compute exact GELU values\n",
    "exact_values = exact_gelu(x)\n",
    "\n",
    "# Compute approximate GELU values using both methods\n",
    "approx_method1 = hack_gelu_method1(x)\n",
    "approx_method2 = hack_gelu_method2(x)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) between the approximations and the exact values\n",
    "mse_method1 = jnp.mean((exact_values - approx_method1)**2)\n",
    "mse_method2 = jnp.mean((exact_values - approx_method2)**2)\n",
    "\n",
    "print(f\"Mean Squared Error (Method 1): {mse_method1}\")\n",
    "print(f\"Mean Squared Error (Method 2): {mse_method2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89008fa8-7f30-4614-91e7-cf99c1547497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 03:26:13.669528: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Check GPU availability and run a simple operation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevices()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mdot(x, x)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mdevice())\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2161\u001b[0m, in \u001b[0;36mones\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m   2159\u001b[0m shape \u001b[38;5;241m=\u001b[39m canonicalize_shape(shape)\n\u001b[1;32m   2160\u001b[0m dtypes\u001b[38;5;241m.\u001b[39mcheck_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mones\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_jnp_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/lax/lax.py:1206\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype)\u001b[0m\n\u001b[1;32m   1204\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype \u001b[38;5;129;01mor\u001b[39;00m _dtype(fill_value))\n\u001b[1;32m   1205\u001b[0m fill_value \u001b[38;5;241m=\u001b[39m _convert_element_type(fill_value, dtype, weak_type)\n\u001b[0;32m-> 1206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/lax/lax.py:768\u001b[0m, in \u001b[0;36mbroadcast\u001b[0;34m(operand, sizes)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Broadcasts an array, adding new leading dimensions\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  jax.lax.broadcast_in_dim : add new dimensions at any location in the array shape.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sizes), \u001b[38;5;28mlen\u001b[39m(sizes) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mndim(operand)))\n\u001b[0;32m--> 768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbroadcast_in_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/lax/lax.py:797\u001b[0m, in \u001b[0;36mbroadcast_in_dim\u001b[0;34m(operand, shape, broadcast_dimensions)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m   dyn_shape, static_shape \u001b[38;5;241m=\u001b[39m [], shape  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbroadcast_in_dim_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstatic_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbroadcast_dimensions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:132\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m   in_avals, in_shardings \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39munzip2([arg_spec(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args])\n\u001b[0;32m--> 132\u001b[0m   compiled_fun \u001b[38;5;241m=\u001b[39m \u001b[43mxla_primitive_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOrigShardings\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    135\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/util.py:284\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/util.py:277\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:223\u001b[0m, in \u001b[0;36mxla_primitive_callable\u001b[0;34m(prim, in_avals, orig_in_shardings, **params)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out,\n\u001b[1;32m    222\u001b[0m donated_invars \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mFalse\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(in_avals)\n\u001b[0;32m--> 223\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_xla_callable_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprim_fun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_in_shardings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prim\u001b[38;5;241m.\u001b[39mmultiple_results:\n\u001b[1;32m    227\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: compiled(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:253\u001b[0m, in \u001b[0;36m_xla_callable_uncached\u001b[0;34m(fun, name, donated_invars, keep_unused, in_avals, orig_in_shardings)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_xla_callable_uncached\u001b[39m(fun: lu\u001b[38;5;241m.\u001b[39mWrappedFun, name, donated_invars,\n\u001b[1;32m    249\u001b[0m                            keep_unused, in_avals, orig_in_shardings):\n\u001b[1;32m    250\u001b[0m   computation \u001b[38;5;241m=\u001b[39m sharded_lowering(\n\u001b[1;32m    251\u001b[0m       fun, name, donated_invars, keep_unused, \u001b[38;5;28;01mTrue\u001b[39;00m, in_avals, orig_in_shardings,\n\u001b[1;32m    252\u001b[0m       lowering_platform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomputation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsafe_call\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:2323\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2320\u001b[0m   executable \u001b[38;5;241m=\u001b[39m MeshExecutable\u001b[38;5;241m.\u001b[39mfrom_trivial_jaxpr(\n\u001b[1;32m   2321\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_args)\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2323\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompiler_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2329\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:2645\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2642\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2645\u001b[0m xla_executable, compile_options \u001b[38;5;241m=\u001b[39m \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_replicated\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2652\u001b[0m   semantics_in_shardings \u001b[38;5;241m=\u001b[39m SemanticallyEqualShardings(in_shardings)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:2555\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, _allow_propagation_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[1;32m   2550\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, compile_options\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2554\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2555\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable, compile_options\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:497\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m    493\u001b[0m use_compilation_cache \u001b[38;5;241m=\u001b[39m (compilation_cache\u001b[38;5;241m.\u001b[39mis_initialized() \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    494\u001b[0m                          backend\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;129;01min\u001b[39;00m supported_platforms)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_compilation_cache:\n\u001b[0;32m--> 497\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m cache_key \u001b[38;5;241m=\u001b[39m compilation_cache\u001b[38;5;241m.\u001b[39mget_cache_key(\n\u001b[1;32m    501\u001b[0m     computation, devices, compile_options, backend)\n\u001b[1;32m    503\u001b[0m cached_executable \u001b[38;5;241m=\u001b[39m _cache_read(module_name, cache_key, compile_options,\n\u001b[1;32m    504\u001b[0m                                 backend)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Check GPU availability and run a simple operation\n",
    "device = jax.devices()[0]\n",
    "x = jnp.ones((1000, 1000))\n",
    "result = jnp.dot(x, x)\n",
    "print(result.device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bda8758-d74e-4ef3-bdc9-e5ab3065405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 03:28:50.405866: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Generate test inputs\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Compute exact GELU values\u001b[39;00m\n\u001b[1;32m     64\u001b[0m exact_values \u001b[38;5;241m=\u001b[39m exact_gelu(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2390\u001b[0m, in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m   2388\u001b[0m num \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mconcrete_or_error(operator\u001b[38;5;241m.\u001b[39mindex, num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument of jnp.linspace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2389\u001b[0m axis \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mconcrete_or_error(operator\u001b[38;5;241m.\u001b[39mindex, axis, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument of jnp.linspace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_linspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from time import time\n",
    "\n",
    "# Specify CPU as the device\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "# Define the exact GELU function\n",
    "def exact_gelu(x):\n",
    "    return 0.5 * x * (1 + jnp.tanh(jnp.sqrt(2 / jnp.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "# Define Method 1 (Piecewise Polynomial Approximation)\n",
    "def hack_gelu_method1(x):\n",
    "    F0_coeffs = jnp.array([-0.01101871, -0.11797748, -0.42209696, -0.50535995])\n",
    "    F1_coeffs = jnp.array([0.10369006, 0.44591346, 0.50133061, -0.00097519])\n",
    "    F2_coeffs = jnp.array([-0.10369006, 0.44591346, 0.49866939, -0.00097519])\n",
    "    F3_coeffs = jnp.array([0.01101871, -0.11797748, 1.42209696, -0.50535995])\n",
    "\n",
    "    less_than_minus_2 = x < -2\n",
    "    between_minus_2_and_0 = (x >= -2) & (x < 0)\n",
    "    between_0_and_2 = (x >= 0) & (x <= 2)\n",
    "    greater_than_2 = x > 2\n",
    "\n",
    "    F0_x = F0_coeffs[0] + F0_coeffs[1] * x + F0_coeffs[2] * jnp.square(x) + F0_coeffs[3] * x**3\n",
    "    F1_x = F1_coeffs[0] + F1_coeffs[1] * x + F1_coeffs[2] * jnp.square(x) + F1_coeffs[3] * x**3\n",
    "    F2_x = F2_coeffs[0] + F2_coeffs[1] * x + F2_coeffs[2] * jnp.square(x) + F2_coeffs[3] * x**3\n",
    "    F3_x = F3_coeffs[0] + F3_coeffs[1] * x + F3_coeffs[2] * jnp.square(x) + F3_coeffs[3] * x**3\n",
    "\n",
    "    result = jnp.where(less_than_minus_2, F0_x,\n",
    "                       jnp.where(between_minus_2_and_0, F1_x,\n",
    "                                 jnp.where(between_0_and_2, F2_x, F3_x)))\n",
    "    return result\n",
    "\n",
    "# Define Method 2 (PUMA's GELU Approximation)\n",
    "def hack_gelu_method2(x):\n",
    "    b0 = x < -4.0\n",
    "    b1 = x < -1.95\n",
    "    b2 = x > 3.0\n",
    "    b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]\n",
    "    b4 = b0 ^ b1  # x in [-4, -1.95]\n",
    "\n",
    "    a_coeffs = jnp.array(\n",
    "        [-0.5054031199708174, -0.42226581151983866, -0.11807612951181953, -0.011034134030615728]\n",
    "    )\n",
    "    b_coeffs = jnp.array(\n",
    "        [0.008526321541038084, 0.5, 0.3603292692789629, 0.0, -0.037688200365904236, 0.0, 0.0018067462606141187]\n",
    "    )\n",
    "\n",
    "    x2 = jnp.square(x)\n",
    "    x3 = jnp.multiply(x, x2)\n",
    "    x4 = jnp.square(x2)\n",
    "    x6 = jnp.square(x3)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x6 + b_coeffs[4] * x4 + b_coeffs[2] * x2 + b_coeffs[1] * x + b_coeffs[0]\n",
    "\n",
    "    ret = b2 * x + b4 * seg1 + b3 * seg2\n",
    "    return ret\n",
    "\n",
    "# Generate test inputs\n",
    "x = jnp.linspace(-5, 5, 1000)\n",
    "\n",
    "# Compute exact GELU values\n",
    "exact_values = exact_gelu(x)\n",
    "\n",
    "# Compute approximate GELU values using both methods\n",
    "approx_method1 = hack_gelu_method1(x)\n",
    "approx_method2 = hack_gelu_method2(x)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) between the approximations and the exact values\n",
    "mse_method1 = jnp.mean((exact_values - approx_method1)**2)\n",
    "mse_method2 = jnp.mean((exact_values - approx_method2)**2)\n",
    "\n",
    "print(f\"Mean Squared Error (Method 1): {mse_method1}\")\n",
    "print(f\"Mean Squared Error (Method 2): {mse_method2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74107b66-f8d4-4b61-836b-682ae081937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for GELU: 0.005192 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def hack_gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Polynomial approximation for GELU\n",
    "    x2 = torch.square(x)\n",
    "    x3 = x * x2\n",
    "    x4 = torch.square(x2)\n",
    "\n",
    "    a_coeffs = torch.tensor(\n",
    "        [\n",
    "            -0.5054031199708174,\n",
    "            -0.42226581151983866,\n",
    "            -0.11807612951181953,\n",
    "            -0.011034134030615728,\n",
    "        ]\n",
    "    ).to(x.device)\n",
    "\n",
    "    b_coeffs = torch.tensor(\n",
    "        [\n",
    "            0.008526321541038084,\n",
    "            0.5,\n",
    "            0.3603292692789629,\n",
    "            0.0,\n",
    "            -0.037688200365904236,\n",
    "            0.0,\n",
    "            0.0018067462606141187,\n",
    "        ]\n",
    "    ).to(x.device)\n",
    "\n",
    "    seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]\n",
    "    seg2 = b_coeffs[6] * x4 + b_coeffs[4] * x2 + b_coeffs[2] * x + b_coeffs[1]\n",
    "\n",
    "    # Use conditions to determine the appropriate segment\n",
    "    result = torch.where(x < -1.95, seg1, seg2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Measure time for GELU\n",
    "input_tensor = torch.randn(1000000)  # Large tensor for timing\n",
    "start_time = time.time()\n",
    "\n",
    "output = hack_gelu(input_tensor)\n",
    "\n",
    "end_time = time.time()\n",
    "gelu_time = end_time - start_time\n",
    "print(f\"Time taken for GELU: {gelu_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2036dbe4-7f53-45e9-b854-e1369a0c3631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-12-10 01:46:56.677958: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mFlaxGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/modeling_flax_utils.py:903\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, dtype, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m     safetensors_from_pt \u001b[38;5;241m=\u001b[39m safetensors_metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# init random models\u001b[39;00m\n\u001b[0;32m--> 903\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_do_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt \u001b[38;5;129;01mor\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m    906\u001b[0m     state \u001b[38;5;241m=\u001b[39m load_pytorch_checkpoint_in_flax_state_dict(model, resolved_archive_file, is_sharded)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/models/gpt2/modeling_flax_gpt2.py:400\u001b[0m, in \u001b[0;36mFlaxGPT2PreTrainedModel.__init__\u001b[0;34m(self, config, input_shape, seed, dtype, _do_init, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    392\u001b[0m     config: GPT2Config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    398\u001b[0m ):\n\u001b[1;32m    399\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_class(config\u001b[38;5;241m=\u001b[39mconfig, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_do_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/transformers/modeling_flax_utils.py:210\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.__init__\u001b[0;34m(self, config, module, input_shape, seed, dtype, _do_init)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Those are public as their type is generic to every derived classes.\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m \u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape \u001b[38;5;241m=\u001b[39m input_shape\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/random.py:160\u001b[0m, in \u001b[0;36mPRNGKey\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(seed):\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRNGKey accepts a scalar seed, but was given an array of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mprng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed_with_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(\u001b[38;5;28;01mTrue\u001b[39;00m, key)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:406\u001b[0m, in \u001b[0;36mseed_with_impl\u001b[0;34m(impl, seed)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseed_with_impl\u001b[39m(impl: PRNGImpl, seed: Union[\u001b[38;5;28mint\u001b[39m, Array]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PRNGKeyArrayImpl:\n\u001b[0;32m--> 406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:690\u001b[0m, in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   seeds_arr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(seeds)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_seed_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:702\u001b[0m, in \u001b[0;36mrandom_seed_impl\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;129m@random_seed_p\u001b[39m\u001b[38;5;241m.\u001b[39mdef_impl\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[0;32m--> 702\u001b[0m   base_arr \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_seed_impl_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m PRNGKeyArrayImpl(impl, base_arr)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:707\u001b[0m, in \u001b[0;36mrandom_seed_impl_base\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed_impl_base\u001b[39m(seeds, \u001b[38;5;241m*\u001b[39m, impl):\n\u001b[1;32m    706\u001b[0m   seed \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(seeds\u001b[38;5;241m.\u001b[39mndim, impl\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 707\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/prng.py:936\u001b[0m, in \u001b[0;36mthreefry_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreefry_seed\u001b[39m(seed: typing\u001b[38;5;241m.\u001b[39mArray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mArray:\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a single raw threefry PRNG key from an integer seed.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    first padding out with zeros).\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_threefry_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dce5a0-b93a-4bc9-86da-260862d29ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation(input_ids, params):\n",
    "    config = GPT2Config()\n",
    "    model = FlaxGPT2LMHeadModel(config=config)\n",
    "\n",
    "    for _ in range(10):\n",
    "        outputs = model(input_ids=input_ids, params=params)\n",
    "        next_token_logits = outputs[0][0, -1, :]\n",
    "        next_token = jnp.argmax(next_token_logits)\n",
    "        input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144afbc-c777-4aec-90cf-23578326db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "inputs_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')\n",
    "outputs_ids = text_generation(inputs_ids, pretrained_model.params)\n",
    "\n",
    "print('-' * 65 + '\\nRun on CPU:\\n' + '-' * 65)\n",
    "print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4641420-cde4-413d-a597-79d61c4ab4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 01:27:18,651\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "RayTaskError(NameError)",
     "evalue": "\u001b[36mray::_run()\u001b[39m (pid=73196, ip=172.17.0.10)\n  File \"/root/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/device/pyu.py\", line 156, in _run\n    return fn(*args, **kwargs)\n  File \"/tmp/ipykernel_62950/3698411567.py\", line 16, in get_model_params\nNameError: name 'FlaxGPT2LMHeadModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(NameError)\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m input_token_ids \u001b[38;5;241m=\u001b[39m bob(get_token_ids)()\n\u001b[1;32m     28\u001b[0m device \u001b[38;5;241m=\u001b[39m spu\n\u001b[0;32m---> 29\u001b[0m model_params_, input_token_ids_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, input_token_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m output_token_ids \u001b[38;5;241m=\u001b[39m spu(text_generation)(input_token_ids_, model_params_)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/device/base.py:70\u001b[0m, in \u001b[0;36mDeviceObject.to\u001b[0;34m(self, device, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Device, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Device object conversion.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m        DeviceObject: Target device object.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_name_of_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/device/register.py:111\u001b[0m, in \u001b[0;36mdispatch\u001b[0;34m(name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch device kernel.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m        Kernel execution result.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_registrar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/device/register.py:80\u001b[0m, in \u001b[0;36mRegistrar.dispatch\u001b[0;34m(self, device_type, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ops[device_type]:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, op: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not registered\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ops\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/kernels/pyu.py:76\u001b[0m, in \u001b[0;36mpyu_to_spu\u001b[0;34m(self, spu, spu_vis)\u001b[0m\n\u001b[1;32m     73\u001b[0m     ret \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mmake_shares(data, vtype)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m---> 76\u001b[0m shares_chunk_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_shares_chunk_count\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvtype\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m shares_chunk_count \u001b[38;5;241m=\u001b[39m sfd\u001b[38;5;241m.\u001b[39mget(shares_chunk_count\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     81\u001b[0m meta, io_info, \u001b[38;5;241m*\u001b[39mshares_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice(\n\u001b[1;32m     82\u001b[0m     run_spu_io, num_returns\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m shares_chunk_count \u001b[38;5;241m*\u001b[39m spu\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m     83\u001b[0m )(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, spu\u001b[38;5;241m.\u001b[39mconf, spu\u001b[38;5;241m.\u001b[39mworld_size, vtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/device/pyu.py:100\u001b[0m, in \u001b[0;36mPYU.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m args_, kwargs_ \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m arg: try_get_data(arg, \u001b[38;5;28mself\u001b[39m), (args, kwargs)\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m _num_returns \u001b[38;5;241m=\u001b[39m check_num_returns(fn) \u001b[38;5;28;01mif\u001b[39;00m num_returns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_returns\n\u001b[1;32m     99\u001b[0m data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 100\u001b[0m     \u001b[43msfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_num_returns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYU remote function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_returns=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_returns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, kwargs len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(kwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_returns \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/ray/remote_function.py:226\u001b[0m, in \u001b[0;36mRemoteFunction.options.<locals>.FuncWrapper.remote\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremote\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdated_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/distributed/primitive.py:225\u001b[0m, in \u001b[0;36mRemoteFunctionWrapper._remote\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_remote\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 225\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_remote(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/distributed/primitive.py:215\u001b[0m, in \u001b[0;36m_resolve_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m arg_flat, arg_tree \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_flatten((args, kwargs))\n\u001b[1;32m    211\u001b[0m refs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    212\u001b[0m     pos: arg \u001b[38;5;28;01mfor\u001b[39;00m pos, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arg_flat) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ray\u001b[38;5;241m.\u001b[39mObjectRef)\n\u001b[1;32m    213\u001b[0m }\n\u001b[0;32m--> 215\u001b[0m actual_vals \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos, actual_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(refs\u001b[38;5;241m.\u001b[39mkeys(), actual_vals):\n\u001b[1;32m    217\u001b[0m     arg_flat[pos] \u001b[38;5;241m=\u001b[39m actual_val\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sf/lib/python3.8/site-packages/ray/_private/worker.py:2309\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2307\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(NameError)\u001b[0m: \u001b[36mray::_run()\u001b[39m (pid=73196, ip=172.17.0.10)\n  File \"/root/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/device/pyu.py\", line 156, in _run\n    return fn(*args, **kwargs)\n  File \"/tmp/ipykernel_62950/3698411567.py\", line 16, in get_model_params\nNameError: name 'FlaxGPT2LMHeadModel' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 01:27:36,970\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_run()\u001b[39m (pid=73294, ip=172.17.0.10)\n",
      "  File \"/root/miniconda3/envs/sf/lib/python3.8/site-packages/secretflow/device/device/pyu.py\", line 156, in _run\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_62950/3698411567.py\", line 21, in get_token_ids\n",
      "NameError: name 'AutoTokenizer' is not defined\n"
     ]
    }
   ],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# In case you have a running secretflow runtime already.\n",
    "sf.shutdown()\n",
    "\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])\n",
    "conf['runtime_config']['fxp_exp_mode'] = 1\n",
    "conf['runtime_config']['experimental_disable_mmul_split'] = True\n",
    "spu = sf.SPU(conf)\n",
    "\n",
    "\n",
    "def get_model_params():\n",
    "    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    return pretrained_model.params\n",
    "\n",
    "\n",
    "def get_token_ids():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer.encode('Chinaa is', return_tensors='jax')\n",
    "\n",
    "\n",
    "model_params = alice(get_model_params)()\n",
    "input_token_ids = bob(get_token_ids)()\n",
    "\n",
    "device = spu\n",
    "model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)\n",
    "\n",
    "output_token_ids = spu(text_generation)(input_token_ids_, model_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5ab25d-b5a8-40ca-87bb-ba2b35c527fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs_ids \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241m.\u001b[39mreveal(output_token_ids)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m65\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRun on SPU:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m65\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sf' is not defined"
     ]
    }
   ],
   "source": [
    "outputs_ids = sf.reveal(output_token_ids)\n",
    "print('-' * 65 + '\\nRun on SPU:\\n' + '-' * 65)\n",
    "print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57468d3-6f6a-4220-a572-db587c87a5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sf)",
   "language": "python",
   "name": "sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
